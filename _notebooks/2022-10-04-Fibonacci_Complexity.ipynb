{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMF9IzjbqjSTDm6GsIttHlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmartel/fp/blob/master/_notebooks/2022-10-04-Fibonacci_Complexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computational Complexity of Fibonacci Sequences. Part 1.\n",
        "> \" I was reviewing the computational complexity of fibonacci sequences 1, 1, 2, 3, 5, 8, 13, ..., and also reflecting on Wolfram's idea of computational irreducibility. Is O(log(n)) provably the minimal time complexity of computing fibonacci sequences? Can we formalize Wolfram's definition and obtain a strategy for proving the minimality of these running times? \"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: false\n",
        "- comments: true\n",
        "- author: JHM"
      ],
      "metadata": {
        "id": "LoDxmvmBVLur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The papers of Alan Craig Feinstein on $P$ versus $NP$ have caused me to [previously review](https://jhmartel.github.io/fp/fastpages/jupyter/2022/09/03/PversusNP.html) some basic questions in computational complexity. This post continues our line of thought on what \"computational complexity\" means from the pragmatic perspective. This in contrast to \"theoretical\" computation results, which is a contradiction in terms. \n",
        "\n",
        "In this post we simply consider the question:\n",
        "_What is the computational complexity of computing the $n$-th Fibonacci number?_ \n",
        "\n",
        "Let's recall the basic definition: the fibonacci sequence is a sequence of integers $f_0, f_1, f_2, \\ldots$ defined recursively by the rules \n",
        "$$f_0=1, ~~ f_1=1, ~~\\text{and}~~ f_{n+1}=f_n+f_{n-1}~~\\text{for}~~ n\\geq 1.$$\n",
        "\n",
        "Naively to compute $f_{2022}$ would require we apply the definition and find $f_{2021}$ and $f_{2020}$, and then continuing the recursion we need to call all the previous values of $f_k$ for $k < 2022$. What is the precise computation?\n",
        "\n",
        "Let's begin with the pseudocode. The idea is memoization. If we apply the above recursive formula to evaluate $f_{2022}$, then we'll find ourselves reusing values of $f_k$ for $k < 2022$. The memoization is simply a device for recording and reusing these intermediate values. So to compute $f_{2022}$ we will need to _store_ the previous values, although there is a method (Bottom-Up evaluation) which is similar, and which inductively evaluates $f_1$, $f_2$, $f_3$, etc., until we reach $f_{2022}$. So if we compute $f_{2022}$ and memoize all the terms $m=\\{f_1, f_2, f_3, \\ldots\\}$ then we see the memo $m$ grows unbounded in length. Some authors modify the memo to only contain the two \"largest\" elements, thus reducing the length of the list. This would require updating the memo $$\\{f_1, f_2\\} \\leadsto \\{f_2, f_3\\} \\leadsto \\{f_3, f_4\\} \\leadsto \\cdots \\leadsto \\{f_{n-1}, f_n\\} \\leadsto \\cdots.$$ But still the number of bytes necessary to represent the fibonacci elements grows exponentially with $n$ since $f_{n+1} \\approx \\varphi^n$ where $\\varphi$ is the golden ratio $\\varphi=\\frac{1+\\sqrt{5}}{2} \\approx 1.618033$. \n",
        "\n",
        "The memoization gives a time complexity of $O(n)$ to compute $f_n$. But memoization requires memory to store the memo! We think this needs be accounted for in the computational complexity, since obviously it's part of the _computation_. "
      ],
      "metadata": {
        "id": "tp77VBf3-c38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Code taken from https://stackoverflow.com/a/61604929/17406611\n",
        "\n",
        "fib_cache = {0 : 0, 1 : 1}\n",
        "\n",
        "def fib(n):\n",
        "    if n in fib_cache:\n",
        "        return fib_cache[n]\n",
        "    for i in range(100, n, 100):\n",
        "      fib(i)\n",
        "    fib_cache[n] = result = fib(n-1) + fib(n-2)\n",
        "    # print(fib_cache) # <--this line is diagnostic.\n",
        "    return result\n",
        "\n",
        "  # There's interesting comment on why the range(100, n, 100) line is important\n",
        "  # to avoid a \"recursive depth error\". The point is that the memo needs to be filled\n",
        "  # as the algorithm develops, otherwise it recurses too far and returns an error!"
      ],
      "metadata": {
        "id": "5ZHAQkdTEE9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crashes the RAM\n",
        "# fib(1111111)\n",
        "\n",
        "fib(1111)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNktvRzaJO5c",
        "outputId": "e54089cd-2fb0-4130-dac0-79d3ded8b2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6851462981265369536304298877223231154064355390623195419885661484162849735541256952762360871448156142552148460793441585691068131682370855135019896825808086317430648360941203391832868742715640036246053259136014253626356840914521594989"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _Wolfram's (non)Definition of Computational Irreducibility._\n",
        "\n",
        "In Wolfram's book \"A New Kind of Science\", there is introduced the idea  of _computational irreducibility_. We quote from Chapter 13, pp.739-740. \n",
        "\n",
        "[Begin quote] \"_Computational irreducibility leads to a much more fundamental problem with prediction. For it implies that even if in principle one has all the information one needs to work out how some particular system will behave, it can still take an irreducible amount of computational work actually [sic] to do this._   \n",
        "\n",
        "___Indeed, whenever computational irreducibility exists in a system it means that in effect there can be no way to predict how the system will behave except by going through almost as many steps of computation as the evolution of the system itself.___\n",
        "\n",
        "_In traditional science it has rarely even been recognized that there is a need to consider how systems that are used to make predictions actually operate._ ___But what leads to the phenomenon of computational irreducibility is that there is in fact always a fundamental competition between systems used to make predictions and systems whose behaviour one tries to predict.___\n",
        "\n",
        "_For if meaningful general predictions are to be possible, it must at some level be the case that the system making the predictions be able to ___outrun___ the system it is trying to predict. But for this to happen the system making the predictions must be able to perform more sophisticated computations than the system is trying to predict._\n",
        "\n",
        "_But the remarkable assertion that the Principle of Computational Equivalence  makes is that this assumption is not correct, and that in fact almost any system whose behaviour is not obviously simple performs computations that are in the end exactly equivalent in their sophistication._\n",
        "\n",
        "_So what this means is that systems one uses to make predictions cannot be expected to do computations that are any more sophisticated than the computations that occur in all sorts of systems whose behaviour we might try to predict. And from this it follows that for many systems no systematic prediction can be done, so that there is no general way to shortcut their process of evolution, and as a result their behaviour must be considered computationally irreducible._\" [End quote]\n",
        "\n",
        "Here's what I like about Wolfram's idea of \"computational irreducibility\". Firstly it aligns with experience. Mathematicians who actually compute things know that there's no shortcuts for alot of operations. It's very difficult to add two numbers $a+b$ without actually \"adding\" them arithmetically. In otherwords, mathematicians don't expect shortcuts. This relates to Craig Alan Feinstein's arguments that $P \\neq NP$, and specifically that the subset-sum problem is computationally irreducible. To decide whether a set of signed integers $S$ contains a zero subset-sum, there really is nothing we can do except exhaustively search through all the subsets and add the elements. But most theoretical computer scientists \"dream\" of somehow there existing a shortcut, a black box, a sophisticated magical algorithm that will compute the sums without actually computing them! So Wolfram's idea of \"computational irreducibility\" is in opposition to the theoretical computer scientists' optimism. \n",
        "\n",
        "But what's lacking is any rigorous proof or definition of _computational irreducibility_. If I had to make a pseudo-definition (intuitive) I would first say: \n",
        "\n",
        "- a computation is the evaluation of a function $f: X \\to Y$, \n",
        "\n",
        "- and the evaluation $f$ is _computationally irreducible_ if the complexity of any equivalent composition $g: X \\to Z$ and $h: Z \\to Y$ with $f = h \\circ g$ satisfies $$c(f) \\leq c(h) + c(g),$$ where $c(f)$ represents the \"computational complexity\" of evaluating a function $f$. \n",
        "\n",
        "However the complexity $c(f)$ is not well-defined at this point. But the idea is that a function $f$ always admits (nonunique!) compositions $f=h \\circ g$, but the question would be whether these compositions $h, g$ are any simpler than the evaluation of $f$ itself! \n",
        "\n",
        "The above is not sufficiently rigorous since we have not defined $c(f)$. Now it must be clear that $c(-)$ is _not_ defined on the category of functions, since compositions $f=h\\circ g$ are _identical_ functions, but not necessarily identical _computations_. \n",
        "\n",
        "For example, Wolfram would argue that the Collatz function arising in the Collatz conjecture [$3x+1$ problem](https://www.youtube.com/watch?v=094y1Z2wpJg) is computationally irreducible. There are no shortcuts for evaluating the Collatz function, and therefore there will never be a _proof_ of Collatz' conjecture. This is why no mathematician should study the problem, because the results/outputs of the Collatz function cannot be logically established, they can only be _empirically_ established by directly evaluating the Collatz function using the definition. \n",
        "\n",
        "\n",
        "# _Topological Irreducibility_\n",
        "\n",
        "Let's examine the word _irreducibility_ somewhat more. Wikipedia has many references, but the primary definition (for our perspective) is the topological irreducibility. Here is the definition from Wikipedia \n",
        "\n",
        "_Def. (Reducible and Irreducible Topological Spaces)_\n",
        "- _A topological space $X$ is reducible if it can be written as a union  \n",
        "$ X= X_1 \\cup X_2$ of two closed proper subsets $X_1, X_2$ of $X$._\n",
        "- _A topological space $X$ is irreducible (or hyperconnected) if it is not reducible. Equivalently, if all non empty open subsets of X are dense, or if any two nonempty open sets have nonempty intersection._\n",
        "\n",
        "So the computational aspect of irreducibility would involve computing intersections of nontrivial open subsets of the space. Formally it would be better to provide the relative definition of irreducibility for subsets $A \\subset X$, but we just use the subspace topology on $A$ in $X$. I.e., open subsets of $A$ are defined as intersections $U \\cap A$ of open subsets $U$ of $X$. It's useful to introduce the relative definition because our first idea for defining computational complexity is to consider the topology of the graph of the function in question, i.e. $graph(f) \\subset X \\times Y$. Now if we have a composition $f = h \\circ g$, then we obtain two graphs(!) namely $$graph(h) \\subset Z \\times Y$$ and $$graph(g)\\subset X \\times Z.$$ \n",
        "\n",
        "I suppose it would be interesting to determine whether the graph of the Collatz function is an irreducible topological subset of the product $\\bf{N} \\times \\bf{N}$. And here we need specific the topology of $\\bf{N}$, which is not so obvious, although perhaps the discrete topology is the most natural. \n",
        "\n",
        "To this point we haven't proved anything new, we've only conceptualized the problem, trying to more formally define \"irreducibility\". We have yet to define the complexity $c(f)$ of a function although our idea is that $c(f)$ should reflect a property of $graph(f)$. Dependancy on the graph of $f$ allows us to compare compositions $f=h\\circ g$ and maybe develop comparisons between $c(f)$, $c(g)$, $c(h)$, etc.. \n",
        "\n",
        "But the point is that we have a strategy now, and we can consult the literature by looking for techniques to prove the irreducibility of topological spaces. This is a general problem, and we can even begin with the more specialized case of algebraic geometry. \n",
        "\n",
        "In algebraic geometry, if a variety is irreducible then what does that mean _in concrete terms_ for the equations defining the variety? For hypersurfaces it means the variety can be described by two equations instead of one. For example a polynomial $p(x)$ is reducible if $p(x)=q_1(x) . q_2(x)$ for nontrivial polynomials $q_1, q_2$. What's interesting for polynomials is that the degrees of the products are strictly smaller, namely $$deg(p) = deq(q_1)+deg(q_2).$$\n",
        "\n",
        "The above definition implies the standard algebraic geometry definition of irreducible algebraic varieties $V$ via the topology of prime ideals in the spectrum $Spec(V)$ of $V$.\n",
        "\n",
        "But this is the beginning of another post. \n",
        "-JHM."
      ],
      "metadata": {
        "id": "yXRTcGnFm9QB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hide \n",
        "(Under Construction) Here we review a well known subject, namely the computational complexity of Fibonacci sequence $\\{ 1, 1, 2, 3, 5, 8, 13, 21, \\ldots\\}$, but trying to understand the difference between $O(n)$ and $O(log_2 n)$ complexity. We use the occasion to try and clarify Wolfram's idea of computational irreducibility. Is $O(log_2 n)$ the minimal time complexity of evaluating the $n$th fibonacci element?"
      ],
      "metadata": {
        "id": "si1cF12vaDkv"
      }
    }
  ]
}