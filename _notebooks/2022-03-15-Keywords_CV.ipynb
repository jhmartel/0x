{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmartel/fp/blob/master/_notebooks/2022-03-15-Keywords_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnBV6RAxEyJr"
      },
      "source": [
        "# \"How to Automatically Align Cover Letters and CVs with Job Posts\"\n",
        "\n",
        "> \"Using scaPy to automatically align with job postings.\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [python, jupyter, job, scapy]\n",
        "- hide: true\n",
        "- search_exclude: true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> youtube: https://youtu.be/gdK9Mj5PO2g"
      ],
      "metadata": {
        "id": "nRVN3qNl_nvT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7ZyS15yvNg"
      },
      "source": [
        "What's my goal? To get a job involving mathematics, cyber security, technical writing and coding.\n",
        "\n",
        "Do i need send out 200+ cv's without any replies? Some people say \"yes\", but I really don't know if it's productive.\n",
        "\n",
        "On LinkedIn there was a better suggestion: Find five or six positions where i'm a moderate match (~~60-70%) and reverse-engineer a cv and cover letter for each of them. Here we mean using the same verbs (action words), noun groups (things), and entities. For example, can we ensure that the name of the company, and role, and somebody specific (human reader) is also referenced in the submitted cv and cover letter?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0d5oc0802WU"
      },
      "source": [
        "The starting point is spaCy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5XHXC982VCq"
      },
      "outputs": [],
      "source": [
        "# pip install -U spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_cJ3hu9Y2kV"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNWUlES9ZW0p"
      },
      "source": [
        "We find the job posting, and we are going to copy the contents to a text file, and upload into our google drive.\n",
        "\n",
        "For example, we were reviewing a company called \"Giatec\" and the position posted at https://giatecscientific.bamboohr.com/jobs/view.php?id=107&source=bamboohr\n",
        "\n",
        "Another job is https://www.linkedin.com/jobs/view/2750187552/?eBP=JOB_SEARCH_ORGANIC&recommendedFlavor=ACTIVELY_HIRING_COMPANY&refId=nml6mNf8uTGRIKmPP5zvJw%3D%3D&trackingId=HfH6ZH1GBKokUeeNyN9TdA%3D%3D&trk=flagship3_search_srp_jobs\n",
        "\n",
        "We copy paste the relevant contents of the job post to a .txt document. For example we called the file giatek.txt . \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beJzwqYF2RZP",
        "outputId": "0aef5d14-f336-450e-83b4-71fc656ff8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noun phrases: ['Principal Data', 'Scientist/ Modeler', 'Cyber Risk Analytics', 'CAN Remote Option', 'the job', 'a new challenge', 'Join Guidewire', 'a Principal Data Modeler', 'Guidewire', 'Cyence Risk Analytics products', 'the property & casualty (P&C) industry', 'new and evolving risks', 'cyber', 'internet-scale data listening', 'adaptive machine learning', 'insurance risk modeling', 'Cyence Risk Analytics', 'insights', 'P&C customers', 'new risks', 'advantage', 'new opportunities', 'new products', 'Who', 'We', 'What', 'We', 'What', 'We', 'Guidewire', 'the AWS', 'insurance', 'the market leader', '380 insurance companies', 'our mission', 'we', 'underwriters', 'policies', 'agents', 'claims', 'We', 'a great decision', 'house', 'Our products', 'cyber risk quantification', 'potent ML sandboxes', 'We', 'a post-IPO company', 'the vision', 'insurance', 'a lot', 'Who', 'You', 'Your responsibilities', 'a Principal Modeler', 'methodologies', 'the impact', 'cyber security risk', 'other emerging risks', 'the P&C industry', 'different data sources', 'features', 'assumptions', 'our set', 'risk models', 'Calibrating', 'testing', 'different types', 'models', 'the insurance space', 'Integrating modeling processes', 'our production pipeline', 'our platform Communicating technical details', 'the models', 'their outputs', 'individuals', 'various backgrounds', 'cyber analysts', 'data engineers', 'modelers', 'product managers', 'What', 'We', 'Master', 'PhD degree', 'a quantitative field', '(e.g. CS', 'years', 'solid statistics', 'predictive modeling foundations', 'Experience', 'data analysis', 'feature engineering', 'data visualization', 'hypotheses', 'Experience', 'different types', 'datasets', 'missing information', 'Experience', 'parametric and non-parametric models', 'a production-level environment', 'High level proficiency', 'Python', 'R', 'SQL Good', 'verbal communication skills Ability', 'a dynamic environment', 'new tools', 'domain knowledge', 'the way', 'A positive attitude', 'a growth mindset Bonus Insurance', 'actuarial modeling', 'financial modeling', 'experience General understanding', 'computer network', 'cybersecurity Experience', 'Cloud infrastructure', '(e.g. AWS', 'open source data processing frameworks', '(e.g. Hadoop', 'Spark', 'Mongo', 'Cassandra', 'Guidewire  Guidewire', 'the platform P&C insurers', 'Guidewire', 'core', 'data', 'digital', 'analytics', 'AI', 'our platform', 'a cloud service', 'More than 400 insurers', 'the world', 'Guidewire', 'a partner', 'our customers', 'we', 'their success', 'We', 'our unparalleled implementation track record', '1000+ successful projects', 'the largest R&D team', 'partner ecosystem', 'the industry', 'Our Marketplace', 'hundreds', 'add-ons', 'integration', 'localization', 'innovation', 'Guidewire Software Inc.', 'equal employment opportunities', 'all applicants', 'employment', 'discrimination', 'harassment', 'any type', 'regard', 'race', 'color', 'religion', 'age', 'sex', 'national origin', 'disability status', 'genetics', 'veteran status', 'sexual orientation', 'gender identity', 'expression', 'any other characteristic', 'federal, state or local laws', 'All offers', 'a criminal history', 'other background checks', 'it', 'the position', 'We', 'individuals', 'disabilities', 'reasonable accommodation', 'the job application', 'interview process', 'essential job functions', 'other benefits', 'privileges', 'employment', 'us', 'accommodation']\n",
            "Verbs: ['CAN', 'join', 'help', 'model', 'evolve', 'combine', 'provide', 'help', 'face', 'take', 'develop', 'believe', 'build', 'run', 'support', 'craft', 'settle', 'believe', 'make', 'should', 'require', 'range', 'redefine', 'do', 'will', 'include', 'develop', 'implement', 'quantify', 'emerge', 'affect', 'explore', 'come', 'enhance', 'validate', 'feed', 'communicate', 'relate', 'work', 'look', 'test', 'work', 'miss', 'implement', 'write', 'think', 'pick', 'open', 'trust', 'engage', 'innovate', 'grow', 'combine', 'deliver', 'include', 'run', 'evolve', 'enable', 'support', 'provide', 'accelerate', 'provide', 'prohibit', 'protect', 'protect', 'pass', 'will', 'ensure', 'provide', 'participate', 'perform', 'receive', 'contact', 'request']\n",
            "59\n",
            "['communicate', 'participate', 'accelerate', 'implement', 'innovate', 'prohibit', 'redefine', 'quantify', 'validate', 'develop', 'deliver', 'explore', 'combine', 'contact', 'require', 'support', 'receive', 'perform', 'request', 'provide', 'believe', 'include', 'protect', 'enhance', 'evolve', 'emerge', 'enable', 'should', 'settle', 'affect', 'engage', 'ensure', 'relate', 'write', 'range', 'trust', 'think', 'craft', 'model', 'build', 'miss', 'work', 'look', 'make', 'open', 'feed', 'face', 'test', 'help', 'pass', 'come', 'pick', 'grow', 'will', 'take', 'join', 'CAN', 'run', 'do']\n",
            "the length of the verb list is:  59\n"
          ]
        }
      ],
      "source": [
        "f=open(\"./guidewire.txt\", 'rb').read()\n",
        "f_string=f.decode()  # converts the binary text document into a python string.\n",
        "\n",
        "import re\n",
        "f_string = re.sub('\\n', ' ', f_string)\n",
        "\n",
        "#Now we get a nice clean string, with all \"\\n\" characters removed.\n",
        "#print(f_string)\n",
        "\n",
        "doc = nlp(f_string)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "#for entity in doc.ents:\n",
        "#    print(entity.text, entity.label_)\n",
        "\n",
        "\n",
        "\n",
        "# we can begin with the longest verbs, which we assume are the most \"interesting\".\n",
        "\n",
        "verb_list=[]\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ == \"VERB\":\n",
        " #       print(token.lemma_)\n",
        "        verb_list=verb_list+[token.lemma_]\n",
        "    else: \n",
        "        pass\n",
        "\n",
        "verb_list=list(set(verb_list))\n",
        "verb_list.sort(reverse=True, key=lambda verb: len(verb))\n",
        "\n",
        "print(len(verb_list))\n",
        "print(verb_list)\n",
        "\n",
        "\n",
        "print(\"the length of the verb list is: \", len(verb_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "id": "5Z983l0JGTT0",
        "outputId": "113dde92-d727-443c-c885-1e305ca74946"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-464af29b72aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mverb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mverb_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0muser_sent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"  --->>  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msentence_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "sentence_list=[]\n",
        "for verb in verb_list:\n",
        "    user_sent=input(\" \"+str(verb)+\"  --->>  \")\n",
        "    if len(user_sent)>1: \n",
        "        sentence_list=sentence_list+[user_sent]\n",
        "        \n",
        "    else:\n",
        "      \n",
        "        pass\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ein_aJpF1Nz"
      },
      "outputs": [],
      "source": [
        "text_file = open(\"sentence_list_guidewire.txt\", \"w\")\n",
        "#Opens or creates the .txt file, sharing the directory of the script#\n",
        "\n",
        "for sent in sentence_list:\n",
        "    text_file.write(sent)\n",
        "    text_file.write(\"\\n\")\n",
        "#Writes the variable into the .txt file#\n",
        "text_file.close()\n",
        "#Closes the .txt file#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99_Sy0luEcj8"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Keywords_CV.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObjSjUKSAdHSb9wzvB62Yl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}