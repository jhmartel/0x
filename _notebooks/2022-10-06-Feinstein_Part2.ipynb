{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmf7QQ509Mn7etw5g2MmaF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmartel/fp/blob/master/_notebooks/2022-10-06-Feinstein_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deciding Injectivity of Linear Transformations and C.A.Feinstein's Proposed Negative Solution to P versus NP.\n",
        "> \" We continue to review and reflect on questions suggested by Craig Alan Feinstein's proposed negative solution to P versus NP. We are specifically reflecting on a signed subset sum problem, which is effectively a linearization of the unsigned subset sum. But then the question reduces to deciding the injectivity of a linear map, namely from a large dimensional vector space to a much smaller dimensional vector space. This raises the question of what is the computational complexity of deciding injectivity of linear maps? Is it really ever possible to deterministically decide the injectivity of a map without necessarily running through every evaluation? \"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: false\n",
        "- comments: true\n",
        "- author: JHM"
      ],
      "metadata": {
        "id": "amcSf3yUe-Eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# _More Remarks on Craig Alan Feinstein's P versus NP argument._\n",
        "In previous articles here and here we have been reviewing Craig Alan Feinstein's proposed solution to the $P$ versus $NP$ problem, and he has been poorly received by the so-called experts. But we continue to find his papers interesting, and this post is mainly about his [_Dialogue Concerning The Two Chief World Views_](https://arxiv.org/pdf/1605.08639.pdf) which contains some interesting remarks on precisely _what_ is the argument being made by CAF that $P \\neq NP$. \n",
        "\n",
        "1. The first point is that given a finite subset $X \\subset \\bf{Z}$, there are indeed exponentially many expressions in $2^X$. This is obvious. But generically there are also $2^{\\#(X)}$ _distinct values_ in the image $\\epsilon: 2^X \\to \\bf{Z}$ where $\\epsilon(Y) = \\sum_{y\\in Y} y$ is the natural addition map defined on subsets $Y \\in 2^X$. \n",
        "\n",
        "This point is made as rebuttal to the objection that if $X$ is bounded a priori, then it's known that there are _polynomial many_ _distinct values_ of the image $2^X \\to \\bf{Z}$. [Reference needed]. We think this is important observation, but one which might require more evidence or proof. Can we more formally prove that _arbitrary_ subsets $X \\subset \\bf{Z}$ almost surely have exponentially many distinct values in the image $\\epsilon: 2^X \\to \\bf{Z}$?\n",
        "\n",
        "If $X$ is in fact a subset of the powers of $2$, namely $$ \\{1, 2^2, 2^4, 2^8\\}$$ then there's large gaps between the elements, and we think it's clear that the image is exponential. What's also interesting in these cases is that it's also evident that there is a _negative solution_ to the SUBSET-SUM problem. \n",
        "\n",
        "And this raises the second interesting point in CAF's paper:\n",
        "\n",
        "2. The algorithms which are competing with the \"meet in the middle\" algorithm (which has $O(2^{n/2})$ time-step complexity) must also have the ability to deterministically return a _negative answer_ to SUBSET-SUM(X) if there are no zero subset sums for the given input $X$. \n",
        "\n",
        "Perhaps it can be argued that generically the solution to SUBSET-SUM is negative, and therefore the exponential search is almost always required to determine negative solutions. It's maybe useful to contrast this with algorithms which are known _a priori_ to have positive solutions, for example linear sort of a list of integers. There are surely many more examples of algorithms which are known a priori to terminate with positive solutions. For example, Hilbert Nullstellensatz as described by [Blum-X-Smale-Shub-et.al.] over the complex numbers. \n",
        "\n",
        "And actually, doesn't it now appear that: the whole question of SUBSET-SUM(X) is precisely about the injectivity of the map $\\epsilon: 2^X \\to \\bf{Z}$. This is because $\\epsilon$ is basically linear and the difference $$\\epsilon(f)-\\epsilon(g) = 0$$ if and only if $\\epsilon(f-g)=0$. But $f-g$ represents a _signed_ symmetric difference, so it's not exactly a solution of SUBSET-SUM.  \n",
        "\n",
        "Idea: perhaps it's possible that the signed subset-sum problem is equivalent to unsigned subset-sum. If we return a signed solution $f-g$ then we can examine whether its a positive solution by looking at the signs of the coefficients. This would only add a linear time-complexity. So maybe we should simply focus on the complexity of injectivity of $2^X$ and its relation to negative solutions. Because any algorithms which competes with MITM algorithm must at least solve this decision problem. \n",
        "\n",
        "\n",
        "# _On the Injectivity of_ $\\epsilon: 2^X \\to \\bf{Z}$\n",
        "\n",
        "The idea of the previous section is to actually relax somewhat and consider the Signed Subset-Sum problem, maybe abbreviated $SS_{\\pm}(X)$ for a given input $X$ in contrast to $SS(X)$. With this point of view the signed subset sum problem becomes precisely equivalent to the injectivity of the augmentation map $\\epsilon: 2^X \\to \\bf{Z}$. \n",
        "\n",
        "This suggests the following question: \n",
        "\n",
        "___What is the computational time-step complexity of deterministically deciding the injectivity of a linear map $T: V_1 \\to V_2$.___ \n",
        "\n",
        " What would it mean if the complexity of injectivity of linear maps was essentially $O(dim(V_1))$, or can it be decreased? \n",
        "\n",
        " What does category theory say about injectivity? \n",
        " \n",
        " Does the strange Yoneda Lemma have any application? I know the logician's find that an important _representation theorem_ for certain categories defined over Sets or something, but I never quite understood it.  \n",
        "\n",
        " [To be continued ... -JHM]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kvwUUjTwlJP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hide\n",
        "\n",
        "# _Review of Irreducibility in Algebraic Geometry_\n",
        "Let's begin with polynomials. If $f=g_1^{a_1} \\cdots g_k^{a_k}$, then what does the factorization say about $V(f)$ (the zero locus)? Obviously that the product vanishes means the factors vanish, so long as that holds true then $V(f)$ becomes the union of the factors $V(g_i)$. This addresses the implicit equation $f=0$. But if we are looking at evaluation/computation, then the evaluation of $f$ becomes factored into a product and requires computation of the factors $g_i$. \n",
        "\n",
        "Vague idea: to relate the degree of $f$ to the complexity of evaluating $f$ ? Different than the _dimension_ of $V(f)$, which again relates to the vanishing set. So this is warning: algebraic geometry is not studying functions but vanishing loci, i.e. the zero sets of polynomials. But computation is about the evaluation of polynomials.\n",
        "\n",
        "# _Improving Algorithms_\n",
        "\n",
        "- finding path: random walk is not good method, but if we know the distance, then we can try greedy algorithms (possibly with backtracking). \n",
        "\n",
        "- lookup in a telephone book: the speedup is permitted only if the telephone book is _ordered_. If the telephone book is initially randomized, then the telephone book needs to first be sorted.\n",
        "\n",
        "-\n",
        "\n",
        "# _Accounting for Computational Complexity of Turing Machines_\n",
        "The Turing machine is dynamic, and we think a proper accounting of the complexity of Turing machines needs account for both time steps (counting a sequence of operations) but also the RAM and spatial requirements of the algorithm. We think this is more realistic, and perhaps more useful for practical computation. \n",
        "\n",
        "We should here record that we do possess the book Blum-X-Shub-Smale on Real Complexity Theory, and we have glanced through the book, but did not really read or learn anything from it. However we do like the style of the book and find it's general subject matter very interesting. It's true that there are unsatisfactory aspects of computation theory, and we think it's probably interesting that numerical analysis and numerical methods have more to say. As the authors say, they hope that complexicty can benefit from topology and analysis, although it has been the subject of logic to date. "
      ],
      "metadata": {
        "id": "lMUlkbBTd8hc"
      }
    }
  ]
}