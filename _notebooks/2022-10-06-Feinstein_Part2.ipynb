{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgfNINMS/qUbZUQ28jGoUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmartel/fp/blob/master/_notebooks/2022-10-06-Feinstein_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deciding Injectivity and C.A.Feinstein's Proposed Negative Solution to P versus NP.\n",
        "> \" We continue to review and reflect on questions suggested by Craig Alan Feinstein's proposed negative solution to P versus NP. We are specifically reflecting on a signed subset sum problem, which is effectively a linearization of the unsigned subset sum. But then the question reduces to deciding the injectivity of a linear map, namely from a large dimensional vector space to a much smaller dimensional vector space. This raises the question of what is the computational complexity of deciding injectivity of linear maps? Is it really ever possible to deterministically decide the injectivity of a map without necessarily running through every evaluation? \"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: false\n",
        "- comments: true\n",
        "- author: JHM[link text](https://)"
      ],
      "metadata": {
        "id": "amcSf3yUe-Eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# _More Remarks on Craig Alan Feinstein's P versus NP argument._\n",
        "In previous articles here and here we have been reviewing Craig Alan Feinstein's proposed solution to the $P$ versus $NP$ problem, and he has been poorly received by the so-called experts. But we continue to find his papers interesting, and this post is mainly about his [_Dialogue Concerning The Two Chief World Views_](https://arxiv.org/pdf/1605.08639.pdf) which contains some interesting remarks on precisely _what_ is the argument being made by CAF that $P \\neq NP$. \n",
        "\n",
        "1. The first point is that given a finite subset $X \\subset \\bf{Z}$, there are indeed exponentially many expressions in $2^X$. This is obvious. But generically there are also $2^{\\#(X)}$ _distinct values_ in the image $\\epsilon: 2^X \\to \\bf{Z}$ where $\\epsilon(Y) = \\sum_{y\\in Y} y$ is the natural addition map defined on subsets $Y \\in 2^X$. \n",
        "\n",
        "This point is made as rebuttal to the objection that if $X$ is bounded a priori, then it's known that there are _polynomial many_ _distinct values_ of the image $2^X \\to \\bf{Z}$. [Reference needed]. We think this is important observation, but one which might require more evidence or proof. Can we more formally prove that _arbitrary_ subsets $X \\subset \\bf{Z}$ almost surely have exponentially many distinct values in the image $\\epsilon: 2^X \\to \\bf{Z}$?\n",
        "\n",
        "If $X$ is in fact a subset of the powers of $2$, namely $$ \\{1, 2^2, 2^4, 2^8\\}$$ then there's large gaps between the elements, and we think it's clear that the image is exponential. What's also interesting in these cases is that it's also evident that there is a _negative solution_ to the SUBSET-SUM problem. \n",
        "\n",
        "And this raises the second interesting point in CAF's paper:\n",
        "\n",
        "2. The algorithms which are competing with the \"meet in the middle\" algorithm (which has $O(2^{n/2})$ time-step complexity) must also have the ability to deterministically return a _negative answer_ to SUBSET-SUM(X) if there are no zero subset sums for the given input $X$. \n",
        "\n",
        "Perhaps it can be argued that generically the solution to SUBSET-SUM is negative, and therefore the exponential search is almost always required to determine negative solutions. It's maybe useful to contrast this with algorithms which are known _a priori_ to have positive solutions, for example linear sort of a list of integers. There are surely many more examples of algorithms which are known a priori to terminate with positive solutions. For example, Hilbert Nullstellensatz as described by [Blum-X-Smale-Shub-et.al.] over the complex numbers. \n",
        "\n",
        "And actually, doesn't it now appear that: the whole question of SUBSET-SUM(X) is precisely about the injectivity of the map $\\epsilon: 2^X \\to \\bf{Z}$. This is because $\\epsilon$ is basically linear and the difference $$\\epsilon(f)-\\epsilon(g) = 0$$ if and only if $\\epsilon(f-g)=0$. But $f-g$ represents a _signed_ symmetric difference, so it's not exactly a solution of SUBSET-SUM.  \n",
        "\n",
        "Idea: perhaps it's possible that the signed subset-sum problem is equivalent to unsigned subset-sum. If we return a signed solution $f-g$ then we can examine whether its a positive solution by looking at the signs of the coefficients. This would only add a linear time-complexity. So maybe we should simply focus on the complexity of injectivity of $2^X$ and its relation to negative solutions. Because any algorithms which competes with MITM algorithm must at least solve this decision problem. \n",
        "\n",
        "\n",
        "# _On the Injectivity of_ $\\epsilon: 2^X \\to \\bf{Z}$\n",
        "\n",
        "The idea of the previous section is to actually relax somewhat and consider the Signed Subset-Sum problem, maybe abbreviated $SS_{\\pm}(X)$ for a given input $X$ in contrast to $SS(X)$. With this point of view the signed subset sum problem becomes precisely equivalent to the injectivity of the augmentation map $\\epsilon: 2^X \\to \\bf{Z}$. \n",
        "\n",
        "This suggests the following question: \n",
        "\n",
        "___What is the computational time-step complexity of deterministically deciding the injectivity of a linear map $T: V_1 \\to V_2$.___ \n",
        "\n",
        " What would it mean if the complexity of injectivity of linear maps was essentially $O(dim(V_1))$, or can it be decreased? \n",
        "\n",
        " What does category theory say about injectivity? \n",
        " \n",
        " Does the strange Yoneda Lemma have any application? I know the logician's find that an important _representation theorem_ for certain categories defined over Sets or something, but I never quite understood it.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kvwUUjTwlJP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are further remarks to be made about CAF's \"Two Worlds\" paper. \n",
        "\n",
        "3. The critics employ an invalid meta-mathematical critique of CAF's proof when they _inaccurately_ reason that \"_if your proof was correct, then [THIS] would be true._\" (A kind of reductio ad absurdum)\n",
        "\n",
        "One such example is this: they say that if we apply CAF's proof to finding solutions in integers of linear equations $$\\sum_i a_i x_i =0 $$ where $a_i\\in \\bf{Z}$ and $x_i \\in \\bf{Z}$, is asymptotically much faster than $O(2^{n/2})$. \n",
        "This is true, but a trivial example since solutions to such equations are very immediately found. For example, set all unknowns to zero except two. Then it's trivial to solve over $\\bf{Q}$ the linear homogeneous equation $ax+by=0$ when $a,b$ are nonzero constant integers. \n",
        "\n",
        "However it's somewhat more interesting for inhomogeneous linear equations over integers. For example, it's not so easy to solve $$2x_1 + 13x_2 =9.$$ Here we enter into the commutative algebra interpretation. For the range of all possible values obtained by $2x_1 + 13 x_2$ is determined by a unique integer (modulo units), namely the greatest common divisor $gcd(13, 2)$. And the determination of gcd's is obtained via the Euclidean algorithm. For example, we have $13=2.6+1$. Therefore $1.13 - 6.2=1$ and $(x_1, x_2)=(-6, 1)$ is a solution in integers. \n",
        "\n",
        "That's clear, so what do the critics say? Well, the image of $2x_1 + 13x_2$ _is_ indeed large (infinite), yet we have found solutions without searching through this infinite maze. This is true, so what's the difference? Feinstein argues that the linear equation over integers admits many algebraic identities, or rearrangements. This is somewhat the key of the euclidean algorithm which reduces the coefficients of the equation into essentially equivalent linear expressions, and ultimately into a trivial equation.\n",
        "\n",
        "Here enters somewhat the idea of the ideal generated by the coefficients over $\\bf{Z}$. The image of the linear integer equation is infinite, but it's highly structured, being determined by the multiples of the generator of the ideal, i.e. the gcd of the coefficients. This structure enables the significant speedup in finding solutions.___And the point is that this reduction is not possible in the (signed) subset-sum problem for finite sets.___ Why? Because the image is much less structured, and not forming any \"ideal\". \n",
        "\n",
        "If we speculate a little, perhaps this is another reason why SUBSET-SUM is so difficult. It's an extremely constrained version of a much easier problem. I.e. if we are allowed to arbitrarily duplicate elements of $X$, then we could basically realize better approximate solutions to zero subset sum.  \n",
        "\n",
        "# _Complexity of Linear Diophantine Equations over $\\bf{Z}$_\n",
        "\n",
        "This relates to a curious point about Euclidean algorithm, namely the backsubstitution that is necessary to return to the first solution. If we are simply studying the decidability problem, then we never need this backsubstitution. This relates to CAF's observation that the equation $$83x_1 + 18x_2=t$$ for example, has a solution if and only if there exists $z\\in \\bf{Z}$ such that $gcd(83, 18).z=t$ where $t$ is the integer target value. With Euclidean we find $gcd(83, 18)=1$ and therefore the equation $83x_1 + 18x_2=t$ is effectively equivalent to the equation $$1.z=t$$ which has trivial solution. So Euclidean algorithm allows us to reduce the equation into a simpler, almost trivial equation. \n",
        "\n",
        "But the point CAF makes, and perhaps it deserves more elaboration, is that the equations defining SUBSET-SUM are not defined over structured rings with Euclidean algorithms. Maybe it suggests a non-algorithm of trying to perform Euclidean algorithm with only coefficients of $\\pm 1$ involved and never really being allowed to re-use an element. It's highly constrained.\n",
        "\n",
        "\n",
        "\n",
        "To conclude, the decidability of integer solutions depends on a reduction, depending on Euclidean algorithm finding the primite generator of the ideal generated by the integral coefficients, to a much simpler equation $$gcd(a_i).z=t,$$ and the decidability question becomes reduced to \"Is the gcd of the coefficients a divisor of the target?\". \n",
        "\n",
        "So after Euclid, everything is reduced to the question _How is divisibility decided_ ? I.e., how to decide the function _\"isDivisible(a,b)\"_. There's a discussion of the decidability [here](https://cstheory.stackexchange.com/questions/16788/whats-the-most-efficient-algorithm-for-divisibility). Also interesting is the relation to Newton's method, sometimes called Newton-Raphson division. \n",
        "\n",
        "Remark. The efficiency of the Newton-Raphson is not immediately relevant to the question, since it efficiently finds a solution, yet our decidability problem is related to determining whether $x=t.a^{-1}$ is integral. The question would then become whether Newton's method is sufficiently accurate to distinguish integral versus rational values!\n",
        "\n",
        "For example, to solve $ax=t$ requires $x=t.a^{-1}$. But the value of $a^{-1}$ can be expressed as finding the zero of the function $0=f(x)=\\frac{1}{x}-a$, which has derivative $f'(x)=\\frac{-1}{x^2}$. Applying Newton's method to finding approximate zeros $x_n$, we find approximate quotient equal to the product $t.x_n$ and we must decide whether $t.x_n$ is _integral_. \n",
        "\n",
        " [To be continued ... -JHM]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9wdi-vionT75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hide\n",
        "\n",
        "# _Review of Irreducibility in Algebraic Geometry_\n",
        "Let's begin with polynomials. If $f=g_1^{a_1} \\cdots g_k^{a_k}$, then what does the factorization say about $V(f)$ (the zero locus)? Obviously that the product vanishes means the factors vanish, so long as that holds true then $V(f)$ becomes the union of the factors $V(g_i)$. This addresses the implicit equation $f=0$. But if we are looking at evaluation/computation, then the evaluation of $f$ becomes factored into a product and requires computation of the factors $g_i$. \n",
        "\n",
        "Vague idea: to relate the degree of $f$ to the complexity of evaluating $f$ ? Different than the _dimension_ of $V(f)$, which again relates to the vanishing set. So this is warning: algebraic geometry is not studying functions but vanishing loci, i.e. the zero sets of polynomials. But computation is about the evaluation of polynomials.\n",
        "\n",
        "# _Improving Algorithms_\n",
        "\n",
        "- finding path: random walk is not good method, but if we know the distance, then we can try greedy algorithms (possibly with backtracking). \n",
        "\n",
        "- lookup in a telephone book: the speedup is permitted only if the telephone book is _ordered_. If the telephone book is initially randomized, then the telephone book needs to first be sorted.\n",
        "\n",
        "-\n",
        "\n",
        "# _Accounting for Computational Complexity of Turing Machines_\n",
        "The Turing machine is dynamic, and we think a proper accounting of the complexity of Turing machines needs account for both time steps (counting a sequence of operations) but also the RAM and spatial requirements of the algorithm. We think this is more realistic, and perhaps more useful for practical computation. \n",
        "\n",
        "We should here record that we do possess the book Blum-X-Shub-Smale on Real Complexity Theory, and we have glanced through the book, but did not really read or learn anything from it. However we do like the style of the book and find it's general subject matter very interesting. It's true that there are unsatisfactory aspects of computation theory, and we think it's probably interesting that numerical analysis and numerical methods have more to say. As the authors say, they hope that complexicty can benefit from topology and analysis, although it has been the subject of logic to date. "
      ],
      "metadata": {
        "id": "lMUlkbBTd8hc"
      }
    }
  ]
}