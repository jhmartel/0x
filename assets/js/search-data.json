{
  
    
        "post0": {
            "title": "Remarks on Craig Alan Feinstein's papers and P versus NP.",
            "content": ". P versus NP has long complicated backstory, see Stephen Cook&#39;s official statement of the problem for the Clay Millenium Problems. https://www.claymath.org/millennium-problems/p-vs-np-problem . We have also recently been reviewing Craig Alan Feinstein&#39;s papers on the problem, especially his article https://arxiv.org/abs/1605.08639. . Now must record that Feinstein&#39;s work is somewhat controversial, although we find it quite interesting. For example here is a critique of one of Feinstein&#39;s earlier papers, notice the year 2007, https://arxiv.org/abs/0706.2035. Let us remark that controversy is very important for science, and we strongly believe in the need for creative controversy. So we follow the debate with much interest. . [Insert description of Subset-Sum Problem over an ordered set of length N] . [Insert Feinstein&#39;s argument to reduce the complexity to $O(2^{N/2})$] . Here Feinstein claims (and his critics argue that he asserts without proof) that $O(2^{N/2})$ is a lower bound for the worst-case complexity of the SUBSET-SUM problem. The problem however is that computer science and complexity ``theorists&quot; have no strategies for proving lower bounds! . So it appears to us that if $P neq NP$ (as is widely expected), then it&#39;s formally unprovable (because of the unprovability of lower bounds). In other words, as Feinstein argues, the critics can always say &quot;Well what if one day an algorithm is discovered by some new method that is much faster $ ldots$.&quot; And what&#39;s the answer to such a &quot;what if?&quot; statement? . If we review S. Cook&#39;s statement of the P vs NP, we see very few references to lower bounds, and it&#39;s indeed the central problem in the area. . There is another comment to make regarding the effect of so-called quantum computers and quantum algorithms. With QCs, complexity is measured by the number of quantum gates in the circuit realizing the algorithm. But is the complexity in QC really comparable to complexity in classical algorithms? . Firstly, the quantum algorithms are not deterministic, i.e. they have a nontrivial probability of giving the wrong answer as output. For example, Grover&#39;s search algorithm is quite famous (almost as famous as Schor&#39;s factorization algorithm), and given a function $f$ on an unstructured set of $N$ elements, finds the input which satisfies a given output in $O( sqrt{N})$ evaluations. We are being vague here, but the wikipedia article on Lov Grover and his algorithm has useful links: . e.g. https://arxiv.org/abs/quant-ph/0109116 and https://en.wikipedia.org/wiki/Grover&#39;s_algorithm . . So we can finish this article here. What&#39;s our main point? That IF $P neq NP$, then there will never be a proof, because lower bounds on complexity are not possible. Critics will always say there is a possibility of a magic algorithm that speeds up the process. And the confusion being imported by quantum computers encourages the magical optimism, &quot;Maybe there will even be a quantum algorithm tomorrow which is much faster than anything known!&quot; . So meanwhile we focus on other problems and would not recommend anybody work on P versus NP. .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/09/03/PversusNP.html",
            "relUrl": "/fastpages/jupyter/2022/09/03/PversusNP.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Summer 2022 is over. That was fast.",
            "content": ". Wow, I&#39;ve been busy in real world and have paused some writing projects. Of course there is still ALOT to be said. But I&#39;m searching for opportunities, and have travelled to Calgary, AB, to see what&#39;s happening here. Again, I am travelling to make a decision about where to reside. . What&#39;s been on my mind? . The Qiskit and quantum computing has reached the following stage: the mathematical theory is perfect but incomplete, unsatisfactory, and the physical interpretation is not sufficient. [Insert comment: there is tensorial categorical issue with the quantum definitions of observables and projections of variables, e.g. the Einstein-Podolsky-Rosen (EPR) paradox and interpretations. This tensorial definition is provided by Gromov in his ERGO brain papers and specifically his paper &quot;In Search of Structure Part 1: Entropy&quot;.] | Again there is alot to say. . The further logical incompleteness of quantum is reflected in Neil Bohr&#39;s famous proclamation that &quot;Those who are not shocked when they first come across quantum theory cannot possibly have understood it.&quot; . Or Richard Feynman &quot;If you think you understand quantum mechanics, you don’t understand quantum mechanics. &quot; . Or Werner Heisenberg&#39;s &quot;Not only is the Universe stranger than we think, it is stranger than we can think.&quot; . Now let me say that these statements are True, but they are also True evidence that Quantum is unacceptable. It is admittedly inconsistent in its very foundations, at its root, genesis, and zero block. So why is quantum accepted at all? Because there is something useful about it. So let us continue to be guided by pragmatism. What&#39;s useful? . Another remark on Bohr&#39;s claim. There is something shocking in quantum physics, and this is well documented in the historical accounts of Bohr, Planck, and Einstein. And what is the shock? It is found in Bohr&#39;s planetary two-body model of the hydrogen atom which violates classical physics. More specifically, the apparent non-radiation of Bohr&#39;s planetary model of the hydrogen atom is the shocking postulate. But the apparent nonradiation is an ad hoc postulate, and which is contradictory to the classical dynamics. Quantum is shocking for those who try to explain the atom via classical electrodynamics, e.g. the Maxwell field theory and Lorentz force laws. However as Weber and Gauss understood from Ampere&#39;s Newtonian theory of electricity, there are many more dynamic and stable configurations in Weber&#39;s relational action-at-a-distance model than the standard Lorentz-Maxwell field framework. . So we view Niels Bohr&#39;s above statement as True. Indeed for the student to accept the mathematical premise of Bohr&#39;s quantum theory is to positively reject the classical physics, because Bohr does not resolve the problem of the apparently non-radiating hydrogen atom . Rather he takes this apparent non-radiation as a fundamental postulate, regardless of its apparent irreconciliability with the basic classical theory of two-body electronic systems. . What is the alternative? . The alternative was proposed by Wilhelm Weber before Planck, and before Bohr. The Weber planetary model of the atom is stable, and does resolve the inherent contradictions of Bohr&#39;s model. In otherwords, Weber restores the planetary atom to classical physics. . Update: I&#39;ve been looking for work since around November 2021. Today is June 2022, so that makes approximately seven months unemployed. I work labour jobs on the side, but that&#39;s just to make ends meet. I will say I consider it quite demoralizing and humiliating to be ready, willing, and able to work a job, and yet having no job. I&#39;m sorry my excuse is I&#39;m too talented and too skilled and too good to be working labour like a beast. The consequences of this choice are this: instead of readily earning 32$/hour as entry level wage, I&#39;m poor and effectively homeless. But I&#39;m a dreamer, and i&#39;m called to do physics and mathematics, and I&#39;ll do it until I die. . I have recently updated my special relativity paper titled &quot;On Critical Foundations of SR and Light Propagation in Vacuum&quot;. I actually think the paper is quite readable at this stage. The main idea is simply a modification of Fizeau&#39;s spinning wheel experiment following a suggestion of Ralph Sansbury which -- at least to my mind -- would be decisive in demonstrating that &quot;light&quot; is really something that &quot;travel through space&quot;. | Remark. In Maxwell&#39;s treatise, especially the final chapters of Vol II, and his review of action-at-a-distance theories of Gauss, Weber, Neumann, etc., Maxwell is clear that it&#39;s inconceivable for him to imagine light as anything other than a &quot;particle&quot; which necessarily travels through space. This presumption then leads to the confused issue of whether light is particle or wave or somehow both? (I.e. Bohr complementarity). . My new goal is to acquire the IBM Qiskit Developer Accreditation. This involves a 60 question multiple choice exam in August. . Why? . Qiskit and Quantum Computing (QC) seems like perfect opportunity to prove both my math and programming skills. Honestly I don&#39;t think anybody can understand Qiskit or the quantum gates without strong linear algebra. . | Qiskit and especially Qiskit Pulse are excellent testing grounds for fundamental physics. Indeed my hypothesis is that decoherence and quantum errors are going to remain a persistent problem in QC. It would be amazing to actually derive/prove the persistence of these errors via Weber&#39;s electrodynamics. . |",
            "url": "https://jhmartel.github.io/fp/2022/09/02/_08_29_LongLostUpdate.html",
            "relUrl": "/2022/09/02/_08_29_LongLostUpdate.html",
            "date": " • Sep 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "An Idea on Constructing Counterexamples to Larry Guth's Sponge Problem using Jammed Packings.",
            "content": ". In this post I&#39;m simply calling attention to a possible counterexample to Larry Guth&#39;s Sponge Problem. I have other interests at the moment, so I know I won&#39;t be working on this problem for awhile and I leave my notes here in case somebody else wants to complete it. This same idea, in even less developed form, was posted on Mathoverflow here: https://mathoverflow.net/questions/381172/status-of-larry-guths-sponge-problem. . See also: https://www.github.com/jhmartel/Sponge/ . The idea is how to use jammed disk packings to construct counterexamples to Larry Guth&#39;s Sponge Problem. These will be packings which have a large linear diameter but have arbitrarily small volume (i.e. arbitrarily low &quot;density&quot; in the terminology of disk packings). The most interesting constructions of arbitrarily small density packings are constructed by Werner Fischer (see references in the above Mathoverflow post). Our idea is that from these packings we can construct open subsets $V$ with arbitarily small volume and which do not admit any expanding-embeddings into the unit disk. . A negative answer to Guth&#39;s Sponge Problem implies that Guth&#39;s proof of the width-volume inequalities [ref] cannot be simplified or deduced from width-volume inequalities on the unit disk. In otherwords, Guth&#39;s very interesting proof [ref] remains also a necessary proof. . The above idea is not rock solid. What we have not proven is the following statement: . Unproven Lemma: Let $P$ be a disk packing of an open subset $U$ of ${ bf{R}}^n$. If $P$ is rigid relative to $U$, then $diam(f(U)) geq diam(U)$ for all expanding-embedding maps $f: U to { bf{R}}^n$. . Here $diam$ refers to the linear diameter of $U$ as a subset of ${ bf{R}}^n$. We say $P$ is rigid relative to $U$ if for every force load $b$, the packing cannot be rearranged without some outward-expansion on the boundary $ partial U$. . So the rough idea is that : force loads $b$ cause a reaction in the packing, and what&#39;s interesting is whether the load forces the domain $U$ containing $P$ to increase in volume. . However what can possibly happen is that the force load $b$ causes an expansive rearrangement of the disks in the packing, and where the volume of $U$ increases, but potentially the disks can then be rearranged in a configuration with smaller linear diameter. . Perhaps the correct lemma is this: If $P$ is a disk packing in $U$, and $P$ is rigid relative to $U$, and if $U$ has minimal possible linear diameter containing $P$ (this implies $U$ is a subset of the smallest disk containing $P$), then $diam(f(U)) geq diam(U)$ for every expanding-embedding $f$ of $U$. . What we are seeking is some sort of monotonicity property. However it&#39;s possible that monotonicity can hold only for limited types of open subsets, for example those $U$ containing the packing $P$ and the minimal disk containing $P$. (I know there&#39;s alot of definitions here, some implicit, and I&#39;m rushing.) . These ideas are related to the recent results that &quot;Dionysian packings of arbitrarily small density can be mechanically rigid&quot; [ref]. [https://www.youtube.com/watch?v=M62Fewh-dHw] .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/08/31/GuthSpongeCounterexamplesViaPackings.html",
            "relUrl": "/fastpages/jupyter/2022/08/31/GuthSpongeCounterexamplesViaPackings.html",
            "date": " • Aug 31, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Preprint on Closing Steinberg Symbols in Mapping Class Group with Applications to Constructing Spines.",
            "content": ". Today we announce that our preprint on &quot;Closing Steinberg Symbols in Mapping Class Groups&quot; has achieved a basically stable form. The article is available at [https://www.github.com/jhmartel/MCG]. . There is a proof missing in one of the lemmas, but otherwise the paper can be read as is. We don&#39;t solve all the problems we wanted to, and we cannot yet produce an explicit spine for the genus two mapping class group, for example, but we&#39;ve written what we can. . Comments are welcome at jhmartel {at} proton {dot} me. .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/08/30/MCG_CS_Article_Available.html",
            "relUrl": "/fastpages/jupyter/2022/08/30/MCG_CS_Article_Available.html",
            "date": " • Aug 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Remarks on Ahlfor's Measure Conjecture.",
            "content": ". A recent MO post has prompted us to express something about our views regarding $ Gamma$ actions and $ Gamma$-invariant Borel measures and proper discontinuous actions, $E Gamma$ spaces, etc.. The conjecture is rather famous, especially in three dimensions. But since our PhD thesis (2019) we have some nonstandard approaches to the problem, which we will elaborate below. . It&#39;s not easy to describe the current state of the conjecture, depending on several implications and reductions to other conjectures, especially in three-dimensions. A good reference is here. . The above MO question is interesting in that it&#39;s asking about the corresponding conjecture in higher dimensions. I sent Alex Kolpakov a message asking him about the question because he&#39;s the person who I know knows the most about four-dimensional manifolds. . The setting for Ahlfor&#39;s problem begins with a discrete finitely generated group $ Gamma &lt; Isom( bf{H}^{n+1})$ of some hyperbolic space. The discreteness of the group representation implies that $ Gamma$ acts proper discontinuously on the hyperbolic space. The hyperbolic space also has a canonical volume measure. We say the representation has finite covolume if the quotient $ bf{H} / Gamma$ has finite volume. In otherwords $ Gamma$ represents the fundamental group of a hyperbolic orbifold $X:= bf{H}^{n+1} / Gamma$, and the action $$ Gamma times bf{H}^{n+1} to bf{H}^{n+1}$$ is the universal covering action $$ pi_1 times tilde{X} to tilde{X} $$ where $ tilde{X} approx bf{H}^{n+1}$. . Now given an action of $ Gamma$ on $ bf{H}$ we next need to define the topology of the &quot;boundary at-infinity&quot;. . To review: geodesics in hyperbolic space $ bf{H}^{n+1}$ diverge exponentially in every direction. The hyperbolic space is expansive at-infinity. And even though the Poincare disk might naively appear to be a small bounded disk, this disk in fact contains an infinitely large expansive world. . . We need carefully define the topology to properly define the limit points $L_ Gamma$ of a $ Gamma$-orbit $ Gamma.pt$. In fact some confusion arises because the topology on the visual sphere at-infinity is not always made explicit by authors. Naively the majority definition of the topology on the visual sphere consists of open subsets which are presumed to contain ideal points $ xi$ at infinity. This perspective implicitly assumes that the boundary disk is the natural compactification of the hyperbolic space $ bf{H}$. . But actually the correct definition of these ideal points is via horoballs and the horofunctions. A point at-infinity $ xi$ really corresponds to a $1$-Lipschitz function $b_ xi: bf{H} to bf{R}$, and specifically the pole at negative infinity $- infty$ of $b_ xi$. The level sets of the horofunction $b_ xi$ are horospheres centred at $ xi$. . So we have two definitions of the points at-infinity, and two different topologies. For example: . Horofunction definition of limit points at-infinity: the point $ xi$ is a limit point of the orbit $ Gamma .pt$ with respect to the horoball topology if the values of the orbit $b_ xi( gamma.pt)$, $ gamma in Gamma$, accumulate near the negative infinity pole. I.e. iff there exists an infinite subsequence such that $b_ xi( gamma.pt)$ diverges to $- infty$. . | &quot;Open&quot; topology definition of limit points: the visual point $ xi&#39; in partial_ infty bf{H}$ is a limit point of the orbit $ Gamma .pt$ w.r.t. the open topology if every open neighborhood $U$ of $ xi&#39;$ in $ bf{H} cup partial_ infty bf{H}$ has a non trivial intersection with the orbit $U cap Gamma.pt neq emptyset$. . | The problem with the second definition, which admittedly the majority topology in the literature, is that the point at-infinity has not been defined, and no criteria is given for testing whether the &quot;point&quot; $ xi&#39;$ is contained or not contained in the open subset $U$ of $ bf{H}$. . Therefore in the horofunction topology, the ideal points at-infinity are not &quot;points on a sphere&quot; they are instead $1$-Lipschitz functions obtained as a natural limit of the distance functions $d(x,y)$. The point at infinity $ xi$ is instead the negative infinity pole of the corresponding horofunction $b_ xi$. . Naively one says there is a circle at infinity, namely the visual sphere $ partial_ infty bf{H} approx bf{S}^1$. With respect to the hyperbolic metric, of course the boundary is infinitely far away. Now we need to topologize the visual boundary such that the $ Gamma$-action on $ bf{H}$ extends to a continuous topological action on $ bf{H} cup partial bf{H}$. . One method of topologizing the boundary is via the visual angle metric. Another interesting topology on the boundary is the Busemann topology. The key fact necessary is that the topology of $ partial_ infty$ needs be defined such that the group $ Gamma$ acts continuously on $ bf{H} cup partial_ infty bf{H}$. . [Edit] If we fix a basepoint $pt$ in $ bf{H}$, then one naturally defines the boundary at-infinity $ partial_ infty bf{H}$. The boundary at-infinity is topologized with the visual angle metric, and it&#39;s a standard fact that the boundary at-infinity is a topological sphere $ partial_ infty bf{H}^{n+1} approx bf{S}^n$. The sphere at-infinity does not have a $ Gamma$-invariant metric since $ Gamma$ actions by translating the basepoint $pt$. It is a standard fact that the sphere at-infinity has a well-defined $ Gamma$-invariant metric modulo quasi-isometries. The definition of quasi-isometries will not be further developed in this article, except to say this: that the topological sphere at-infinity is a difficult object to study with respect to the $ Gamma$ action. . Standard fact: Under assumptions [insert] the topological action $ Gamma times bf{S}^n to bf{S}^n$ is ergodic with respect to the class of $n$-dimensional Lebesgue measures on $ bf{S}^n$. In other words, if $ lambda$ is the $n$-dimensional Lebesgue measure on the topological sphere at-infinity $ bf{S}^n$, then the only $ lambda$-measurable $ Gamma$-invariant functions $f: bf{S}^n to bf{R}$ are constant a.e.. [Insert reference to Mostow&#39;s proof, or Thurston&#39;s book]. Roughly speaking, the topological $ Gamma$-action on $ bf{S}^n$ is very complicated, acting with sources and sinks and strange dynamics which are not measured by the $n$-dimensional Lebesge measure $ lambda$. . To finally state Ahlfor&#39;s measure conjecture we need to define the limit set of a $ Gamma$-orbit. Informally we can define $L_ Gamma subset partial_ infty bf{H}$ as the accumulation points of the orbit $ Gamma. pt$. . Ahlfor&#39;s Measure Conjecture: If $ Gamma, X$ satisfy the assumptions (i) and (ii), namely having finitely many ends and finite volume, then the limit set $L_ Gamma$ is either a zero or full measure subset of $ partial_ infty tilde{X}$. In otherwords, the limit set $L_ Gamma$ is either equal to $ bf{S}^n$ or has vanishing $n$-dimensional measure. . In most circumstances, the action of $ Gamma$ on the visual sphere is ergodic with respect to the natural Lebesgue measure on the sphere. However the example in the above MO post shows the action is not necessarily ergodic in higher dimensions for certain groups. However in this counter-example to ergodicity the group $ Gamma$ is finitely-generated but not finitely-presentable. . Now we must admit that the structure of the limit points $L_ Gamma$ is very difficult to comprehend. Many experts have been involved in producing graphics and simulations of these limit sets, especially for Kleinian and Fuchsian groups. Mumford&#39;s book &quot;Indra&#39;s Pearls&quot; is also relevant here. . Now our approach to Ahlfor&#39;s Measure Conjecture is to first realize that the visual boundary is not the correct boundary for the group action $ Gamma times bf{H} to bf{H}$. We view the visual boundary as extremely unnatural with respect to the group $ Gamma$. Specifically, the problem is that the $ Gamma$ action is not proper discontinuous on the visual sphere. This means that point orbits on the sphere are not well separated by the $ Gamma$ action. In otherwords, the visual boundary is very confusing because group elements which otherwise are very distinct can have similar actions on the visual boundary, and their distinct identities become confused. . # generated by the letters, a, A, b, B. Eventually we use a dictionary to convert these # letters and words into numerical matrices. import numpy as np #from numpy.linalg import matrix_power gen=[&#39;a&#39;,&#39;A&#39;,&#39;b&#39;,&#39;B&#39;] inverse_list=[[&#39;a&#39;,&#39;A&#39;], [&#39;A&#39;, &#39;a&#39;], [&#39;b&#39;, &#39;B&#39;], [&#39;B&#39;, &#39;b&#39;]] def test(x): a, b = x if (x in inverse_list): return False else: return True def f(x): l=[] level, word_list, tail = x new_level=level+1 for letter in gen: if test([tail, letter])==True: new_word_list=word_list+[letter] new_tail=letter l=l+[[new_level, new_word_list, new_tail]] else: pass return l print(f([1,[&#39;B&#39;, &#39;a&#39;],&#39;a&#39; ])) # Below we print the ends. seed=[0, [], &#39;&#39;]; def ends(x): m, seed = x sample_list=[seed]; for n in range(m): new_list=[] for x in sample_list: new_list=new_list+f(x) sample_list=new_list return [len(sample_list), sample_list] print(ends([4, seed])) . [[2, [&#39;B&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [2, [&#39;B&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [2, [&#39;B&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;]] [108, [[4, [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;]]] . # into a matrix product. We evaluate these products over the ends of the tree # generated by the words in the letters a, A, b, B. import numpy as np from numpy.linalg import matrix_power as mp # S is a typical pseudo-anosov type transformation # T was randomly chosen, and might not be a good example. S=np.array([[2,1], [1,0]]); T=np.array([[-1,-1], [2,1]]); Id=np.array([[1.0,0.0], [0.0,1.0]]); # matrix_power is abbreviated mp. data={&#39;a&#39; : S, &#39;A&#39; : mp(S, -1), &#39;b&#39; : T, &#39;B&#39; : mp(T, -1) }; def bracket(w): x, y = w return (x@y)@(mp(x,-1)@mp(y, -1)) ## the @ symbol represents matrix multiplication, i.e. np.matmul() print(&quot;The commutator of S and T is:&quot;, bracket([S,T])); print(&quot;The commutator of S and S is trivial:&quot;, bracket([S,S])) # matmul is the numpy function for matrix multiplication. def product(word_list): m=Id; for word in word_list: m=data[word]@m; return m sample_word_list=[&#39;a&#39;, &#39;B&#39;, &#39;A&#39;, &#39;b&#39;]; swl1=[&#39;a&#39;, &#39;b&#39;, &#39;A&#39;, &#39;b&#39;] # print(product(swl1)); # print(product(sample_word_list)) . The commutator of S and T is: [[-5. -3.] [-3. -2.]] The commutator of S and S is trivial: [[1. 0.] [0. 1.]] . e=ends([5, seed]); max_index=e[0]; matrix_list=[]; def maxFunc(x): return np.abs(x).max(); for n in range(max_index): matrix_list=matrix_list+[product(e[1][n][1])] matrix_list.sort(key=maxFunc, reverse = True) def rpl(x): return (maxFunc(x)**(-1))*x def quad(x): return np.matmul(x.transpose(), x) # looks at the first largest elements. We should keep those terms which # are of similar order of magnitude as det(rpl(x)). for x in matrix_list[0:10]: print(&quot;The matrix product is:&quot;, x); # print(&quot;The sup norm of the matrix product is:&quot;, maxFunc(x)); # print(&quot;Renormalized projectivization:&quot;,rpl(x)); # print(&quot;The determinant of the renormalized projectivization is:&quot;, np.linalg.det(rpl(x))); print(&quot;The log-absolute-determinant of the renormalized projectivization is:&quot;, np.log( np.abs( np.linalg.det(rpl(x)) ) ) ); print(&quot;The quadratic form Transpose(x)*x of x=rpl(x) is:&quot;, quad(rpl(x)) ); eigen=np.linalg.eig(quad(rpl(x))) # print(&quot;The eigendistribution: is:&quot;, eigen); print(&quot;The log-absolute value of the near zero eigenvalue is:&quot;, np.log(np.abs( eigen[0].min() )) ) min_index=eigen[0].argmin() max_index=eigen[0].argmax() print(&quot;The approximate limit point is:&quot;, eigen[1][max_index] ); # print(&quot;The rank-one quadratic form represented by the limit point is:&quot;, (eigen[1][max_index])@(eigen[1][max_index].transpose()) ) print(&#39; n&#39;) ## N.B. the limit point is the eigenvector corresponding to the nonzero eigenvalue. ## we would like to retain only those elements such that floor(log|det(x)|) is maximized. ## this means considering only those words whose renormalized projectivizations are of the same ## order of magnitude, i.e. of the same maximal &quot;dimension&quot;. ## TO DO: quantify the uncertainty in the approximate limit point. . The matrix product is: [[ 29. 12.] [-75. -31.]] The log-absolute-determinant of the renormalized projectivization is: -8.634976227072784 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.14951111 0.4752 ] [0.4752 0.19644444]] The log-absolute value of the near zero eigenvalue is: -17.567056648013256 The approximate limit point is: [ 0.92414737 -0.38203618] The matrix product is: [[-29. -12.] [ 75. 31.]] The log-absolute-determinant of the renormalized projectivization is: -8.634976227072784 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.14951111 0.4752 ] [0.4752 0.19644444]] The log-absolute value of the near zero eigenvalue is: -17.567056648013256 The approximate limit point is: [ 0.92414737 -0.38203618] The matrix product is: [[ 31. 12.] [-75. -29.]] The log-absolute-determinant of the renormalized projectivization is: -8.634976227072784 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17084444 0.4528 ] [0.4528 0.17511111]] The log-absolute value of the near zero eigenvalue is: -17.567056648013256 The approximate limit point is: [ 0.93268339 -0.36069612] The matrix product is: [[-31. -12.] [ 75. 29.]] The log-absolute-determinant of the renormalized projectivization is: -8.634976227072784 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17084444 0.4528 ] [0.4528 0.17511111]] The log-absolute value of the near zero eigenvalue is: -17.567056648013256 The approximate limit point is: [ 0.93268339 -0.36069612] The matrix product is: [[-31. -13.] [ 74. 31.]] The log-absolute-determinant of the renormalized projectivization is: -8.608130186408527 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17549306 0.49251278] [0.49251278 0.206355 ]] The log-absolute value of the near zero eigenvalue is: -17.539682136925283 The approximate limit point is: [ 0.92231631 -0.38643579] The matrix product is: [[ 31. 13.] [-74. -31.]] The log-absolute-determinant of the renormalized projectivization is: -8.608130186408527 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17549306 0.49251278] [0.49251278 0.206355 ]] The log-absolute value of the near zero eigenvalue is: -17.539682136925283 The approximate limit point is: [ 0.92231631 -0.38643579] The matrix product is: [[70. 29.] [29. 12.]] The log-absolute-determinant of the renormalized projectivization is: -8.496990484098841 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17163265 0.48530612] [0.48530612 0.20102041]] The log-absolute value of the near zero eigenvalue is: -17.310726354950265 The approximate limit point is: [ 0.92387953 -0.38268343] The matrix product is: [[-41. -17.] [ 70. 29.]] The log-absolute-determinant of the renormalized projectivization is: -8.496990484098706 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.34306122 0.55653061] [0.55653061 0.23061224]] The log-absolute value of the near zero eigenvalue is: -17.447393627661846 The approximate limit point is: [ 0.92382689 -0.38281051] The matrix product is: [[ 41. 17.] [-70. -29.]] The log-absolute-determinant of the renormalized projectivization is: -8.496990484098706 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.34306122 0.55653061] [0.55653061 0.23061224]] The log-absolute value of the near zero eigenvalue is: -17.447393627661846 The approximate limit point is: [ 0.92382689 -0.38281051] The matrix product is: [[-12. 29.] [ 29. -70.]] The log-absolute-determinant of the renormalized projectivization is: -8.496990484098877 The quadratic form Transpose(x)*x of x=rpl(x) is: [[ 0.20102041 -0.48530612] [-0.48530612 1.17163265]] The log-absolute value of the near zero eigenvalue is: -17.31072635129126 The approximate limit point is: [-0.38268343 -0.92387953] . # To do: are two-generator subgroups sufficient to enumerate all the limit points? # To do: need collect approximate limit points and attach a confidence value, i.e. rank by how small the smallest eigenvalue is in quad(rpl(x)). . What&#39;s the python code say? . First we need to generate the ends of a tree. That is, the ends of a Cayley graph. Why? Because the limit points correspond to divergent sequences of group elements. We see that a sequence diverges iff the sup norm of the matrices diverges to infinity, i.e. some element must be diverging. Here we generate the ends of the tree &quot;abstractly&quot;, and then replace with actual matrix elements $S, T$ using a python dictionary. . Second, given divergent sequences we take the renormalized projective limit (RPL) of this sequence. This is inspired by H. Furstenberg&#39;s paper &quot;On Borel&#39;s Density Theorem&quot; The idea of taking the RPL is based on the natural ambient compactification of $ bf{H}$ when viewed in Voronoi&#39;s Cone model of hyperbolic space. That is, the RPL naturally defines the limit point as lying on the visual sphere at-infinity. . When we study RPL&#39;s with near zero determinants, then we find approximate limit points of the orbit. In the above computations, we restrict ourselves to two-generator subgroups of $ Gamma$. Note: a ping-pong argument should tell us that the limit points of a two-generator subgroup accumulate in a Cantor set at-infinity. .",
            "url": "https://jhmartel.github.io/fp/2022/06/16/AhlforsConjecture.html",
            "relUrl": "/2022/06/16/AhlforsConjecture.html",
            "date": " • Jun 16, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Ampere Birkeland Currents.",
            "content": ". This article is an extension of Don Scott&#39;s work on modelling Birkeland currents. Search for his articles here. These Birkeland currents (BCs) are cosmic low beta plasma discharges delivering electric current along filaments in space. We recall that $ beta$ is the ratio of the plasma thermal pressure $p=Nk_B T$ with the so-called &quot;magnetic pressure&quot; $B^2/2 mu_0$. So low beta means that so-called magnetic pressure is the domainant force in the interaction. . Professor Scott&#39;s model is derived from Maxwell&#39;s equations and Lorentz&#39; force law. Our goal is to reconsider his model in light of Ampere&#39;s electrodynamics. Thus the subject of this article is ABC, or Ampereian Birkeland Currents. . Ampere&#39;s Force Law . Ampere&#39;s investigations in 1820-1826 led to his proposing a force law between two current elements $I_1 ds_1$ and $I_2 ds_2$. The current element has a scalar $I$ which represents the net current intensity, and $ds$ is a differential vector element representing arc length along the circuit. The magnitude of $I$ typically represents the current intensity, while the magnitude of $ds$ represents the current velocity. . Alternatively we could follow Weber and write $I ds = q dv$. That is, an electric current element is essentially equivalent to charge in motion, i.e. a charge $q$ in velocity $v$. . Ampere determined that the force between two current elements was radial and proportional to the current intensity and to current velocities. Thus the force $F$ was proportional to $I_1 I_2 ds_1 ds_2$. Ampere discovered the inverse square distance proportionality. And moreover determined the precise coefficient of proportionality. In cgs units Ampere&#39;s force law is expressed: . $$ F_{~I_2 ds_2 ~~ text{on} ~~ I_1 ds_1 } =F_{21}= - I_1 I_2 frac{ hat{r}_{12}}{r_{12}^2} [ 2(ds_1 cdot ds_2) - 3 ( hat{r}_{12} cdot ds_1) ( hat{r}_{12} cdot ds_2) ] =-F_{12} .$$ . The following code uses Ampere&#39;s formula: . def dot(v1, v2): vx1, vy1, vz1 = v1 vx2, vy2, vz2 = v2 return vx1*vx2+vy1*vy2+vz1*vz2 def scalar(a, v1): vx1, vy1, vz1 = v1 return [a*vx1, a*vy1, a*vz1] ## Ampere&#39;s Force def F(state1, state2): x1, y1, z1, vx1, vy1, vz1 = state1 x2, y2, z2, vx2, vy2, vz2 = state2 v1=[vx1,vy1,vz1] v2=[vx2,vy2,vz2] I1=dot(v1,v1)**0.5 I2=dot(v2,v2)**0.5 rho_12 = ((x2-x1)**2+(y2-y1)**2+(z2-z1)**2)**0.5 rhat12 = scalar(rho_12**-1, [x1-x2, y1-y2, z1-z2]) coefficient = -I1*I2*(2*dot(v1,v2)-3*dot(rhat12, v1)*dot(rhat12, v2))*(rho_12)**-1 return [x1,y1,z1]+scalar(coefficient, rhat12) s0=[0,0,0,.1,.7,0] s1=[1,0,0,0.9,0,0] print(F(s0,s1)) print(F(s1, s0)) . [0, 0, 0, -0.05727564927611038, 0.0, 0.0] [1, 0, 0, 0.05727564927611034, 0.0, 0.0] . Ampere&#39;s force law has several predictions. . Parallel collinear current elements repel; | Reversing a parallel colinear current attracts by 2/3rd of the repelling force; | Parallel adjacent current elements attract; | Parallel oppositely oriented current elements repel. | . These can all be tested via various samples. . s0=[ 0,0,0, 0,0,1 ] s1=[ 0,0,2, 0,0,1 ] s3= [ 0,0,3, 0,0,-1 ] print(F(s0,s1)) print(F(s1,s0)) print(F(s0,s3)) print(F(s3,s0)) # Parallel Adjacent Current Elements Attract s2=[ 1,0,0, 0,0,1] print(F(s0, s2)) . [0, 0, 0, 0.0, 0.0, -0.5] [0, 0, 2, 0.0, 0.0, 0.5] [0, 0, 0, -0.0, -0.0, 0.3333333333333333] [0, 0, 3, -0.0, -0.0, -0.3333333333333333] [0, 0, 0, 2.0, -0.0, -0.0] . What is a Birkeland current? . Here we imagine a direct current through space which is being conducted in a plasma cylinder. We cannot assume that the cylinder is rigid and strictly right angled throughout, i.e. the Birkeland currents will possibly be radially contracting. We do not wish to assume rotational symmetry around the central axis, although it is convenient in some settings. For example, two doubly infinite parallel filaments will attract/repel depending on their relative orientations. This interaction contributes to the internal potential energy, and the filaments will attract/repel towards a lower energy state. . . Note: in the above right hand image, the force between the Birkeland currents is probably computed according to Coulomb&#39;s formula, and not Ampere&#39;s. So we would expect the force to differ from that graph. . We are trying to find the mathematical equations for the Birkeland currents. We model the currents as a cylindrical configuration of plasma ions in motion, and deliverying a sustained direct current. Therefore we want to know something about the distributions $j$ of electrical charges in motion satisfying: . $$ int_{area} mathbf{j} cdot mathbf{n} dS=I=constant,$$ where the integral is defined over a two-dimensional cross-section of the cylinder. . Now we follow Scott&#39;s analysis, namely that the Birkeland currents are ideally suited to satisfy the minimum total potential energy principle. In otherwords, the stable ground state of a Birkeland current is that with minimal internal energy. In the Ampere perspective, we have charges in motion and the primary dominant force is Ampere&#39;s. Now we must proceed cautiously: if we follow Scott, then the primary force is $F=q(E+v times B)$ and the internal potential energy is measured by the force $j times B$. Thus Scott&#39;s interpretation of &quot;force free field aligned currents&quot; leads to the equation $$j times B =0$$ throughout the current. Moreover Maxwell&#39;s equation says $ nabla times B = mu j$, and therefore $$( nabla times B) times B =0.$$ This leads to the hypothesis (&quot;ansatz&quot;) that $$ mu j = alpha B.$$ Moreover Scott discovered that if we assume $B_r=0$, then we find $B_z$ satisfies a zero order Bessel equation in the variable $r$. . The above gives an interpretation of the Birkeland current from the standard Lorentz force and Maxwell equation perspective. But we are proceeding with strictly Ampere&#39;s force law as the only force acting on the current. Therefore we must carefully think about the meaning of minimal total potential energy in the current. . [To be continued] .",
            "url": "https://jhmartel.github.io/fp/2022/05/23/Weber_Birkeland_Currents.html",
            "relUrl": "/2022/05/23/Weber_Birkeland_Currents.html",
            "date": " • May 23, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Eddington's May 29, 1919 Eclipse.",
            "content": ". As the reader of this blog will know, the author has an interest in fundamental physics, and is especially critical of the standard early 20th century methods. Our purpose in this article is to review some basic elements of Eddington&#39;s 1919 expedition to observe the possible gravitational deflection of starlight. The 1919 eclipse is a key experimental pillar of general relativity (GR). . Here is an interesting physics stackexchange question about the correct procedure used in the Eddington 1919 observation. . It was argued by the late Nasa Engineer and physicist Edward Dowdye Jr. (RIP) that Eddington&#39;s assumption of there being no refractive medium near the solar &quot;surface&quot; (which is a questionable assumption) is incorrect and invalidated by experiment. As known today, the Sun has a plasma limb (of approximately [insert ref] dimension). . Modern GR apologists would argue that many more experiments have been performed, and that we need judge the theory by it&#39;s best experiments. Then proceeds a list of complicated papers, e.g. as provided by Prof. Levraz in this thread. We reproduce his list below. . E. B. Fomalont and R. A. Sramek, Phys. Rev. Lett. 36, 1475 – Published 21 June 1976 . Shapiro, S. S., Davis, J. L., Lebach, D. E., &amp; Gregory, J. S. (2004). Measurement of the solar gravitational deflection of radio waves using geodetic very-long-baseline interferometry data, 1979–1999. Physical review letters, 92(12), 121101. . Fomalont, E. B., &amp; Kopeikin, S. M. (2003). The measurement of the light deflection from Jupiter: experimental results. The Astrophysical Journal, 598(1), 704. . The key prediction we are interested in is that reported by Edward Dowdye Jr., which is summarized in the following gif taken from his former website. . . Now my question is how to independantly perform this experiment. Dowdye comments that delays, like the so-called &quot;Shapiro effect&quot;, were typical, but no bending outside of the plasma limb was observed, i.e. no apparent bending in vacuum. Dowdye appears to have had access to NASA&#39;s Voyager data. [Need elaborate]. . In practice we assume that the light rays are arriving in the &quot;far field limit&quot;, i.e. the incoming rays are parallel at infinity, but their trajectories are effected by their interaction with the Sun. In GR this interaction is continuous with respect to the distance of the ray from the Sun. So the incoming rays are initially parellel, but will develop curvature as they approach the Sun (in the GR model). . So how do we test Dowdye&#39;s claim? According to Dowdye&#39;s arguments about the Gauss sphere, we need not wait for another solar eclipse. If we are $kR$ solar radii distance from the Sun, then the expected angle of deflection is -- according to Dowdye&#39;s reasoning -- equal to $ frac{1}{k} delta theta_R$. Here $ delta theta_R$ is Dowdye&#39;s notation for the expected deflection at the edge of the Sun, i.e. at distance equal to one solar radii. . [To do: check that Einstein&#39;s GR also predicts $ frac{1}{k} delta theta_R$ deflection] . Dowdye suggests that we don&#39;t need to wait for another solar eclipse, since there should be measurable deflection for starlight which passes several solar radii near the Sun. I.e., we need study constellations which appear &quot;near&quot; the Sun. We might take regular photographs of stars near the Sun, and compare with photographs of these same star systems when the Sun is far from these stars, say, six months later. . Such an elementary test... has it not been performed? . Remark. Testing the optical position of these stars is perhaps too simplistic. More modern experiments would use radio waves, or radiation at different wavelengths beyond the visible light spectrum. We are not sure how the modern experiments use &quot;quasar&quot; radio sources in the sky instead of optical light. Again, following Dowdye, we might expect delays in the radio transmissions when they pass near the Sun. However we do not expect displacement of the radio sources. . [To be continued...]. .",
            "url": "https://jhmartel.github.io/fp/2022/05/19/EddingtonEclipse.html",
            "relUrl": "/2022/05/19/EddingtonEclipse.html",
            "date": " • May 19, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Physics is a calling, and I want to run a physics lab.",
            "content": ". So i tell people that i want to run a physics lab in five years. The typical response is &quot;But what do you know about physics or running a physics lab?&quot; And that&#39;s a fair question for the general population. But what don&#39;t i know about physics that i need to know? Surely plenty, but why should that stop me from running a lab? . I look to Ernest Rutherford, or Kristian Birkeland, or Ampere. These were great physicists in the 19th and early 20th century. You might not know this, but I know that i am them. Sort of a &quot;cut from the same cloth&quot; idea. . My background -- what i always tell people -- is that in high school i decided i wanted to be a mathematician. It was a calling, and it was a dream that i pursued for 12+ years, and that dream continues and has brought me here. So here i am now telling you my dream is to run a physics lab. Do you doubt it? . I am formally trained in mathematics, which is a different discipline i admit, but very relevant to physics. And i don&#39;t mean modern theoretical physics, but very relevant to classical physics. Let us remember that all the great physics was performed in 19th and 18th century by gentleman and gentlewomen of modest means, but of extraordinary mind and ability. We are especially mindful of Faraday, Ampere, and Weber. Moreover Gauss and Riemann themselves were especially physically oriented. . Are &quot;trained physicists&quot; the only persons qualified to run physics labs? No, but they are sometimes qualified to operate in a lab. Good, i might need some physicists in the lab depending on their specialties. An electrician is always useful, or somebody trained in electrochemistry or optics, for example. Moreover experiments also need calibrations, and here sometimes experienced specialists are required. . But none of that is any reason why ii can&#39;t run a physics lab. . Why do i want to run a physics lab? because i want to be captain of the ship, because new programs are going to be needed, new discoveries and results are going to be obtained. This is the Thomas Kuhn type of paradigm shift, where the previous generation will be the last to know that the revolution has begun to turn over. . From our view we see classical physics being recalled to life, and being properly rejoined with the experimentalists of 18th, 19th century. We see finally a solution to the same problems that Bohr to Sommerfeld to Heisenberg to Schrodinger all confronted but counld not resolve. In this path the quantum is unnecessary hypothesis. For recall that Planck introduced the quantum as a hypothesis to explain his formula for relating temperature to the spectrum of blackbody radiatio, and he had no explanation for it And its caused no small amount of trouble for the 20th century. What if the quantum discontinuity could finally be understood in the classical continuity? . We see the opportunity for this now, presently in 2022. . So why do i want to run a lab? Because i want to be positioned to lead the way, be among the shock troops of this new frontier of electrical physics. But it remains something of an obscure path, but which is becoming clearer. There is opportunity to become a pioneer and build the trail, and to document the route. And i&#39;m not the first, but i feel like the first wave who&#39;ve learned from Electric Universe and Thunderbolts Project, and picking up the tradition left from Wilhelm Weber. . The next step is building a team. At this stage, I really ought to have graduate students or collaborators. My greatest fault is that I&#39;ve always worked independantly. But now I need teammates, because the task i want to now undertake is too big for me alone. . There&#39;s alot of experiments I would like to do. The simplest experiments would begin with Weber&#39;s potential = $$U(r,r&#39;)= frac{q_1 q_2}{r^2}(1- frac{r&#39;^2}{2 c^2})$$ and the basic predictions of Weber electrodynamics. These are explained in the Brazilian AKT Assis&#39; work on Weber. . We are tempted by the possibility of explaining the relatively short lifetimes of positronium. We are also interested in resolving all the classical objections to Bohr&#39;s model of the atoms, e.g. hydrogen, helium, etc.. . Theoretical calculations with Weber&#39;s potential applied to the Weber-Bohr-Sansbury model of the electron and &quot;positron&quot; could yield comparisons with the relatively short lifetimes of para-positronium and ortho-positronium (approx. 0.12 ns and 138.6 ns, respectively according to the Wikipedia reference). | This would be an interesting test of Weber versus QED. . How difficult is it to create positronium in the laboratory? There is possibility of using a radioactive source that ejects positrons. Specifically, Carl Andersen discovered that alpha particles (helium nuclei) bombarding beryllium (atomic number 4 Be !) emitted positrons. Beryllium Be is rather little known atomic element, but now appears very interesting from the Weber viewpoint. . How does the Weber potential model the emission of positrons when beryllium is bombarded by helium nuclei? | [To be continued...] .",
            "url": "https://jhmartel.github.io/fp/2022/05/17/iWantToRunAPhysicsLab.html",
            "relUrl": "/2022/05/17/iWantToRunAPhysicsLab.html",
            "date": " • May 17, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Fizeau and Sansbury",
            "content": ". As described in previous posts, our critical essay on foundations in SR is basically stable and readable. Here we begin to elaborate on the concluding section, which briefly mentions Ralph Sansbury&#39;s proposed experiment. Sansbury&#39;s experiment is this. We take $c$ the speed of light as equal to $1$ foot per nanosecond $[ mu s]=10^{-8}$ seconds. . Sansbury&#39;s Experiment . We quote from Sansbury&#39;s paper and his book &quot;Faster than Light&quot;. . (Case 1) A $15$ nanosecond light pulse from a laser was sent to a light detector, $30$ feet away. When the light pulse was blocked at the photodiode during the time of emission, but unblocked at the expected time of arrival, $31.2$ nanoseconds after the beginning of the time of emission, for $15$ nanosecond duration, little light was received. (A little more than the $4mV$ noise on the oscilloscope). This process was repeated thousands of times per second. . (Case 2) When the light was unblocked at the photodiode during the time of emission ($15$ nanoseconds) but blocked after the beginning of the time of emission, during the expected time of arrival for $15$ nanoseconds, twice as much light was received ($8mV$). This process was repeated thousands of times per second. . Sansbury&#39;s conclusion? That this indicated that light is not a moving wave or photon, but rather the cumulative effect of instantaneous forces at a distance. That is, undetectable oscillations of charge can occur in the atomic nuclei of the photodiode that spill over as detectable oscillations of electrons after a delay. . Sansbury found the equipment necessary for the experiment too expensive to rent for an extended period of time, and he was possibly not a sufficient expert in calibrating the equipment. So Sansbury&#39;s experiment appears to have not been sufficiently investigated, and we would argue that the experiment has neither been reproduced nor properly reviewed. Thus we turn to the classical Fizeau experiment, and consider its similarities to Sansbury&#39;s setup. . Fizeau&#39;s Saw Tooth Experiment (1849) . Now Sansbury&#39;s apparatus has some similarity with Fizeau&#39;s sawtooth apparatus, as used circa 1849 to prove some of the first sensible measurements of luminal velocity. . Here is a blog which inlcudes a transcription of Fizeau&#39;s original 1849 paper in Comptes Rendus. . Here is an interesting youtube video en francais sur la mesure de Fizeau. Around the 4-5 minute mark is the most interesting. While the speed of the wheel is increased, the received light signal becomes increasingly erratic and intermittent, until a sufficiently high speed of rotation is achieved and the received light signal becomes eventually null and no light is received. . Here is a useful physics stackexchange answer. . Sansbury vs. Fizeau . Now Fizeau&#39;s original setup is a type of Sansbury test where the wheel is in uniform motion. Sansbury&#39;s setup involves either a wheel in nonuniform motion, or equivalently a wheel in uniform motion with a nonuniform sawtooth distribution. . We remark that Fizeau was historically looking to estimate the luminal velocity $c$. Sansbury&#39;s experiment assumes $c$ as given, estimated at $1$ foot per nanosecond. Sansbury&#39;s goal is to distinguish light propagation from particle model, and his setup is meant to test whether light is even something that travels at all. . Let&#39;s review the basic math of Fizeau&#39;s apparatus, it&#39;s very simple. No matter how we setup the mirrors, we suppose light has some total travel time. In Fizeau&#39;s original setup, light travelled a total path of $ approx 16 km$. With an expected speed of light $c=3 times 10^8 km$, then the expected travel time is . begin{equation} frac{16 times 10^3 [m]}{3 times 10^8 [m]/[sec]}=5.333 ldots times 10^{-5} [sec]. end{equation} . Now consider the wheel with angular velocity $ omega$ having units of $[degrees]/[sec]$ and having a toothlength equal to $1/720$ degrees. The time required to turn one toothlength is therefore $ frac{1/720}{ omega}= frac{1}{720 omega} [sec]$. Thus we find that the expected travel time is equal to the time to rotate one toothlength if the following equality holds $$ frac{16 times 10^3 }{3 times 10^8} = frac{1}{720 omega}, $$ which implies $$ omega approx 26 ~~~ frac{[rotations]}{[sec]}.$$ . Sansbury&#39;s (Case 1) could be realized if the wheel was allowed to rotate nonuniformly, i.e. if the wheel could be accelerated in &quot;impulses&quot; something closer to the actual discrete motions of a clock. For example, if the wheel is initially opened at the time of emission, then immediately rotated one saw tooth length (to the closed position) during the expected time of travel, and just prior to the expected time of arrival is rotated another tooth length (to the open position), then Fizeau would predict that the receiver would observe a strong light source. However Sansbury predicts that the receiver would observe rather a very weak signal. Notice here we require the wheel to move twice as fast as Fizeau&#39;s angular velocity. In otherwords the wheel must rotate two complete tooth lengths before the estimated arrival time. . Sansbury&#39;s (Case 2) could be realized if the wheel was rotating nonuniformly. For example, if we keep the wheel fixed during the expected time of flight of the light particle, and turn the wheel one complete toothlength at the expected time of arrival, then Sansbury would predict a relatively strong signal would be received. Fizeau and the particle model would however predict no light would be received, since in the model it would be blocked by the sawtooth at the expected time of arrival. . N.B. Fizeau&#39;s conception of the sawtooth wheel is classical. But what happens if we retrospectively apply the SR methodology to the experiment, what results are obtained? It appears that SR has a null effect on the entire experiment, i.e. returns the same results as the classical case. While the sawteeth lengths are contracting in SR, this effects the circumference of the wheel but not the angular velocity. Thus Fizeau experiment appears insensitive to any Lorentz SR effects and an experiment which cannot prove SR in contrast to the classical mechanics. . We do not require mathematics at this stage, but rather to perform an experiment. However there is an intersting math aspect to the question, &quot;How to keep a nonuniform wheel in uniform motion?&quot;. . Fizeau&#39;s original wheel was materially balanced: the distribution of teeth was equidistant and regular. The centre of mass corresponded with the axis of rotation. . But if we begin to study nonuniform wheels, then the behaviour becomes more difficult depending on, say, whether the centre of mass coincides with the axis of rotation. . Some comparisons between Fizeau and Sansbury: . Fizeau&#39;s involves several reflecting mirrors (beam splitters). Therefore there is more interaction involved in Fizeau&#39;s setup than with Sansbury&#39;s. For Fizeau&#39;s is a type of two-way trip of light, where the source and receiver are space-coincident. But Sansbury&#39;s is a one-way trip, requiring some electronics at the receiver namely a photodiode, to measure the amount of electrons released by the light emission. . | If the phase of Fizeau&#39;s wheel could be controlled, then we could compare the behaviour of the experiment when the wheels differ by one saw tooth length. The trouble in Fizeau is that, because the source and receiver coincide, it&#39;s evident that no light is emitted when the phase is shifted one tooth length. Sansbury&#39;s experiment however does emphatically require the apparatus to be open at the moment of emission. . | Faster Fizeau Wheels . We could test some of Sansbury&#39;s ideas if we could increase the angular velocity of the Fizeau wheel by factor of $4$, i.e. we need a wheel of roughly $100$ revolutions per second instead of $20$ revolutions per second. . Given such a revolution speed, then we could change the sawtooth pattern of the wheels, having some that are $1/4$ closed, $1/2$ closed, and $3/4$ closed wheels. For example, we could have the alternating sawtooth $$ ldots 0101010101 ldots$$ or we could have $$ ldots 001100110011 ldots$$ both of which are $1/2$ closed but having different patterns. And these patterns would have different predictions depending on the photon model or Sansbury&#39;s cumulative action-at-a-distance. Likewise it would be interesting to compare the predictions given a wheel having a $1/4$-closed sawtooth pattern $$ ldots 0001000100010001 ldots$$ versus a $3/4$-closed pattern $$ ldots 0111011101110111 ldots.$$ . If we could get the Fizeau wheel to spin $200$ revolutions per second, then we could test the theories according to $8$-periodic patterns, i.e. with sawtooth patterns being $1/8, 2/8$, $ ldots$, $7/8$ths closed. . If we could build a larger wheel with more teeth, say, $1440$ teeth, then $2880$ teeth, then basic gear ratio would increase the speed of the initial pinion wheel by factor of $ times 2$, $ times 4$, etc.. . The heuristics by which we can determine reasonable revolutions per second depends probably on some energy estimates and would require smaller and smaller radii. . [To be continued...] .",
            "url": "https://jhmartel.github.io/fp/fizeau/sansbury/c/sr/2022/04/29/FizeauSansbury.html",
            "relUrl": "/fizeau/sansbury/c/sr/2022/04/29/FizeauSansbury.html",
            "date": " • Apr 29, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "GR, OT. Part 2.",
            "content": ". Costs and Optimal Transport (OT) . What is OT about? Everybody knows its about costs, and specifically trying to minimize the expected cost of a coupling or semicoupling or correlation $ pi$ between a source $(X, sigma)$ and a target $(Y, tau)$. The coupling $ pi$ is a measure on $X times Y$ which correlates $ sigma$ with $ tau$. . But what is the cost $c(x,y)$ of transporting a unit mass from $x$ to a unit mass at $y$? . Here we assume that the measures $ sigma, tau$ represent matter which needs be conserved. So if a unit mass at $x$ is transported to a unit mass at $y$, then the intermediate masses, say $ mu_s$ for some parameter $s$, needs to have total mass $ int mu_s$ equal to $1$ for all parameters. We will comment below on the possible confusion arising from the use of the term &quot;mass&quot; in applications of OT to GR. . A priori, it is difficult to construct costs $c$ between spaces $X, Y$ which occupy different spaces, having no spatial relationship between points $(x,y)$, and having no measure of either near or far, or hot and cold. In case $X,Y$ occupy different universes, then they might be said to be infinitely far apart and the only canonical cost appears to be a constant zero (or constant infinity) cost. Therefore insofar as optimal transport is studying geometric transport, it is necessary that the source and target spaces $X,Y$ have some spatial relations between the various elements. As trivial as this sounds, it is very difficult and important problem to construct interesting geometric costs. . In practice the author finds best results are obtained when the target $Y$ is given as a subset of the source $Y hookrightarrow X$. Interesting topological applications arise when $Y= partial X$ is a type of rational bordification of $X$, e.g. $Y= partial X[t]$ where $X[t] subset X$ is a $ Gamma$-rational excision like Borel-Serre bordifications of locally symmetric spaces. See our thesis for such examples. . The author is also not convinced one can invent en abstracto interesting geometric costs. . So rather we turn to physical models for inspiration. In our view, cost always represents a cost of energy in Joules, and not necessarily of dollars, or kilometers. . So a cost $c(x,y)$ measures an interaction between a unit mass at $x$ and a unit mass at $y$. . In our minds, this tells us that costs $c(x,y)$ represent interaction energies. . In our thesis [Ibid] we compared the properties of attractive costs, e.g. the quadratic geodesic cost $c(x,y)=d(x,y)^2/2$ when $Y subset X$, with the class of so-called repulsive costs. These costs were interesting because the geometry of the singularities of $c$-optimal transports had very different structures. . We were motivated by electrodynamics. Heuristically, the attractive costs represents interaction energies between oppositely charged positive source and and negatively charged target configurations. The repulsive cost represents interaction energies between, say, positive source and positive target configurations. We recall that opposite charges attract and like charges repel, hence the terminology. This final idiom that like charges repel has a very interesting modification when interaction energy is measured via Weber&#39;s potential, but we leave this for future topic. This is briefly discussed here. . GR and OT . We would like to continue to develop our ideas on GR and OT. This seems popular subject, and I know we have an interesting viewpoint on the whole situation. Where are we? In previous section we were discussing cost as energy. This implies that the expected cost, or integral $ int c(x,y) d pi(x,y)$ also has well-defined units of energy. Bearing this in mind, let us now continue our discussion of GR and OT. . As we discussed here what Robert calls the Lorentz distance $ ell( sigma, tau)$ between measures $ sigma, tau$ on $ mathbf{R}^{3,1}$ and which he interprets as the &quot;expected maximal proper time between the events $ mu, nu$&quot; is not readily interpreted as an energy, and so we find it difficult to accept $ ell$ as a suitable cost in the classical sense. . And so we return to the question &quot;what does $ ell(x,y)$ represent given events $x,y$? &quot; Given the above discussion, let us then also ask: &quot;what does the additive expected value represent $ int ell (x,y) d pi(x,y)$ when $ pi$ is a coupling measure between events $ sigma, tau$? Our point here is that the cost has definite energy units in the classical setting, and these energy units are additive, and therefore the integral representing the expected value again has well-defined energy units. However with the Lorentzian distance $ ell$, there are no units to justify or confirm that the additive average is well-defined. Of course, the numerical integral value is well-defined, but there are many other possible modifications. Robert&#39;s paper has anticipated this objection somewhat in his general treatment of $q ell(x,y)^q$ for $0 &lt; q leq 1 $. . Is the above question irrelevant? Some might rationalize it away, and dismiss the objection. They might ask &quot;why should $ ell$ need an interpretation?&quot; or &quot;why should $ ell$ require units?&quot;. . Our response would be, &quot;well, are we looking for something to compute, or are we looking for something to experimentally verify?&quot; If we are just computing values, and if that is considered physics, then okay, we don&#39;t need units. But if physics is to relate to observation, then we necessarily need units. Why? Because units are used to quantify uncertainty. Again, this is the classical viewpoint. . Remark. If the Lorentz-Minkowski metric $ds^2$ was positive definite, then we could happily represent $ ell$ as a sum of positive squares, in which case we have a formula in the units of energy, i.e. kinetic energy assuming that one can define the inertial mass. But in the SR spacetime formulation, there seems no opportunity to introduce inertial mass, and the sum of signed squares has no Riemannian metric meaning. . Problem: Construct Interesting Energetic Costs . So before we digress into a question about GR and OT, let&#39;s pose some problems. Basically the usual quadratic cost $c(x,y)=d(x,y)^2/2$ is taken as the canonical cost on a Riemannian manifold. Thus we witness the same thing with proposals for applying OT to SR and GR, and this is indeed natural. . Our question here is where to find more examples of costs. For as we developed in our thesis, for one example, different costs can generate singularities of very different homotopy type. From our point of view, this depends on whether costs are attractive or repulsive. . For example with an attractive cost, one is typically looking at ground states which collect near the target. However for repulsive costs, the ground states are typically deeply nested in the source domain, i.e. states are being repelled from the target, and look to escape as far away as possible. . We continue to use electrodynamic energies as the basic supply of interesting costs. The author would be open to hearing other recommendations for interesting costs. . m, matter, mass, Mach. . I can&#39;t help myself from making another comment on the challenge of applying OT to GR. In OT one often speaks about mass transport, where it&#39;s always assumed that a continuity equation holds. Thus when the measures are transported there is conservation and nothing lost or gained along the way. . So if OT is to study &quot;mass transport&quot; in the setting of GR, are we to assume that mass also will satisfy local conservation ? In otherwords, what are the measures $ sigma, tau$ actually representing on the spacetime ? . Wal Thornhill has made this point himself, that among the greatest hazards and sources of confusion in physics is the unfortunate coincidence that both mass and matter begin with the same letter &quot;m&quot;. . Really no joke. That&#39;s the cause of all the trouble. What happens is mathematicians and physicists both get lazy and begin to interpret &quot;m&quot; for matter and mass as if they are equivalent or interchangeable. This possibly originates in Newton&#39;s own non-definition of mass as simply the presence of immediate ponderable matter, and Newton assumed that there was some unspecified constant of proportionality between mass and matter and so &quot;up to a constant which we can set to $1$&quot; both &quot;m[atter]&quot; and &quot;m[ass]&quot; became confused with the letter $m$. . Einstein&#39;s Equivalence Principle is another source of confusing the gravitational mass $m_g$ of an object (which following Newton is something vaguely defined like the quantity of gravitational charge, or in otherwords quantity of matter) with the inertial mass $m_i$. Thus Einstein&#39;s argument that $m_g=m_i$ is another source of confusion. The argument against Einstein&#39;s equivalence principle is elementary, and recognized by Einstein himself that rotating bodies do not admit global inertial frames! Therefore the inertial frames used to convert the gravitational potential into an inertial reference frame is only defined locally on the tangent space. It is very limited first-order observation. . Amazing there is another &quot;m&quot; that enters the problem of &quot;matter&quot; and &quot;mass&quot;, namely Mach! Because Mach proposed that the inertial mass $m_i$ must be defined as the potential energy of the body relative to the fixed stars at infinity. What are the implications? Namely that . matter can neither be created nore destroyed . while inertial mass is variable depending on the interaction of the matter with the matter of the fixed stars at infinity. . This is essentially my understanding of AKT Assis&#39; development of Relational Mechanics. This is also Halton Arp&#39;s interpretation of the observed intrinsic red shifts of quasars which are visibly interacting with nearby systems. Arp&#39;s idea was that, quasars are creation hotspots in the universe, where newly created atoms have less mass because they have been interacting with only a limited part of the universe for a short period of time. Therefore their inertial mass is much smaller, and therefore the wave lengths emitted by the atoms is increased. This increased wave length is caused not by usual red shift velocity mechanism, but by the Machian dependance of inertial mass with the other matter in the universe, and these matter-to-matter signals take time. This is the meaning of the intrinsic red shift, as opposed to the Hubble-Einstein velocity red shift. Further details can be found in Arp&#39;s book &quot;Seeing Red&quot;. .",
            "url": "https://jhmartel.github.io/fp/einstein/sr/ot/lorentz/2022/04/27/GR_OT_Part2_Costs.html",
            "relUrl": "/einstein/sr/ot/lorentz/2022/04/27/GR_OT_Part2_Costs.html",
            "date": " • Apr 27, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Einstein and Maxwell",
            "content": ". We&#39;ve been working on an essay on the foundations of special relativity (SR). Why do we invest so much time and effort into SR foundations ? Well honestly because i think the foundations are never taught, so there&#39;s alot to say. Moreover the SR converts tend to ignore foundations, evading them and jumping ahead to their conclusions. So the task of foundations is typically neglected by adherents, and it&#39;s left to the skeptics to develop the foundational issues. And the student of the history of SR will know that there has always been a strong skeptic school in SR (and GR) and this school frequently included Einstein himself at various times in his life, and for good reason. There is much to critically examine in the SR theory, and this the purpose of our essay. . Einstein SR and Maxwell . Einstein&#39;s theory of special relativity (SR) arose from Einstein&#39;s study of Maxwell&#39;s equations (ME) circa 1870 AD. But what is the logical and mathematical relation between SR and Maxwell? This is interesting question because they are essentially antagonistic. Although Einstein was much influenced by the Maxwellian field theory viewpoint, his own early work (1905) was based on the antithetical photon model of light. . Briefly, by Maxwell equations we understand that in a given reference frame $K$ we have the existence of electric and magnetic fields $E,B$ satisfying the four equations on $div(E), div(B)$ and $curl(E)$ and $curl(B)$ relative to a charge volume density $ rho$ and electric current density $J$. In vacuum where $ rho=0$ and $J=0$, composing the first-order Maxwell equations together yields the second-order fact that the coordinate components of $E,B$ satisfy wave equations with speed of propagation $c= sqrt{ epsilon_0 mu_0}$. This usually leads to the idea that electromagnetic field disturbances travel at the speed of $c$ in aether. And indeed Maxwell&#39;s equations expressly assume an aether as the medium by the which the electromagnetic radiation travels. . Now this author does not really accept Maxwell&#39;s field equations as being satisfactory. For example, the magnetic field $B$ is not a rectifiable or reifiable field, meaning it has only potential and not any material substance. The same could be said for Maxwell&#39;s electric field, which again is a potential field describing the force experienced by a charged test particle. This is the so-called continental field-theoretic viewpoint after Maxwell, etc. . However Maxwell&#39;s equations were not satisfactory in their predictions on the photoelectric effect. For example, is light a disturbance in the electric or the magnetic field? If light is such a disturbance, then Maxwell equations predict the interaction of the $E$-wave (or is it $B$-wave) with charged test particles. . Here it&#39;s interesting to compare Einstein&#39;s 1905 explanation of the photoelectric effect using the photon particle theory of light. Thus we tend to interpret Einstein&#39;s developments of SR from a photon or corpuscular point of view. . Problem: The classical homogeneous wave equation has the property of the velocity being dependant on the receiver velocity relative to the medium. But SR argues that the assumption on the &quot;rectilinear uniform propagation of light&quot; somehow yields a wave equation where velocity is receiver independant. But how? [We do not address this important issue here]. . SR and Lorentz Groups . The null result of the Michelson-Morley experiments led to Einstein&#39;s postulating the Lorentz transformations relating space and time variables $x,t$. Undoubtedly the theory of SR is summarized in the representations of the Lorentz group of linear transformations, namely the isometry group designated $O(ds^2)=O(3,1)$ and its standard linear action on ${ bf{R}}^{3,1}$. . For the mathematician, once a single linear representation is given, there are many algebraic constructions possible to obtain further representations, for example the symmetric and alternating representations. We develop this idea further to try and bridge the assumptions of SR to Maxwell&#39;s equations, and especially the wave equation. . Now we discuss several group representations (i.e. linear group actions). . First we begin with the standard linear representation $$ rho_0:{ bf{R}}^{1,3} times L to { bf{R}}^{1,3}$$ which is the linear representation $ rho_0$ represented by left matrix multiplication $(v, lambda) mapsto lambda.v$. . Next we dualize. . Let $C({ bf{R}}^{3,1})$ be the space of polynomial functions on the space. Abbreviate $C:=C({ bf{R}}^{3,1})$. Naturally we assemble $C$ from the dual functionals $ lambda in {({ bf{R}}^{3,1})}^*$. Taking products and polynomials in the dual functions $ lambda$ we obtain the contragradient represention $$ rho_0^*:C( { bf{R}}^{3,1}) times L to C({ bf{R}}^{3,1}). $$ . The idea is that the vector spaces $V$ and $V^*$ are isomorphic (non canonically) in finite dimensions. Moreover the algebra generated by $V^*$ yields an (infinite-dimensional) space of polynomial functions on $V$. . Now what are vector fields? . In differential topology, the vector fields $ frac{ partial }{ partial x}$ act on functions as derivations, i.e. as linear maps $$ frac{ partial }{ partial x}: C to C $$ satisfying Liebniz product formula. Iterating these linear maps generates an algebra of operators on $C$, namely the operators polynomial in $ partial / partial x$. On the other hand, the differential $dx$ itself as contained in the cotangent space is not an algebra. . Iterating the derivations $ frac{ partial }{ partial x} circ frac{ partial }{ partial y}= frac{ partial^2 }{ partial x partial y}$ leads to the usual linear differential operators on $C$. We are specially interested in d&#39;Alembert&#39;s operator $$ square:= frac{-1}{c^2} frac{ partial^2}{ partial t^2} + frac{ partial^2}{ partial x^2}+ frac{ partial^2}{ partial y^2}+ frac{ partial^2}{ partial z^2} .$$ . Our main proposal, and this is not yet altogether rigorous, is to identify the Minkowski squared line element $ds^2$ as dual in a certain yet-to-be-defined algebraic sense to the d&#39;Alembert operator $ square$. The difficulty is that the symmetric product of the differential operators $ partial/ partial x$ and $ partial / partial y$ is distinct from the composition of the differential operators $ partial^2 / partial x partial y$. . The term $dx^2$ in Minkowski&#39;s line element is formally a section of the $(T^*)^{ otimes 2}$ bundle over the manifold space, here ${ bf{R}}^{4}$. So here is the informal computation. Let us formally relabel the variables $$x_0, x_1, x_2, x_3 = t,x,y,z, $$ respectively. Now the choice of Lorentz metric $h$ can be represented as a square symmetric matrix $$[h]= begin{pmatrix} -c^2 &amp; 0 &amp; 0 &amp; 0 0 &amp; 1 &amp; 0 &amp; 0 0 &amp; 0 &amp; 1 &amp; 0 0 &amp; 0 &amp; 0 &amp; 1 end{pmatrix}.$$ . The choice of $h$ allows us to define an isometry between the differential forms and vector fields, i.e. secord order linear operators. Specifically, $h$ allows us to define explicit isometry between symmetric $(2,0)$ tensors and symmetric $(0,2)$ tensors. . Lemma: The metric $h$ identifies the dual of $ds^2$ with d&#39;Alembert&#39;s wave operator $ square$. That is $(ds^2)^* = square.$ . Proof. The proof is basic linear algebra. First one needs prove that the $h$-dual of $dx_0$ is $dx_0^* =-c^{-2} frac{ partial}{ partial x_0}$. Likewise we find $dx_i^*= frac{ partial}{ partial x_i}$ for $i=1,2,3$. This is all in the tangent space, i.e. between $(1,0)$ and $(0,1)$ tensors. Now we consider the squares, i.e. the symmetric $(2,0)$ tensors. We find that $$(ds^2)^*=-c^2 (dx_0^*)^2 + (dx_1^*)^2 + (dx_2^*)^2 + (dx_3^*)^2, $$ and which is equal to $$-c^{-2} ( frac{ partial}{ partial x_0})^2 + ( frac{ partial}{ partial x_1})^2 +( frac{ partial}{ partial x_2})^2+( frac{ partial}{ partial x_3})^2, $$ which is equal to d&#39;Alembert&#39;s square operator $ square$ as desired. [See our remarks above on the nonrigorous nature of this argument, and is the subject of investigation.] . The Lorentz invariance of $ square$ shows the solutions to the homogeneous wave equation (HWE) are Lorentz covariant and $ square phi =0$ if and only if $ square lambda cdot phi =0$ for every Lorentz transformation $ lambda in L$. This is the wave equation version of the fact that the null cone $ds^2=0$ is Lorentz covariant. . Now Einstein&#39;s (A12) postulates the uniform rectilinear propagation of light in vacuum. This would suggest a corpuscular model of light, being represented as affine parameterized lines $$s mapsto (s, x(s), y(s), z(s))=(s, gamma(s)) $$ in ${ bf{R}}^{3,1}$ satisfying $D^2_{ss} gamma =0$. . Is the equation $D^2_{ss} gamma=0$ Lorentz covariant? (Yes?) . But what are the corresponding &quot;uniform rectilinear&quot; solutions $ phi$ for the dual HWE: $~~ square phi=0$ ? Compare this. . An idea: there has always been correspondance between lines in $V$ (one-dimensional linear subspaces) and quadratic functionals via the Segre embedding, or $ lambda mapsto lambda^2$ where $ lambda in V^*$ is a linear functional. . The following questions will be answered below: . Are the quadratic functions $q(x)=h(v,x)^2/2$ solutions to $ square =0$ for null vector $v in N$? (Yes, we prove below). . | Can we find quadratic functions $q$ whose level sets are everywhere orthogonal to the null cone $N$ ? . | . The idea would be to derive some canonical solutions $ square q=0$ from quadratics arising from vectors on the null cone. . If $v$ belongs to null cone, then $q(x):=h(v,x)^2/2$ for $x in V$ defines a quadratic function on $V$ with $q(v)=h(v,v)^2=0$. . It&#39;s clear that $q$ is minimized along $v^ perp$, i.e. $q(x,v)=0$ for all $x in v^ perp$ and $v in v^ perp$. Here $v^ perp$ consists of all $u$ such that $h(u,v)=0$. . Lemma. For every vector $v in { bf{R}}^{3,1}$, let $q(x):=h(v,x)^2/2$ be the quadratic form defined by $v$. Then $ square q=0$ if and only if $v in N$ and $h(v,v)=0$. . Proof. We claim that $ square q=h(v,v)$ when $q(u)=h(v,u)^2/2$. If the vector $v$ has coordinates $v= langle v_t, v_x, v_y, v_z rangle$, then $h(v,x)^2$ is equal to $$(-c^2 v_t t + v_xx+ v_yy+ v_zz)^2/2,$$ which is equal to $$c^4 v_t^2 t^2 +v_x^2 x^2 + v_y^2 y^2 + v_z^2 z^2 + (mixed~ terms).$$ Applying d&#39;Alembert&#39;s operator we find $$ square q =2( -c^2 v_t^2+v_x^2 + v_y^2 + v_z^2)=2 h(v,v),$$ since $ square(mixed~~terms)=0$ and the claim follows. . Thus we find that null vectors $v in N$ yield solutions $q_v$ to HWE. . A superposition principle also applies, where any signed measure $ mu in mathscr{M}(N)$ yields a $ mu$-averaged solution $q(x):= int_N q_v (x) d mu(v)$ to the HWE. Here it would be useful to have a representation theorem, something like, if $ phi$ is any solution of HWE, then $ phi$ can be represented as a $ mu$-average of the $h_v$ as described above. . [To do: establish the conservation of energy for the HWE from the same principles.] .",
            "url": "https://jhmartel.github.io/fp/einstein/maxwell/wave%20equation/lorentz/sr/2022/04/26/Einstein_Maxwell.html",
            "relUrl": "/einstein/maxwell/wave%20equation/lorentz/sr/2022/04/26/Einstein_Maxwell.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "GR, OT. Part 1.",
            "content": ". I recently watched Yann Brenier&#39;s YT lecture on Optimal Transport (OT) and General Relativity (GR). He mentioned Professor McCann&#39;s GRO paper. The question ``How to apply OT to GR ?&quot; seems to be gaining attention. But as Brenier suggested, there possibly appears some arrogance in the OT theorists who believe that OT is readymade to make a breakthrough in the GR field. . I have my own thoughts on the problem.T he trouble is that GR is a very radical paradigm for thinking about physics, especially about energy which is notoriously undefined (and undefinable!) in GR. Likewise work is undefined in GR. Thus it&#39;s basically impossible to properly relate thermodynamics to GR. See mathoverflow, physics.stackexchange for numerous questions about the impossibility of really defining energy tensors which satisfy a conservation law, e.g. 1, 2. These difficulties gave Einstein alot of grief throughout his life. . In OT, everything is generally controlled by the geometry of the cost $c: X times Y to mathbf{R} cup {+ infty }$, where $(X, sigma)$ and $(Y, tau)$ are the source and target measure spaces, respectively. But cost is really more properly understood as energy. . What is the cost of transporting/correlating a unit source mass $dx$ to a unit target mass $dy$ ? Well... isn&#39;t it really the energy required to transport/correlate? . Thus when OT studies couplings and semicoupling measures $ pi$ between the source measure $ sigma$ and target $ tau$, the optimization problem is really about the min energy states, i.e. ground states. So OT is already positioned to contribute to GR, as soon as GR can define energy! . For example, if we consider Professor McCann&#39;s paper, we see that path integrals of $ds&#39;:= sqrt{-ds^2}$ along so-called timelike curves lead to his definition of $ ell(x,y;q)$ for a parameter $0 &lt; q leq 1$. At $q=1$ the function $ ell(x,y)$ is like the Lorentz &quot;distance&quot; (caution this is misnomer, since $ ell$ satisfies a reverse triangle inequality!). The function is frequently related to the so-called proper time function $d tau$ defined by $ds&#39;=cd tau$. Since we do not readily admit that $ds&#39;$ has units of $[length]$, we likewise believe it&#39;s an error to generally refer to $d tau$ as having units of $[time]$. While $ds&#39;$ and $ ell$ do represent covariant scalars, we consider it antithetical to the premise of GR to consider these absolute scalars as having any physical units. . So Prof. McCann&#39;s interpretation of the &quot;Lorentz cost&quot; $$ ell( mu, nu)= sup_{ pi} [ int ell(x,y)^{1/q}~ d pi(x,y)~]^q, ~~0 &lt; q leq 1$$ as representing the quote &quot;maximum expected proper-time which can elapse between the distribution of events $ mu, nu$ &quot; is an interesting heuristic, but perhaps not a strictly proper GR interpretation. . But another trouble with the differential geometry of GR is that the Minkowski-Lorentz quadratic forms $g=ds^2=-c^2dt^2 + dx^2+dy^2+dz^2$ is a unit-less number. This is where the absolute part of the absolute differential calculus makes itself known: whatever real number is being represented by $ sqrt{-ds^2}$, it has no ``physical meaning&quot;. . Where does the proper time interpretation come from? The interpretation comes from the use of a so-called &quot;instantaneous rest frame&quot;. But how much information can a particular choice of coordinate system provide? This requires the observer to find coordinates $( tau, xi, eta, zeta)$ where $ tau$ represents ``time &quot; and all the partial derivatives vanish $$ frac{ partial xi}{ partial tau}= frac{ partial eta}{ partial tau}= frac{ partial zeta}{ partial tau}=0.$$ In this particular coordinate system one finds $ds^2=-c^2d tau^2$ and $ds&#39;=c ~ d tau$. But what does this computation really show? We consider it less persuasive than it might appear : can we really conclude that $ds&#39;$ has units of $[length]$ or that $ds&#39;/c$ has units of $[time]$, based on the form of the equation in one coordinate system ? . Conclusion. . Our point is that the tensor calculus approach to GR requires users to basically &quot;surrender their units, rigid rods and rulers at the door&quot;. Once inside the covariant category, the rods no more represent objective lengths. Irrespective of their material composition, the Lorentz transformation formulas take over and contract what was otherwise incontractible. The users themselves will see no contraction because also their corresponding &quot;proper times&quot; will be contracted. So when the Lorentzian scalar $ds&#39;$ is used by the Riemannian geometer, a careful mind needs to not readily confuse the units of $ds&#39;$ as representing $[length]$ in the timelike future directions. . These opinions originate from around year 2013--2018, when I really spent alot of time in symplectic geometry, almost-complex structures, pseudo-Riemannian and Lorentzian geometry, and took long road to realize that importing Riemannian definitions into pseudo-Riemannian structures does not yield metric Riemannian results. For example, a function $ ell(x,y)$ which satisfies a reverse triangle inequality $$ ell(x,y) geq ell(x,z)+ ell(z,y)$$ can not really correspond to a units of $[length]$. Nonetheless it&#39;s common to see triangles with relabelled edges and where the lengths in no way correspond to the length in the image. . Here looks to be very thorough introduction to the current differential geometric approach to energy in GR. I&#39;m vaguely aware of the ADM definition, originating with Weyl I believe, and which converts the covariant divergence free $ nabla_i T^{ij}=0$ into a local integral conservation equation. .",
            "url": "https://jhmartel.github.io/fp/einstein/gr/ot/units/tensors/2022/04/24/GR_1.html",
            "relUrl": "/einstein/gr/ot/units/tensors/2022/04/24/GR_1.html",
            "date": " • Apr 24, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Alexandrov and Souls. Part 1.",
            "content": ". Here is available a very rough work-in-progress. I&#39;ve let the idea rest for seventeen months. It&#39;s difficult to illustrate the idea in python, and there appears no application of python to Alexandrov geometry yet. . What is Alexandrov geometry? . Introductions and references are available from Alexander, Kapovitch, Petrunin. The subject is notoriously difficult. The original paper of Burago, Gromov, Perelman is still useful. . An Alexandrov space $(X,d)$ is a fat and round metric space, like a sphere or cube but possibly with corners and sharp angles, and where triangles are fat. Examples include: convex sets $C$, the boundaries of convex sets $ partial C$, affine space $ mathbf{R}^d$. There are several important metric constructions by which spaces with curvature bounded below ($CBB[ kappa]$ in the notation of AKP). . We are interested in the singular mm-spaces, so we use the synthetic definition of sectional curvature and not necessarily the Gaussian definition of differential Riemannian geometry. Alexandrov spaces have sectional curvature $ kappa geq 0$, which implies that every geodesic triangle $ Delta(a,b,c)$ in $X$ is fatter than the comparison triangle $ tilde{ Delta}$ in $ mathbf{R}^2$. . In Alexandrov geometry, geodesics frequently focus and collide. Initially geodesic rays might appear to be orthogonal, but at a not-too-faraway time, the rays will focus and converge back to a point. After this refocussing the geodesics might or might not diverge again to infinity. Positive sectional curvature tends to manifest in general relativity spacetimes, where the presence of mass (attractive matter) tends to cause light rays to focus and converge. . Our goal is a general method for constructing the souls of singular finite-dimensional Alexandrov spaces. . The original result on souls was achieved by G. Perelman (1994), building on the work of Gromoll, Meyer, Cheeger. Perelman&#39;s result in the Riemannian setting was that the souls $S$ of Riemannian Alexandrov spaces $(X,g)$ had the property that: $X$ was diffeomorphic to the normal bundle of $S$. . So what remains to be proved in the singular setting? . On a singular Alexandrov space, the space of directions at every point is isometric to an Alexandrov space of curvature $ kappa geq 1$. The metric distance on the space of directions $ Sigma_p X$ at a point $x$ is defined by the angle between the directions. Alexandrov spaces (curvature bounded below by zero) have well-defined angles and directions. For almost all points, this space of directions is isometric to a $d-1$ dimensional sphere $ mathbf{S}^{d-1}$. This is well-known regularity property of Alexandrov spaces, namely that almost all points are regular. This is analogous to the fact in convex analysis that proper lower semicontinuous convex functions are differentiable almost everywhere. . The real challenge in the singular setting is to define gradient flows which are naturally defined and extend through the singular points. . Python and Alexandrov? . Our approach to mathematics is based on discovery. So instead of trying to prove everything a priori, we prefer to experiment and discover what is true. But how to use python to study Alexandrov spaces? It&#39;s not clear... . There are many analogies between lsc (lower semicontinuous) proper convex functions $f: mathbf{R}^d to mathbf{R} cup {+ infty }$ and Alexandrov spaces. For example, the regularity of Alexandrov spaces follows from the fact that: lsc proper convex functions are differentiable almost-everywhere on their domain. This is because the gradient $D f$ is Lipschitz on its domain, and therefore differentiable almost-everywhere. Therefore $f$ is even twice-differentiable almost everywhere on its domain. . Similarly, if an Alexandrov space contains a doubly-ended geodesic ray, then the Splitting theorem says $X= mathbf{R}^1 times X_0$ where $X_0$ is again Alexandrov. For convex lsc proper functions, the analogous fact is this: if the graph of the derivative $Df$ contains a double ended straight line, then up to a change of variable, $f$ can be factored as $f= ell(x_1) + f_0(x_2, ldots, x_d)$ where $ ell$ is affine and $f_0$ is a convex function which only depends on the remaining variables $x_2, ldots, x_d$. (There may be slight error here, but this is essentially the idea as i learned from Prof. R.J. McCann.) . But the proper lsc convex functions are something like the local theory of Alexandrov spaces. For example, a difficult result in the foundations of Alexandrov geometry is Toponogov&#39;s Globalization theorem, namely: if a space satisfies Toponogov comparison everywhere on small neighborhoods, then the space satisfies Toponogov comparison globally. . Although we should note that the convex analysis analog does not readily lead to a proof of Toponogov&#39;s Globalization theorem, which is very difficult result in foundations of Alexandrov geometry. For functions $f$ on $ mathbf{R}^d$ the question is: if $f$ is locally convex, then prove $f$ is globally convex. .",
            "url": "https://jhmartel.github.io/fp/alexandrov/souls/singular/2022/04/22/Alexandrov.html",
            "relUrl": "/alexandrov/souls/singular/2022/04/22/Alexandrov.html",
            "date": " • Apr 22, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Yao's Millionaire's Problem. Part 1.",
            "content": ". The purpose of this article is to investigate whether there is strategy or skill possible in the following variation of Yao&#39;s &quot;Millionaire Problem&quot;. . Here is the game. We have a huge grid $ mathbf{R}^2$. Now let two players $A,B$ have secret locations $s_A=(x_A, y_A)$ and $s_B=(x_B, y_B)$. These secrets are points in the euclidean plane $ mathbf{R}^2$. . Now the players $A, B$ are going to take turns guessing affine functions (or affine lines in $ mathbf{R}^2$) and the first player to guess an affine function which separates the secrets wins! . The gameplay is something like this: The players $A,B$ take turns. If player $A$ goes first, then player $A$ chooses an affine function $ ell$ on $ mathbf{R}^2$, and asks player $B$ to reply with the sign of $ ell(s_B)$. We require that $B$ replies honestly with $sgn( ell(s_B))$. This is the end of player $A$&#39;s turn. If $ ell$ separates the secrets, then player A wins. Otherwise it&#39;s player B&#39;s turn. Next player $B$ chooses an affine function $ ell&#39;$, and asks player $A$ to reply with the sign of $ ell&#39;(s_A)$. Once player $A$ replies, then this is the end of player $B$&#39;s turn. Again, if $ ell&#39;$ separates the secrets, then player B wins. Otherwise it&#39;s player A&#39;s turn. . The object of the game is to determine an affine function $ ell$ which separates the secrets, i.e. for which $ sgn( ell(s_A)) neq sgn( ell(s_B)).$ The first player to demonstrate an affine function which separates the secrets wins! . Our interest is to find optimal strategies for this game. Firstly, we have to consider whether any strategy is even possible. For example, can player $A$ use the cumulative history of both player $A$ and $B$&#39;s affine guesses to better inform their next guess? For example, if player $A$ guesses an affine function $ ell$ which does not separate, then player $B$ can use the knowledge of their own private secret to determine which halfspace contains $s_A$. And indeed, by the same reasoning player $A$ can use their knowledge of $s_A$ to likewise determine which halfspace contains $s_B$. So obviously the initial distribution $d lambda$ is updated to the restricted distribution $d lambda cdot 1_H$, where $H$ is the halfspace defined by $ ell$ and containing $s_A, s_B$. With successive guesses, the distribution becomes a descending chain of closed convex sets, namely the intersection of successive halfspaces, having the form $$d lambda leadsto d lambda cdot 1_H leadsto d lambda cdot 1_H 1_{H&#39;} leadsto d lambda cdot 1_H 1_{H&#39;} 1_{H&#39;&#39;} leadsto cdots. $$ . The notation is somewhat strange, but simply expresses that we remain uncertain of the specific location of the secrets $s_A, s_B$, except we know the possibly location is becoming more restricted. . In the millionaire game, the players $A,B$ have an interest in privacy. Their secrets $s_A, s_B$ are intended to be secret. This means the players $A,B$ might not choose affine functions which potentially reveal information about their own secrets. In practice this means players determined to maintain their privacy will always choose affine functions which do not bound compact convex sets. Similarly, an opponent will not readily choose affine functions which separates the domain into a bounded component, since the probability that the opponent&#39;s secret lies in the bounded component is relatively small, while the probability of its lying in the unbounded component is much greater. . The subject of so-called zero knowledge proofs in cryptography is related to the millionaires problem. Here we try to find a balance where the players can choose to reveal as much as they wish of their own balances, while their own guesses are signals/indications in-themselves of the secret balance. . Our question is whether there is any strategy or skill in this game. What is the optimal strategy? Can the player use the knowledge of the opponent&#39;s affine functions to improve their own selection of affine function?? . import numpy as np import matplotlib.pyplot as plt # Now we simulate the millionaire problem on the euclidean two-dimensional plane. # For convenience we rename the players $A,B$ as players $+1, -1$, respectively. # for testing purposes we suppose the players A,B have secrets below: #s_A=input(&quot;What is player A&#39;s secret position?&quot;) s_A=[16,0] s_A=np.array(s_A) #s_B=input(&quot;What is player B&#39;s secret position?&quot;) s_B=[0, 0.2] s_B=np.array(s_B) # now we define some basic functions, i.e. to compute affine functions based on # their normal n and height b. def affine(n,x,b): n=np.array(n) x=np.array(x) # return n.dot(x)+b return n[0]*x[0]+n[1]*x[1] + b # to protect the secret we really only need the sign of the affine function. def sign(x_Real): if x_Real&lt;0: return -1 else: return +1 # here t defines the test function, which returns True iff the affine function # separates the secrets. True is returned if the signs of the affine function # evaluated on the secrets are not equal. def t(n,b): n=np.array(n) if sign(affine(n,s_A,b)) != sign(affine(n,s_B, b)): return True else: return False . #initial conditions. outcome=False history=[] vector_history=[] player=+1 i=0 color=[] while outcome == False: print(&quot; n Player &quot; + str(player) + &quot;&#39;s turn to play:&quot; ) print(&quot;Given the history &quot; + str(history) + &quot; choose your affine function:&quot;) n0 = float(input()) n1 = float(input()) b = float(input()) history = history + [[n0, n1, b]] vector_history=vector_history + [[n0, n1]] i=i+1 if t([n0, n1], b) == True: outcome = True print(&quot;Winner! Player &quot; + str(player)+ &quot; has separated the secrets with &quot; + str([n0, n1, b]) + &quot;. End of Game!&quot;) else: print(&quot;Fail! Player &quot; + str(player) + &quot; has failed to separate the secrets... End of turn.&quot;) player=player*(-1) # the following plots the various normals chosen by the players, but we would # prefer to have the half spaces. V=np.array(vector_history) origin=np.array([[0]*i, [0]*i]) plt.quiver(*origin, V[:,0], V[:,1], scale=21) plt.show() . Player 1&#39;s turn to play: Given the history [] choose your affine function: 1 1 0 Fail! Player 1 has failed to separate the secrets... End of turn. . Player -1&#39;s turn to play: Given the history [[1.0, 1.0, 0.0]] choose your affine function: 3 1 0 Fail! Player -1 has failed to separate the secrets... End of turn. . Player 1&#39;s turn to play: Given the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0]] choose your affine function: 1 4 0 Fail! Player 1 has failed to separate the secrets... End of turn. . Player -1&#39;s turn to play: Given the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0], [1.0, 4.0, 0.0]] choose your affine function: -3 0 -5 Fail! Player -1 has failed to separate the secrets... End of turn. . Player 1&#39;s turn to play: Given the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0], [1.0, 4.0, 0.0], [-3.0, 0.0, -5.0]] choose your affine function: -1 -4 2 Winner! Player 1 has separated the secrets with [-1.0, -4.0, 2.0]. End of Game! . The above is a very simple gameplay, where it happens by chance that the two points can be separated by a flat strip, namely the space between two parallel halfspaces. We included the above simply as an example. . [To Do:] . Use matplotlib to plot the halfspaces, and not simply the normal vector, which is what we have above. . | Determine some automatic routine to compete with a human opponent. . | The millionaire&#39;s problem implicitly assumes the players $A,B$ have large funds, i.e. enough to pay for a dinner! Therefore we might need assume our secrets $s_A, s_B$ are sufficiently far from the origin. (?) . | If the domain is essentially infinite, then a certain amount of privacy will always be maintained, because it&#39;s better to bisect the unknown into two halfspaces of equal (possibly infinite) area. If the affine function indeed separates the secrets, then the position of that secret is only known to occupy an infinite area domain, and thus essentially remains private in a restricted sense. Although of course the direction of the secret, and not necessarily its magnitude will be better known to the opponent, i.e. there will be a definite reduction of uncertainty in the direction of the opponents secret, but not necessarily a reduction in uncertainty in its magnitude. . | If an opponent proposes an affine function which separates the domain into a bounded and unbounded component, then that is huge risk for the player, i.e. it&#39;s unlikely that the small bounded domain (chosen at random) will contain the secret as opposed to the infinite domain. At the risk of belabouring the point: a random infinite domain is more likely to contain an unknown secret than a compact domain. We find this an interesting point... . from scipy.spatial import HalfspaceIntersection prehistory = history[:-1] signs=[] sph=[] for x in prehistory: epsilon=sign(affine([x[0], x[1]], s_A, x[2])) signs=signs+[epsilon] sph=sph+[epsilon*np.array(x)] sph=np.array(sph) # for illustration we have the secret s_A as feasible_point. # its interesting question to select a feasible point which # does not reveal too much information about the secrets... # but obviously any point on the convex hull formed by the secrets s_A, s_B # will be a feasible point. But there are many more choices, so which choice reveals # the least information about the secrets s_A, s_B ? I.e. which feasible point can be chosen # which reveals the least information about s_A, s_B? feasible_point = np.array([16.0, 0.0]) halfspaces = sph*(-1) # we need reverse-signs to align with the convention in qhull that # the halfspaces are defined by the inequality Ax+b &lt;= 0. hs = HalfspaceIntersection(halfspaces, feasible_point) import matplotlib.pyplot as plt fig = plt.figure() ax = fig.add_subplot(&#39;111&#39;, aspect=&#39;equal&#39;) xlim, ylim = (-100, 100), (-100, 100) ax.set_xlim(xlim) ax.set_ylim(ylim) x = np.linspace(-100, 100, 1000) symbols = [&#39;-&#39;, &#39;+&#39;, &#39;x&#39;, &#39;*&#39;] signs = [0, 0, -1, -1] fmt = {&quot;color&quot;: None, &quot;edgecolor&quot;: &quot;b&quot;, &quot;alpha&quot;: 0.5} for h, sym, sign in zip(halfspaces, symbols, signs): hlist = h.tolist() fmt[&quot;hatch&quot;] = sym if h[1]== 0: ax.axvline(-h[2]/h[0], label=&#39;{}x+{}y+{}=0&#39;.format(*hlist)) xi = np.linspace(xlim[sign], -h[2]/h[0], 1000) ax.fill_between(xi, ylim[0], ylim[1], **fmt) else: ax.plot(x, (-h[2]-h[0]*x)/h[1], label=&#39;{}x+{}y+{}=0&#39;.format(*hlist)) ax.fill_between(x, (-h[2]-h[0]*x)/h[1], ylim[sign], **fmt) x, y = zip(*hs.intersections) ax.plot(x, y, &#39;o&#39;, markersize=8) . [[ 1. 1. 0.] [ 3. 1. 0.] [ 1. 4. 0.] [ 3. -0. 5.]] &lt;class &#39;numpy.ndarray&#39;&gt; . [&lt;matplotlib.lines.Line2D at 0x7f1a35df01d0&gt;] . The above intersection of halfspaces isn&#39;t what i expected. The complete intersection is the sector in the upper right hand corner. . Now if we are truly taking secret points $s_A, s_B$ at random in $ mathbf{R}^2$, then almost all random choices of affine functions will not separate the secrets. For example, given the homogeneity of $ mathbf{R}^2$, we can consider the secrets $s_A, s_B$ as being extremely close such that they are basically coincident, or at least as seen from a far distance. But then a random choice of affine function is extremely unlikely to contain the two points. Thus it appears that truly random choices of affine functions have essentially zero probability of separating the secrets. . This leads to the next step in our study of the Millionaire Problem, namely where the initial distribution on $ mathbf{R}^2$ is not necessarily uniform. E.g., perhaps we know that the secrets are distributed within a given large radius ball. If we have no other information about the secrets except that it lies somewhere on the large ball $D$, then one probabilistic strategy is to bisect the ball with affine functions, i.e. randomly guess an affine $ ell$ such that $ 1_D 1_{ ell&gt;0}$ and $1_D 1_{ ell &gt; 0}$ have equal area. . But what about privacy? In the previous case where the distribution was uniform on its support on $ mathbf{R}^2$, the privacy of the secrets was maintained so long as the affine functions were unbounded (from above and below). And the opponents would always guess such affine functions because there odds of correctly separating the secrets is significantly increased. But now its possible that the distribution will not be uniform on its support, and therefore the secrets can be learned with a reduction in uncertainty, i.e. perhaps we know that the opponents secret lies in a bounded set, or that 90% percent of the time the opponents secret lies in a given domain. . We remark that there is something like a &quot;maximum likelihood&quot; principle being used here. Now regarding privacy: if the domain $D$ is bounded, then depending on the distribution, the secret might have diminished privacy. This leads to Part 2 of our study, where the secrets are distibuted according to nonuniform distributions $ mu_A$, $ mu_B$ on $ mathbf{R}^2$. . a. What is the optimal strategy for nonuniform initial distributions $ mu_A, mu_B$ ? . b. Can we quantify the &quot;loss&quot; of privacy when the distributions are supported on unbounded domains versus bounded domains? . (To be continued...) .",
            "url": "https://jhmartel.github.io/fp/millionaire/secret/convex%20analysis/2022/04/20/MillionaireGame.html",
            "relUrl": "/millionaire/secret/convex%20analysis/2022/04/20/MillionaireGame.html",
            "date": " • Apr 20, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Economics of Moving and Delivery.",
            "content": ". It&#39;s commonly reported that &quot;moving&quot; or &quot;la demenagement&quot; is among the most stressful events for consumers, c.f. &quot;Americans find moving more stressful than divorce&quot;. . What are the essential difficulties involved in moving? Here we assume we are moving a residential home. . Moving is labour intensive, involving hands-on moving of numerous boxes, furniture (tables, couches, bedsets, dressers), and fragile items (TVs, mirrors, lamps). . | Moving items is highly constrained, often involving heavy items being securely extracted from dwellings, and all this with zero damage to walls or floors. . | Another difficulty, perhaps unappreciated by the clients, is the necessity of packing the objects into the moving truck. . | This is a type of entropy problem, since the compressed volume of the moving truck restricts the possible range of motions of the objects. Moving requires alot of work and foresight to efficiently pack all the items securely in a truck. . Another view of the entropy difficulty is this: a house has many rooms, with the objects distributed sparsely throughout the space. However in a moving truck, all the objects need to be compressed into a single room (namely the box of the truck). . Can the clients estimate the volume of all their objects? . Is it possible for them to imagine all the objects to be relocated into a single room? . There is considerable stress involved in the action of, say, extracting heavy expensive &quot;precious&quot; furniture through various stairwells, corners, basements, etc.. The business of last-minute kijiji moving is even more stressful, for the clients are typically totally unprepared. For example, they might be selling a freezer located in the basement of a townhouse, with a very tight spiral staircase, and the client has arranged for its delivery to another basement appartment. The client might be moving their entire household, or only moving this single item. Or the client has received a new treadmill, and require its transport into the basement. . Building codes and standard construction methods make the extraction and deliveries somewhat easier. However extreme furniture pieces often push the movers ingenuity to the extreme, and requires alot of experience to immediately know which precise &quot;furniture ballet&quot; is required. As a rule of thumb : if an object makes three points of contact with the wall/floor/ceiling, then the object cannot be pushed any further without causing damage to the surroudings. . So how can the consumer save money when moving? . The answer is basically preparation. . To save money on moving: order a very large truck to make the loading and offloading easier, and pack as much as possible in regular cardboard boxes. This cannot be overremphasized: as much as possible, all the irregular objects should be packed into boxes, and into as many boxes as necessary. . Why? . Because it&#39;s expensive for the movers to waste their time in arranging and sometimes rearranging irregular shapes objects into the truck. The client should dissessemble the furniture as much as possible beforehand. Otherwise the movers need to spend working hours on the assembly/disassembly of furniture, and/or the preparation of more fragile objects. . Another difficulty is the patience and time required to safely move objects in tight spaces. For example, let us consider IKEA items which are very popular. One of the keys to IKEA&#39;s business model is that their furniture is transported and sold totally disassembled, and neatly packing into boxes. I think it&#39;s evident that tables and dressers dissassembled in boxes is more convenient for transport than fully assembled! In IKEA the consumer is required to read the instructions to assemble their items. . Another important constraint in domestic moves is: what is the pathway from the pickup to the truck, then from the truck to the dropoff. These are the environmental factors. Is the client moving in/out of a 20 storey building? Are we moving everything through elevators and long hallways? Is the client moving in/out of a basement? How many stairs? How close can the truck get to the unit? | Now IKEA and movers are not compatible. For the movrs might not be able to move IKEA items in their assembled state. Why? Because the IKEA items are fragile, and not designed to be moved. However IKEA items are also not easily disassembled without causing damage (typically cosmetic) to the items. Therefore movers are typically required to transport IKEA items &quot;as is&quot;, and this is a challenge. For assembled IKEA items have no strength, and are not at all designed for &quot;strongman&quot; transport. . Now for all the complicated parameters that exist in moving, in this article our goal is to reduce everything to the simplest variables. Basically, if a client calls and wants to move their entire household, and wants to have an estimate (or the moving manager wants an estimate for their own schedule), then we ask the following questions: . When is the last time the client has moved their household? . | If appplicable, ask how long it took and how many &quot;human labour hours&quot; were required for their last move? . | How many &quot;bedrooms&quot; are now being moved? . | (Basic logistics: pickup and dropoff addresses). . | What kind of heavy items? (Fridges, couches, exercises equipment, etc.). . | We make some comments: If the client is only moving a select number of items, then ask client &quot;how did the item get here, how many persons were involved, how long did it take&quot;. . Question 1. gives a lower bound (&quot;a floor&quot;) for the moving manager. Our experience is that people only accumulate items, even more items, after they move. When persons are settled in a location, then they collect more and more diverse items. This always adds to the time required and increases the complexity. Question 2. gives some idea, for example was it a team of four movers or two? Was it a big truck, or a smaller cube truck? Were there any incidents during that last move, particular events or damages to the items? . Question 2. is applicable only if the client can remember the last time the item was moved. However it does help manage the clients expectations. . Question 3. is important, especially for single persons who have recently moved themselves. For every single person, there is required approximately 6 to 10 total labour hours required. I.e. two movers require approximately 3 to 5 hours to move a single person (bachelor). This is large interval, which really determines on the particular circumstances. Heavy objects can take 15 -- 30 minutes per item to move per team of two. . Questions 4, 5 are rather standard. Some estimate of the travel time and circumstances is necessary. For example, if the clients are located in an appartment building, then there is frequently a large walking distance required, and if there is an elevator involved, then the time can be much longer. .",
            "url": "https://jhmartel.github.io/fp/2022/04/05/Economics_Moving_Delivery.html",
            "relUrl": "/2022/04/05/Economics_Moving_Delivery.html",
            "date": " • Apr 5, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "Six Design Principles for BoC's CBDC",
            "content": ". Disclaimer: the author is private citizen with zero affiliation with BoC. These brief articles are based on publically available documents and are completely speculative. . This article is brief. We simply enumerate six design principles which we believe are necessary for the BoC&#39;s CBDC. These principles are obviously necessary for the BoC to fulfill and satisfy it&#39;s own mandate and charter. Thus we propose the following six propositions need be satisfied by any proposal for BoC&#39;s CBDC. . (1) CBDC&#39;s must enable negative interest rates. (savings will be subject to time decay) . (2) CBDC&#39;s will be &quot;programmeable money&quot; (nonfungible) . (3) CBDC&#39;s will require static, hardcoded digital identities for the public. (strictly one wallet per person) . (4) CBDC must allow the BoC to exclusively control and monopolize monetary supply. (monetary supply, e.g. buying and selling treasury bonds is the central banks essential primary tool). . (5) BoC must require that only &quot;authorized participants&quot; are allowed to hold and trade base layer cryptocurrencies. (public prohibited from trading base tokens). . (6) The BoC&#39;s CBDC ledger will not be auditable to the public. (privacy is essential for security). . So we could implement the CBDC as smart constracts over a base layer, like Stellar (XLM) or Ripple (XRP) or Cardano (ADA). The smart contract would have strict conditions under which it&#39;s validator function returns true. These validators would need to depend on many &quot;world&quot; parameters, e.g. specific personal identity. That is, the validator would always need to refer to an outside centralized resource before validating the transaction. . End of article. . -JHM. .",
            "url": "https://jhmartel.github.io/fp/boc/cbdc/money/2022/03/26/6ixDesignPrinciplesOfBoCsCBDC.html",
            "relUrl": "/boc/cbdc/money/2022/03/26/6ixDesignPrinciplesOfBoCsCBDC.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "Radioactive Gold Remediation",
            "content": ". Disclaimer: The author is interested in commodities and precious metals, and is generally interested in scientific questions, especially about the mysteries of atomic structure and the recent evidence of Saffire-Aureon project of remediating nuclear waste. The subject of this brief article is to simply highlight that directly testing this possibility on gold and silver bullion would significant diminish the threat of radioactive contamination of precious metals vaults. . This is a brief article. Jim Rickards has remarked in some of his videos, and his book &quot;The New Case for Gold&quot; that gold bullion bars (silver included) are at risk of radioactive contamination. For example, if a palette of bullion bars had been previously exposed to radio activity, then they could perhaps unwittingly be introduced into the general bullion vault and contaminate all the specimens. Or perhaps some radioactive material is released intentionally by bad actors and contaminating the bars. This risk is addressed by a recent amendment of the Royal Canadian Mint in its ETR (exchange-traded receipts) program. Risk of Radioactive Gold is Uninsurable . Presumably there would be an issue in the exchange and transit of such contaminated bullion. The radioactive halflife could potentially be millions of years. . However the recent experimental work of the Safire-Aureon Project has indicated that exposing radioactive materials to $H^+$ ions actually increases the decay rate! In otherwords, the decay rate depends on the atoms relative to their environment. Basically, if the contaminated bars were directly exposed to the electrical plasma environment of Aureon&#39;s &quot;star-in-a-jar&quot;, then the radioactivity could be significantly accelerated. The question is: how many orders of magnitude can the decay-rate be accelerated? Can a million year decay rate be reduced to 1 day? What would be the energy required to maintain $H^+$ bombardment? . Many more details are required, and this is only preliminary. -JHM. .",
            "url": "https://jhmartel.github.io/fp/eu/saffire/gold/pollution/2022/03/25/Radioactive_Gold_Remediation.html",
            "relUrl": "/eu/saffire/gold/pollution/2022/03/25/Radioactive_Gold_Remediation.html",
            "date": " • Mar 25, 2022"
        }
        
    
  
    
  
    
        ,"post18": {
            "title": "Brief Remarks on Designing Bank of Canada's CBDC",
            "content": ". Here we collect some hypotheses regarding the design goals of BoC (Bank of Canada)&#39;s CBDC (central bank digital currency). The author has no priviledged &quot;insider&quot; information, but reasons from the point-of-view of a public citizen who is well-informed of open source documentation, e.g. BoC working-papers, keyword = &quot;digital&quot; . So it&#39;s evident that BoC is actively investigating CBDCs, e.g. Jasper. For better or worse, the digitization of all assets marches on. Therefore if BoC wants to further digitize the monetary functions of the Bank of Canada, then alot of questions arise. First I would have to ask Why?, What are the goals of the CBDC? . For it naively appears that electronic money has already been achieved via Interac cards and the debit payment system. So what is deficient in the current form of electronic money? What is the CBDC going to solve for the BoC? What is the valued added? What is the value added for the bank versus for the public and individual?. . The entry of institutional investment into cryptocurrencies has provoked a reaction from central banks, BIS, IMF, around the world, namely because crypto&#39;s are monetary competitors. In otherwords, the public is increasingly aware of digital options for exchanging their fiat dollars into other anti-inflationary (and often speculative) digital tokens. The market cap of cryptos being actively traded and held on exchanges continues to grow. [insert specific figures]. . So competition has entered, and the central banks are reacting. Presently BoC estimates that only 5 per cent of Canadians have BTC (this is surely an underestimate), so there appears to be relatively small demand for cryptocurrencies, and almost zero demand for central bank digital currencies. . A person might be willing to invest in digital yuan, say, in the prospect of that foreign currency being backed by hard money gold reserves. But any foreign holder of digital yuan (especially at this relatively early stage) is subject to a large amount of spam, and inquiries, and phishing emails. In otherwords, acquiring the digital yuan comes with a diminished privacy and/or anonymity. This is something of a liability to the foreign holder of digital yuan. . We observe that the central banks, and specifically BoC, are keen to distinguish their CBDC&#39;s from the general cryptocurrencies. I.e. it is strongly discouraged to conflate or equivocate &quot;CBDCs&quot; with &quot;cryptos&quot;. This has various motivations, e.g. branding and marketing to control public opinion, but it&#39;s also an important technical distinction, for the CBDC will be strongly centralized with the BoC. . How? . Let&#39;s comment on centralized versus decentralized currencies. The decentralization in crypto refers to various properties of the networks. First, it refers to the fact that transactions can be confirmed by any node that either solves the nonce problem (in PoW) or is randomly elected (where the random choice is decentrally chosen). . In BTC&#39;s original design, there was an anti-inflationary mechanism, namely in the &quot;halving&quot; principle and miner rewards. The miners expend the energy and invest in the IT infrastructure to send and receive and confirm transactions in the ledger, and in most cases, the miner&#39;s receive token rewards for these services. Eventually, I think it&#39;s approximately 2150 AD, the miners will not receive any BTC rewards, and there will be a strict fee structure. I don&#39;t think anybody has any idea whether this will sufficiently motivate miners to preserve the integrity of the ledger, i.e. it might become more profitable to destroy the record of the ledger. . There are differences between proof-of-work (like original BTC) or proof-of-stake (like ETH, or ADA). But the point is that tokens are issued as rewards for miners and nodes. The various cryptocurrencies have different governance policies for how tokens are distributed, to prevent monetary inflation. This is indeed one of the standard arguments in favour of BTC as &quot;perfect money&quot; (although we don&#39;t necessarily agree with Saylor, Keiser, Breedlove, et.al. that BTC is &quot;perfect&quot; money.) . The Bank Of Canada has a monopoly on printing currency (i.e. fiat dollar bills, or electronic money), and it&#39;s extremely unlikely that they will surrender this monopoly in their CBDC. (It&#39;s basically certain, since this monopoly is the exclusive right and foundation of BoC&#39;s mandate). This is one reason why BoC cannot allow cryptocurrencies to compete with BoC&#39;s dollar. If people flee the Canadian dollar, then the main tool and mandate of the BoC (control of monetary supply) is annulled. . So we can assume that the goal of the CBDC is to have tokens which can only be issued by the centralized authority of BoC. This suggests that BoC will either develop their own network for CBDC transactions (extremely unlikely, i think), or they will attempt to build their token on top of another pre-existing network. For example, the BoC&#39;s CBDC&#39;s might essentially be smart contracts on top of either Ripple (XRP) ledger or Stellar (XLM) ledger, or something else. For security purposes I would recommend ADA (Cardano). . This raises the question What will the form of the CBDC smart contracts take? Reading the central bank literature, from our open source public civilian position, it appears that the BoC desires a CBDC with the following properties. . (1) CBDC&#39;s must enable negative interest rates. . (c.f. IMF&#39;s &quot;Breaking the zero bound&quot;, and Finance Minister Mme. Freeland&#39;s comments on &quot;savings as preloaded stimulus&quot;). This is effected by adding a time-decay factor on savings. . (2) CBDC&#39;s will be &quot;programmeable money&quot;, i.e. the CBDC will be nonfungible. . If the CBDC seeks maximal control, then purchases would be validated depending on the parameters (PersonalID, VendorID, ItemID, Price. The PersonalID will have an associated &quot;clearance&quot; or &quot;allowance&quot; of goods and services. These clearances/allowances are like digital permissions, saying &quot;This Person X is permitted to purchase Item Y from vendor Z at this time and date and location at this price up to this amount&quot;. . (3) CBDC&#39;s will require static, hardcoded digital identities for the public. . In otherwords, the pseudo-anonymity of general cryptocurrencies will be replaced with total disclosure of the users to the centralized authority. E.g., if dollar bills always had GPS and PersonalID embedded which exclusively is updated and permissioned by a centralized authority. Moreover the total history of the bill would be recorded in a ledger. . Cryptocurrencies (in general) can function with pseudo-anonymous addresses and multiple wallets for users. However the CBDC can only function with hard-coded digital identity for all public users. . (4) CBDC must allow the BoC to directly control and monopolize monetary supply. . So we must examine how the BoC will maintain CBDC supply and the &quot;fiat price&quot; of the CBDC on the chains. In principle, there exists nearly zero cost for the BoC to issue new money, i.e. they &quot;print&quot; money (brrrrr) either with electronic entries or by increasing the physical printed money supply (at some fixed cost). . Moreover, there is another issue. If the CBDC is a contract on top of, say, the Ripple ledger, then what prevents users from bypassing the BoC CBDC and directly acquiring XRP tokens? For the XRP tokens will be &quot;harder money&quot; and more functional than the CBDC, and there will be a flow from CBDC to XRP via Gresham&#39;s Law. This will require some law or federal emergency mandate that . (5) BoC must require that only authorized participants will be allowed to hold and trade base layer cryptocurrencies. . And indeed this is a trend observed around the world. . There&#39;s more to say, but I&#39;ll keep it short for now. . JHM. .",
            "url": "https://jhmartel.github.io/fp/money/boc/cbdc/2022/03/24/CBDC_Opinion.html",
            "relUrl": "/money/boc/cbdc/2022/03/24/CBDC_Opinion.html",
            "date": " • Mar 24, 2022"
        }
        
    
  
    
  
    
  
    
  
    
        ,"post22": {
            "title": "Closing Steinberg, Part 1. Mapping Class Group",
            "content": "We present the formal definition of Closing Steinberg symbols. . Let $ Gamma$ be a group acting on a space $X$ by group action $X times Gamma to X$. If $P subset X$ is a subset of $X$, then a finite subset $I$ of $ Gamma$ is said to formally close $P$ if the iterated symmetric difference of $ { gamma. P ~|~ gamma in I }$ vanishes (equal to empty set $ emptyset$). . N.B. The vanishing of the iterated symmetric difference means the chain sum $ gamma.P$, for $ gamma in I$, vanishes over mod 2 coefficients. I.e. every element in the $I$-translates of $P$ occurs an even number of times. . The above is the abstract formulation of a problem we call Closing the Steinberg symbol. In our applications the subset $P$ is a panel representing the convex hull of a sphere at infinity which is called the &quot;Steinberg symbol&quot;. . Now the key to Closing Steinberg (CS) is to find nontrivial formal solutions. We will illustrate with the specific group $ Gamma = Mod(S)$ which is the mapping class group of a compact hyperbolic surface $S$. We will begin with genus $g(S)=2$. . Now we introduce the basic functions using Mark C. Bell&#39;s curver: . Now we make the basic definitions of the reference pant, and the mapping class elements $ zeta, nu, mu$. . pant={a,b,c} ## pant={a,b,c} is the standard pant. zeta=a*e*c*f*b ## zeta is the order 6 element in MCG arising from chain relation. ## nu=a*e*c*f ## nu is order 10 element in MCG mu=nu**4 ## mu is order 5 element in MCG. . We define $ xi$ as the union of the standard pair of pants with its $ mu$-translate. The $ mu$-translate of $ {a,b,c }$ is a pair of pants dual to $ {a,b,c }$. . The mapping class element $ mu$ is an order 5 element in $Mod(S_2)$. We propose that the powers of $ mu$, namely $I= {Id, mu, mu^2, mu^3, mu^4 }$, formally close the symbol $ xi$. This solution will be nontrivial because we will establish that $ mu^i xi = mu^j xi$ if and only if $i=j$. Thus the $I$-translates of $ xi$ are distinct, while the chain sum $ sum_{i=0}^4 mu^i xi$ vanishes over ${ bf{Z}}/2$ coefficient. . ## xi is obtained by joining the initial pant p with its mu translate. xi=pant|Translate(mu, pant) ## important to verify that pant and the mu-translate are disjoint. ## Ad(mu,pant) is &quot;opposite pair of pants&quot; print(&quot;The mu translate of the standard pant is disjoint from pant. &quot;, pant &amp; Translate(mu, pant) == set()) print() M0=xi M1=Translate(mu,xi) M2=Translate(mu**2,xi) M3=Translate(mu**3,xi) M4=Translate(mu**4,xi) ## The following proves that all the symbol translates are nontrivial, and there is no complete coincidence ## between the translated symbols. print(&quot;The mu translates of xi are all pairwise distinct:&quot;, M0!=M1 and M0!=M2 and M0!=M3 and M0!=M4 and M1!=M2 and M1!=M3 and M1!=M4 and M2!=M3 and M2!=M4 and M3!=M4 ) print() ## The following proves that the total chain sum of the translated symbols vanishes mod 2. ## I.e. the iterated symmetric difference of the translated symbols is equal to empty set. print(&quot;The iterated symmetric difference of the mu translates is empty.&quot;, ((((M0^M1))^M2)^M3)^M4 ==set()) print() print(&quot;The mu-orbit of xi is supported on ten curves.&quot;, 10==len(M0|M1|M2|M3|M4) ) print() print(&quot;Therefore we find I={Id, mu, mu**2, mu**3, mu**4} is a formal solution to Closing the Steinberg symbol xi in genus two.&quot;) print(&quot;&quot;) . The mu translate of the standard pant is disjoint from pant. True The mu translates of xi are all pairwise distinct: True The iterated symmetric difference of the mu translates is empty. True The mu-orbit of xi is supported on ten curves. True Therefore we find I={Id, mu, mu**2, mu**3, mu**4} is a formal solution to Closing the Steinberg symbol xi in genus two. . So we have found a formal solution $I$ to CS. For applications we need further verify that $I$ satisfies further geometric properties. Specifically we need establish: . the $I$-translates of $ xi$ have a well-defined convex hull $F:=conv(I. xi)$ in $Teich(S)$. | the $ Gamma$-translates of $F$ generate a chain sum $ underline{F}:= sum_{ gamma in Gamma} gamma.F$ with well separated gates equal to the $ Gamma$-translates of $ xi$. | . Informally the idea is that $ xi$ represents a &quot;panel&quot; $P$, and $I$ closes the panel in the sense that the $I$-translates of the panel assemble to a closed ball. (Similar to how the (triangular, hexagonal) panels of a soccer ball assemble to form the closed ball). . But we must further study the $ Gamma$-translates of the ball itself, i.e. of the convex hull $F$. Most important for our setting is that the intersections of the various translates $ gamma F cap gamma&#39; F$ have a &quot;standard form&quot;, namely isometric to $ xi$ (the panel $P$). . Remark. It&#39;s not clear whether the above verification of ``well-separated gates&quot; can be performed in curver. While we are capable of considering the $ Gamma$ action on the elements of $I. xi$, we cannot necessarily compute the intersections $F cap gamma F$. The issue is that $F cap gamma F$ can intersect &quot;at-infinity&quot; (i.e. be asymptotic) without the convex hulls having an intersection in the interior of $Teich(S)$. Formally the solutions to CS solve a problem at infinity, but the next step is to study the solutions in the interior, and this becomes more geometric. .",
            "url": "https://jhmartel.github.io/fp/closing%20steinberg/curver/mcg/2022/02/28/ClosingSteinberg_Intro.html",
            "relUrl": "/closing%20steinberg/curver/mcg/2022/02/28/ClosingSteinberg_Intro.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "Positronium Part I.",
            "content": ". Today we begin the study of Weber&#39;s potential in the isolated two-body system consisting of an electron and positron pair $e^-$ and $e^+$. We assume the particles $e^ pm$ have equal mass $m=m_{e^{ pm}}$. The reduced mass is concentrated at the centre-of-mass $ mu=m/2$. . Weber&#39;s force is attractive between the pair $e^ pm$ at all distances. . The particles $e^ pm$ do not indefinitely spiral inwards. Simulations indicate that the radial distance between $e^ pm$ stays strictly bounded between two upper and lower limits $$0 &lt; r_{lower} leq r leq r_{upper} &lt; + infty .$$ This is rigorously proved in Weber-Clemente, 1990.pdf). . If the electron is indivisible particle, then the above two-body problem models a pair $e^-$ and $e^+$ of isolated electron and positron. . But do the particles $e^ pm$ ever &#39;collide&#39; and annihalate? . In the standard physics textbooks, it seems well known that annihalation between $e^ pm$ occurs and two gamma rays are ejected in opposite directions when $e^ pm$. conserving momentum, etc., and converting all their mass into energy. Thus it&#39;s determined that two gamma rays of energy $0.511 keV$ are released, where Einstein&#39;s formula $E=m_ec^2$ is applied, where $m_e$ is the reduced mass. [ref] The annihalation of $e^ pm$ is apparently an experimental test of the validity of Einstein&#39;s &quot;mass-energy&quot; hypothesis. . But what does Weber&#39;s potential say about the annihalation of $e^+$ and $e^-$ ? . If we know the centre of mass has zero net force, then we can replace the positions $r_1$, $r_2$ of the particles by their relative distance $r_{12}$ from the centre of mass. This yields $$r_1=R + frac{m_2}{m_1+m_2} r_{12}$$ and $$r_2=R - frac{m_1}{m_1+m_2} r_{12}.$$ . Applying Newton&#39;s Second Law that $F_{21}=-F_{12}$ yields the following equation for $r_{12}&#39;&#39;$: $$ mu . r_{12}&#39;&#39; = F_{21},$$ where $ mu$ is the reduced mass of the system, namely $ mu= frac{m_1 m_2}{m_1+m_2}=0.5$. . In the following equations we use numpy.odeint to solve Weber equations of motion of the relative distance $r_{12}$. Therefore we have reduced the two-body problem to a one-body problem. This is a standard reduction. . Given the solution for $r_{12}$, how do we reconstruct the paths/positions of the particles $r_1$, $r_2$ ? Answer: via the relation $r_1=R+ frac{m_2}{m_1+m_2} r_{12}$ and $r_2=R- frac{m_1}{m_1 + m_2}r_{12}$. . Now the relative distance $r_{12}$ is a type of radial distance, and if $r, omega$ is spherical coordinates, then we have $$r&#39;^2=|v|^2= x&#39;^2+y&#39;^2+z&#39;^2=(r&#39;)^2+r^2 ( theta&#39;)^2. $$ The above formula is the usual $|v|^2=v_r^2+v_t^2$, and the tangent velocity $v_t$ satisfies $v_t=r theta&#39;$, where $ theta&#39;$ is the angular velocity. . The conservation of angular momentum says that the angular moment $L= mu r times v$, where $v$ is the linear velocity of $r$, is constant along the motion. Moreover one has $$|L|= mu r^2 theta&#39;.$$ Thus we find the formula $$ theta&#39;= frac{|L|}{ mu r^2}.$$ This implies $$T= frac{ mu}{2}v^2= frac{ mu}{2}[(r&#39;)^2+ frac{|r times v|^2}{r^{2}}]$$ represents the kinetic energy of the system. . The conservation of energy says $T+U$ is constant along trajectories. . # Here we define basic functions. def cross(v1, v2): x1, y1, z1 = v1 x2, y2, z2 = v2 return [y1*z2 - z1*y2, -(x1*z2 - z1*x2), x1*y2 - y1*x2 ] def rho(rel_position): x,y,z = rel_position return (x*x+y*y+z*z)**0.5 def dot(vector1, vector2): x1, y1, z1 = vector1 x2, y2, z2 = vector2 return x1*x2+y1*y2+z1*z2 def rdot(position, vector): return dot(position, vector)/rho(position) def norm(rel_velocity): return rho(rel_velocity) mu=0.5 ## reduced mass of the system. We assume m1 and m2 are equal, hence mu=1/2. c=1.0 ## speed of light constant in Weber&#39;s potential # Define the angular momentum def AngMom(rel_position, rel_velocity): return cross(rel_position, rel_velocity) def L(rel_position, rel_velocity): return norm(cross(rel_position, rel_velocity)) # Linear Kinetic Energy def T(rel_position, rel_velocity): vt = norm(cross(rel_position, rel_velocity)) # next formula decomposes v^2=(vr)^2+(vt)^2, where vt=r*θ&#39;=|L|/(mu*r) return (mu/2)*(rdot(rel_position, rel_velocity)**2) + (mu/2)*(rho(rel_position)**-2)*(vt**2) ## Weber Potential Energy ## Negative sign given -1=q1*q2 def U(rel_position, rel_velocity): x,y,z = rel_position vx,vy,vz = rel_velocity rdot=dot(rel_position, rel_velocity)/rho(rel_position) return -(1/rho(rel_position))*(1-(rdot*rdot)/2) . . import numpy as np from scipy.integrate import odeint, solve_ivp import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D ## Integrating two-body isolated system of oppositely charged particles ## i.e. positron+electron pair. ## The product of the charges q1*q2 is factor in Weber&#39;s force law, and appears twice ## in the formula of Newton&#39;s F=ma. def weber(t, rel_state): x, y, z, vx, vy, vz = rel_state r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r A=(-1)*r**-2 ## minus sign from q1*q2 B=1-(rdot*rdot)/2 C=(mu+((c*c*r)**-1))**-1 ## +plus instead of -minus. dxdt = vx dydt = vy dzdt = vz dvxdt = (x/r)*A*B*C dvydt = (y/r)*A*B*C dvzdt = (z/r)*A*B*C return [dxdt, dydt, dzdt, dvxdt, dvydt, dvzdt] t_span = (0.0, 100.0) t = np.arange(0.0, 100.0, 0.1) y1=[2.0,0,0,] # initial relative position v1=[-0.4, 0.4, 0] # initial relative velocity result = odeint(weber, y1+v1, t, tfirst=True) #here odeint solves the weber equations of motion relative y1+v1 for t. Energy=T(y1,v1) + U(y1,v1) print(&#39;The initial total energy T+U is equal to:&#39;, Energy) print(&#39;The initial angular momentum is equal to&#39;, norm(AngMom(y1,v1))) fig = plt.figure() ax = fig.add_subplot(1, 2, 1, projection=&#39;3d&#39;) ax.plot(result[:, 0], result[:, 1], result[:, 2]) ax.set_title(&quot;position&quot;) ax = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax.plot(result[:, 3], result[:, 4], result[:, 5]) ax.set_title(&quot;velocity&quot;) . The initial total energy T+U is equal to: -0.37999999999999995 The initial angular momentum is equal to 0.8 . Text(0.5, 0.92, &#39;velocity&#39;) . What does the above plot demonstrate? . It reveals a precession motion around the centre of mass. This is not predicted by Coulomb&#39;s force, which bounds the trajectories to elliptical orbits like Newton&#39;s Law of Gravitation. . The force is central, therefore we have conservation of angular momentum, and implies the system is constrained to a plane, namely orthogonal to the angular moment of the system. . Moreover the system satisfies a conservation of linear momentum, namely the sum $T+U$ is constant. . Problem: Verify Assis-Clemente&#39;s 1990 formula for the lower and upper limits of the relative distance along the orbits. | . import matplotlib.pyplot import pylab r_list=[] for j in range(1000): sample_position=[result[j,0], result[j,1], result[j,2]] sample_velocity=[result[j,3], result[j,4], result[j,5]] r_list.append( ( int(j), rho(sample_position) ) ) prelistr = list(zip(*r_list)) pylab.scatter(list(prelistr[0]),list(prelistr[1])) pylab.xlabel(&#39;time&#39;) pylab.ylabel(&#39;rho&#39;) pylab.title(&#39;Solutions have Upper and Lower Limits&#39;) pylab.show() #TU_list=[] #for j in range(1000): # sample_position=[result[j,0], result[j,1], result[j,2]] # sample_velocity=[result[j,3], result[j,4], result[j,5]] # TU_list.append( # (rho(sample_position),T(sample_position,sample_velocity)+U(sample_position, sample_velocity))) #prelist1 = list(zip(*TU_list)) #pylab.scatter(list(prelist1[0]),list(prelist1[1])) #pylab.xlabel(&#39;distance r&#39;) #pylab.ylabel(&#39;T+U&#39;) #pylab.title(&#39;&#39;) #pylab.show() # The plot below demonstrates the conservation of angular momentum. # Note that rdot is directly equal to the sample_velocity. I.e. there is # no need to define rdot=v.hatr/r. This was error. #A_list=[] #for j in range(1000): # sample_position=[result[j,0], result[j,1], result[j,2]] # sample_velocity=[result[j,3], result[j,4], result[j,5]] # A_list.append( # (rho(sample_position), norm(cross(sample_position, sample_velocity)) ) ) #prelist2 = list(zip(*A_list)) #pylab.scatter(list(prelist2[0]),list(prelist2[1])) #pylab.xlabel(&#39;rho&#39;) #pylab.ylabel(&#39;Angular Momentum&#39;) #pylab.title(&#39;Conservation of Angular Momentum&#39;) #pylab.show() #rho_list=[] #for j in range(180): # rho_list.append( # (int(j), rho([result[j,0], result[j,1], result[j,2]]), # ) # ) . . from sympy import * t=symbols(&#39;t&#39;) m=symbols(&#39;m&#39;) c=symbols(&#39;c&#39;) r=Function(&#39;r&#39;)(t) P=Function(&#39;P&#39;)(r,t) F=Function(&#39;F&#39;)(r,t) U=-(r**-1)*(1-((r.diff(t))**2)*(2*c*c)**-1) F=(-1)*(U.diff(t))*((r.diff(t))**-1) pprint(simplify(U)) print() pprint(simplify(F)) ## symbolic computation of the Force law. . . 2 ⎛d ⎞ ⎜──(r(t))⎟ 2 ⎝dt ⎠ - c + ─────────── 2 ────────────────── 2 c ⋅r(t) 2 ⎛d ⎞ 2 ⎜──(r(t))⎟ 2 d ⎝dt ⎠ - c - r(t)⋅───(r(t)) + ─────────── 2 2 dt ─────────────────────────────────── 2 2 c ⋅r (t) .",
            "url": "https://jhmartel.github.io/fp/weber/positronium/two-body/2022/02/22/Positronium_Part1.html",
            "relUrl": "/weber/positronium/two-body/2022/02/22/Positronium_Part1.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "One-Dimensional Ampere Force",
            "content": ". One-Dimensional Ampere . We continue with our discussion of Ampere&#39;s force, and consider the special case where the current elements are all pairwise parallel and colinear. In otherwords we assume the current elements belong the a line $ ell$ in $ bf{R}^3$. For simplicity we consider the current elements belonging to the $z$-axis. . The state of a current element on the $z$-axis is then represented by a real-valued function $v(x)$ representing the velocity of the current element. Likewise we interpret $|v(x)|$ as the intensity of the current element. Ampere&#39;s force law in the one-dimensional case then becomes: . $$F_{y ~ text{on}~x} =|v(x)|~|v(y)|~v(x)~v(y) ~ nabla_1 ( frac{1}{r}) . $$ . If we integrate the force $F$ over the entire line we obtain $$F_{ text{current on}~ x} = |v(x)|~ v(x) ~[ nabla_1 cdot int dy~ frac{|v(y)| v(y)}{x-y} ]. $$ . The hypothesis of the force being balanced on the line is equivalent to the vanishing $$ nabla_1 ~ int dy frac{|v(y)|~ v(y)}{x-y}= frac{d}{dx} ~ int dy frac{|v(y)|~ v(y)}{x-y}=0 .$$ Equivalently, the force is balanced if the integral $$H(v)(x)=h(x):= int_{- infty}^{+ infty} dy~ frac{|v(y)|~ v(y)}{x-y} $$ is a constant function of $x$ wherever $v(x) neq 0$. . N.B. The integral defining $H(v)$ is taken over the entire real line. We observe that there is no absolute value sign on the denominator $x-y$. This is to include the direction of $ hat{r}$ in the original Ampere law. . Our definition of $H$ is Hilbert&#39;s singular integral transform for real-valued functions. Strictly speaking, the singular integral does not always integrate to a finite number. In the literature, the integral is frequently replaced with a Cauchy principal value. This is related to the function $ frac{1}{y}$ being not absolutely integrable on the real line. However there is sufficient cancellation in the integral to obtain a well-defined principal value. . If the current $v$ is everywhere constant, then symmetry shows the integral $H(v)$ is identically zero, and therefore the total force $F$ of the current is everywhere zero. Therefore we find $v=constant$ is a solution to the balance equation. . N.B. Most discussion of the Hilbert transform is restricted to square-integrable functions on the line $L^2( bf{R})$. On $L^2$ one finds the Hilbert transform defines a unitary operator and is an anti-involution, satisfying $H circ H=-Id$. However the only $L^2$ constant function is the zero function. . Question: Are there any time-independant solutions to the balance equation, i.e. does there exist nonconstant $v in L^ infty( bf{R})$ with $H(v)$ constant? . Question: Can we find time-dependant solutions to the balance equation? I.e. find functions $v_t in L^ infty$ such that $H(v_t)$ is constant for all time $t$. . Remark. The Hilbert transform (modulo homothety) is the only singular operator in one-dimensions which commutes with the affine translation and affine dilatation. This corresponds to the Ampere force commuting with the affine change of variables $x mapsto ax+b$. Since Ampere&#39;s force is relational, it seems proper that the corresponding Hilbert transform also possess these same properties. . Remark. It appears that the only Borel-Radon measures $ mu$ on the real line $ bf{R}$ which have a constant Hilbert transform $H( mu)=c$ are the uniform measures $ mu=const.dy$. If this remains true, then we identify the kernel of $H circ H$ with the uniform measures on $ bf{R}$. .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/02/18/One_Dimensional_Ampere.html",
            "relUrl": "/fastpages/jupyter/2022/02/18/One_Dimensional_Ampere.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "Weber's Critical Radius, Work, and E=mc2 Formulas",
            "content": ". In the [previous post] we discussed the equations of motion of a charged particle in free fall w.r.t. Weber&#39;s potential $U$. Now we consider the problem of work, and the problem of moving particles &quot;upstream&quot; of the potential. . Recall the Coulomb-type expression that &quot;opposite charges attract&quot; and &quot;like charges repel&quot;. If we arranged some charged particles on a plate, then in a short time these particles would either be repelled outwards to the boundary of the plate or the opposite charges would cancel on the interior. Again this is with respect to the Coulomb model $V(r)=1/r$. . But Weber&#39;s model has the following amazing prediction: &quot;opposite charges attract except at a small critical distance $r=r_c$ where the charges acquires a negative inertial mass and the Weber force becomes repulsive&quot;. Furthermore, &quot;like charges repel except at a small critical distance where the Weber force becomes attractive&quot;. This latter prediction leads to Weber&#39;s *planetary model of the atom&quot;. [Insert ref]. . In the (cgs) units the critical radius $r_c$ is computed as $r_c= frac{1}{m c^2}$ where $m$ is the relative inertial mass of one of the particles. The proof of this formula is an application of Newton&#39;s second law, that $m a = bf{F}$ where $ bf{F}$ is Weber&#39;s force. [insert ref] . Now the question arises: if we have two identical but opposite electrically charged particles, say $(-1)e$ and $(+1)e$, where $e$ is a small mass, then how much work is required to drive $(-1)e$ and $(+1)e$ to within Weber&#39;s critical radius $r_c$? . Weber comments in his final memoir [ref] that an infinite amount of work would be required, however no technical details are provided. In fact this author thinks its evident that a large finite amount of work is required, and the computation of this work required is the subject of this post. . How to compute Work: We imagine a particle moving through a potential $U$ along a trajectory $ gamma.$ Our goal is to determine how much work is required to breach (&quot;pass through&quot;) the critical radius $r_c= frac{1}{mc^2}$ in units where $4 pi epsilon_0 = 1$. . We begin assuming the particle&#39;s trajectory is rectilinear. This means the tangent $ gamma&#39;$ and $ gamma$ are parallel for all values of the parameter. Moreover we can use the Riemannian idea of parameterizing the curve by arclength. Then we ask how much work is required to move the particle through $U$ at a constant rate of speed, namely $|| gamma&#39;||=1$. . Weber&#39;s force is conservative, so there is some path independance, however the terminal conditions are not fixed a priori. It might happen that breaching the Weber radius is easier if the particles velocity is nearly parallel to the &quot;virtual&quot; surface of the Weber&#39;s critical radius. That is to say, the curvature term $r r&#39;&#39;/c^2$ in Weber&#39;s force law might sometimes reduce the work needed, depending on the sign. . In terms of the relational variables recall the useful formula/definition $$ r&#39; = frac{ bf{r} cdot bf{r&#39;}}{r}.$$ In our setting we find $$r&#39;= frac{ gamma cdot gamma&#39;}{|| gamma||}.$$ Applying the Cauchy-Schwartz inequality, we find $$r&#39;=1.$$ I.e. equality is obtained in Cauchy-Schwartz because $ gamma, gamma&#39;$ are parallel by hypothesis and $|| gamma&#39;||=1$. . Moreover the rectilinear motion implies $r&#39;&#39;=0$, i.e. the trajectory has zero curvature, since the direction of the particle does not change. Therefore Weber&#39;s force leads to work being computed by the integral $$W=(1- frac{1}{2c^2}) int_{+ infty}^{r_c} frac{1}{r^2} dr = (1- frac{1}{2c^2})mc^2=mc^2-m/2.$$ So what is the total energy of the above system? The total energy is the work required $mc^2 - m/2$ plus the initial kinetic energy $T_i=m| gamma&#39;|^2/2=m/2$. Therefore the total energy required to breach the Weber critical radius is $E=W+T_i=mc^2$. . N.B. All the while our particles are &quot;travelling upstream&quot;. So the above computation indicates that if a particle is given sufficient energy (i.e. sufficient kinetic energy, then the particle could breach the Weber critical radius, and arriving at the Weber radius with almost zero kinetic energy). . N.B. The integral representing $W$ is parameterization independant. Therefore the work required by the unit-parameterized path is not overly specialized, but represents the general computation. . The above discussion was restricted to rectilinear trajectories. But the possibility remains that curvilinear (&quot;spiralling&quot;) trajectories require less work to breach the Weber radius. Thus while it appears that breaching the Weber radius via rectilinear paths requires large energy $ approx mc^2$, perhaps the spiralling paths -- where the curvature term maintains a definite sign -- are the more interesting. . Problem: Determine the minimum energy required for an isolated two-body system to breach the Weber critical radius. . Answer: The minimum energy required to breach the critical radius is $E=mc^2$. . This is consequence of the fact that $ bf{F}$ is a conservative force, therefore dependant only on the initial and terminal states, and not the path taken. Moreover the work done by a particle traversing a path $ gamma$ depends only on the difference in potential energies. This implies that the above evaluation in the rectilinear case is essentially the same for all paths from some initial point to to within the critical radius. . Is the energy spectrum within the critical radius continuous? (Yes, i think the orbits can have arbitrary energy within the radius). | How to relate Birkeland currents (FFAC) to Weber&#39;s potential? Don Scott considers the plasma cylinder, with a stead current. Then the min energy state has zero internal pressure, therefore the net internal energy of the plasma cylinder needs be minimized, i.e. vanishing. Difficulty: without the Maxwell equations we do not know relationship between the current flow $I d ell=qdv$. This equality is Weber&#39;s hypothesis, i.e. a current flow is equivalent to a charge in motion, where $dv$ represents the instantaneous velocity of the charged particle. |",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/02/16/WeberEMC2.html",
            "relUrl": "/fastpages/jupyter/2022/02/16/WeberEMC2.html",
            "date": " • Feb 16, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "Phipp's Potential",
            "content": "Here we examine Phipp&#39;s potential $P= frac{1}{r} sqrt{1- frac{r&#39;}{c}}$. It&#39;s clear that Phipp&#39;s potential imposes an upper bound on the velocities of particles in free fall relative to $P$. But while Phipp&#39;s potential guarantees subluminal velocities, it does not appear to satisfy a conservation of energy, and the quantity $T+P$ is not constant along trajectories. . The Phipp potential can induce negative effective mass at small distances, similar to the case of Weber&#39;s potential. However the critical radius depends on the relative velocity of the particles. By contrast Weber&#39;s critical radius depends only on the mass $m$ and $c^2$. . These two facts are demonstrated in the cells below, and demonstrates some severe disadvantages to Phipp&#39;s potential. We are therefore inclined to investigate Weber&#39;s potential as a model of electrodynamics. . from sympy import * t=symbols(&#39;t&#39;) m=symbols(&#39;m&#39;) c=symbols(&#39;c&#39;) r=Function(&#39;r&#39;)(t) P=Function(&#39;P&#39;)(r,t) F=Function(&#39;F&#39;)(r,t) P=(r**-1)*sqrt(1-r.diff(t)/c) F=(-1)*(P.diff(t))*((r.diff(t))**-1) pprint(simplify(P)) print() pprint(simplify(F)) ## symbolic computation of the Force law. . ______________ ╱ d ╱ c - ──(r(t)) ╱ dt ╱ ──────────── ╲╱ c ──────────────────── r(t) 2 d r(t)⋅───(r(t)) 2 ⎛ d ⎞ d dt ⎜c - ──(r(t))⎟⋅──(r(t)) + ────────────── ⎝ dt ⎠ dt 2 ──────────────────────────────────────── ______________ ╱ d ╱ c - ──(r(t)) ╱ dt 2 d c⋅ ╱ ──────────── ⋅r (t)⋅──(r(t)) ╲╱ c dt . import numpy as np from scipy.integrate import odeint, solve_ivp import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D m=0.5 ## mass of second test particle c=1.0 ## speed of light constant in Weber&#39;s potential ## Kinetic Energy: we use the naive expression for vis viva. Is it correct? def T(vx, vy, vz): return m*(vx*vx+vy*vy+vz*vz)/2 ## Phipp&#39;s Potential Energy def P(x,y,z,vx,vy,vz): r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r return (r**-1)*(1-rdot/c)**0.5 ## Integrating equations of motion relative Phipp&#39;s Force Law. def phipp(t, state): x, y, z, vx, vy, vz = state r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r A=r**-2 B=(1-rdot)**0.5 C=(m- ( 2*r*rdot*((1-rdot)**0.5) )**-1 )**-1 dxdt = vx dydt = vy dzdt = vz dvxdt = (x/r)*A*B*C dvydt = (y/r)*A*B*C dvzdt = (z/r)*A*B*C return [dxdt, dydt, dzdt, dvxdt, dvydt, dvzdt] t_span = (0.0, 1000.0) t = np.arange(0.0, 1000.0, 0.001) y0=[10, 0.3, 0, -0.2, -0.9, .1] ## initial state of the system y0=[x, y, z, vx, vy, vz] result = odeint(phipp, y0, t, tfirst=True) Energy=T(y0[3], y0[4], y0[5] ) + P(y0[0],y0[1],y0[2],y0[3],y0[4],y0[5]) print(&#39;The initial total energy T+P is equal to:&#39;, Energy) print(&#39;The luminal energy is:&#39;, m*c*c/2) print(&#39;The energy is subliminal:&#39;, Energy &lt; m*c**2 /2) r = ( y0[0]*y0[0] + y0[1]*y0[1] + y0[2]*y0[2])**0.5 rdot = ( y0[0]*y0[3] + y0[1]*y0[4] + y0[2]*y0[5] ) /r print(&#39;Phipp`s critical radius is equal to:&#39;,(m- ( 2*r*rdot*((1-rdot)**0.5) )**-1)**-1) ## N.B. Phipp&#39;s critical radius depends on the distance and velocity! print(&#39;The initial radius r and velocity r` is within Phipp`s critical distance:&#39;, r &lt; (m*c*c)**-1 ) fig = plt.figure() ax = fig.add_subplot(1, 2, 1, projection=&#39;3d&#39;) ax.plot(result[:, 0], result[:, 1], result[:, 2]) ax.set_title(&quot;position&quot;) ax = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax.plot(result[:, 3], result[:, 4], result[:, 5]) ax.set_title(&quot;velocity&quot;) . The initial total energy T+P is equal to: 0.3257156133373373 The luminal energy is: 0.25 The energy is subliminal: False Phipp`s critical radius is equal to: 1.4309087883533131 The initial radius r and velocity r` is within Phipp`s critical distance: False . Text(0.5, 0.92, &#39;velocity&#39;) . import matplotlib.pyplot import pylab def rho(x,y,z): return (x*x+y*y+z*z)**0.5 v_list=[] for j in range(4000): v_list.append( (rho(result[j,0], result[j,1], result[j,2]), T(result[j,3], result[j,4], result[j,5])) ) P_list=[] for j in range(4000): P_list.append( (rho(result[j,0], result[j,1], result[j,2]), P(result[j,0],result[j,1],result[j,2],result[j,3],result[j,4],result[j,5])+T(result[j,3], result[j,4], result[j,5])) ) prelist1 = list(zip(*v_list)) pylab.scatter(list(prelist1[0]),list(prelist1[1])) pylab.show() prelist2 = list(zip(*P_list)) pylab.scatter(list(prelist2[0]),list(prelist2[1])) pylab.show() .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/02/11/PhippPotential.html",
            "relUrl": "/fastpages/jupyter/2022/02/11/PhippPotential.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "Letter To Prof. Assis.",
            "content": "2022-02-10-Verifying Conservation of Energy with Python. . Dear Professor Assis, . I am currently studying the Weber potential $U= frac{e_1 e_2}{r} (1- frac{r&#39;^2}{2c^2})$ in python, and specifically looking at the equations of motion of an isolated two-body system. For computation i have chosen one particle to be the origin, and look to solve the equations of motion for the second particle. In computations I take $e_1=e_2=1$ and $c=1$, and the relative mass of the second particle to be $m=0.5$. (Is that mass to small?) . I am fairly confident that I have correctly integrated the equations of motion using python&#39;s odeint. . However I am not able to satisfactorily verify conservation of energy $d(T+U)=0$ along the trajectories, and I am not confident that I have the correct expression for kinetic energy $T$, i use the naive expression $T=mv^2/2$, where $v^2$ is computed in usual way as sum of squares of the velocities of the second particle of mass $m=0.5$. When I use the above expression for $T$ I find the sum $T+U$ is not constant along solutions to the equations of motion $mr&#39;&#39;=ma=F$, where $F=- hat{r} frac{dU}{dr}$ is Weber&#39;s force. . From your book on &quot;Relational Mechanics&quot; i understand that kinetic energy is more properly defined as the interaction energy of the particle with the stars at infinity. But is the naive kinetic energy $T=mv^2/2$ the correct expression for the isolated two-body system? . import numpy as np from scipy.integrate import odeint, solve_ivp import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D m=0.5 ## mass of second test particle c=1.0 ## speed of light constant in Weber&#39;s potential ## Kinetic Energy: we use the naive expression for vis viva. Is it correct? def T(vx, vy, vz): return m*(vx*vx+vy*vy+vz*vz)/2 ## Weber Potential Energy def U(x,y,z,vx,vy,vz): r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r return (r**-1)*(1-(rdot**2)/2) ## Integrating equations of motion relative Weber&#39;s Force Law. ## I think I have implemented everything correctly. The formula for rdot is from Relational Mechanics, pp.168 ## but perhaps I have the wrong formula for r&#39;&#39; and have not applied Newtons second law F=mr&#39;&#39; correctly? def weber(t, state): x, y, z, vx, vy, vz = state r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r A=r**-2 B=1-(rdot*rdot)/2 C=(m-((c*c*r)**-1))**-1 dxdt = vx dydt = vy dzdt = vz dvxdt = (x/r)*A*B*C dvydt = (y/r)*A*B*C dvzdt = (z/r)*A*B*C return [dxdt, dydt, dzdt, dvxdt, dvydt, dvzdt] t_span = (0.0, 40.0) t = np.arange(0.0, 40.0, 0.001) y0=[0, 1.2, 0, 0, -0.4, .1] ## initial state of the system y0=[x, y, z, vx, vy, vz] result = odeint(weber, y0, t, tfirst=True) Energy=T(y0[3], y0[4], y0[5] ) + U(y0[0],y0[1],y0[2],y0[3],y0[4],y0[5]) print(&#39;The initial total energy T+U is equal to:&#39;, Energy) print(&#39;The luminal energy is:&#39;, m*c*c/2) print(&#39;The energy is subliminal:&#39;, Energy &lt; m*c**2 /2) r=(y0[0]*y0[0] + y0[1]*y0[1] + y0[2]*y0[2])**0.5 print(&#39;Webers critical radius is equal to:&#39;,(m*c*c)**-1) print(&#39;The initial radius r is within the Weber critical distance:&#39;, r &lt; (m*c*c)**-1 ) fig = plt.figure() ax = fig.add_subplot(1, 2, 1, projection=&#39;3d&#39;) ax.plot(result[:, 0], result[:, 1], result[:, 2]) ax.set_title(&quot;position&quot;) ax = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax.plot(result[:, 3], result[:, 4], result[:, 5]) ax.set_title(&quot;velocity&quot;) . The initial total energy T+U is equal to: 0.8091666666666666 The luminal energy is: 0.25 The energy is subliminal: False Webers critical radius is equal to: 2.0 The initial radius r is within the Weber critical distance: True . Text(0.5, 0.92, &#39;velocity&#39;) . import matplotlib.pyplot import pylab def rho(x,y,z): return (x*x+y*y+z*z)**0.5 v_list=[] for j in range(4000): v_list.append( (rho(result[j,0], result[j,1], result[j,2]), T(result[j,3], result[j,4], result[j,5])) ) U_list=[] for j in range(4000): U_list.append( (rho(result[j,0], result[j,1], result[j,2]), U(result[j,0],result[j,1],result[j,2],result[j,3],result[j,4],result[j,5])+T(result[j,3], result[j,4], result[j,5])) ) prelist1 = list(zip(*v_list)) pylab.scatter(list(prelist1[0]),list(prelist1[1])) pylab.xlabel(&#39;distance r&#39;) pylab.ylabel(&#39;Kinetic Energy T&#39;) pylab.title(&#39;rT plot&#39;) pylab.show() prelist2 = list(zip(*U_list)) pylab.scatter(list(prelist2[0]),list(prelist2[1])) pylab.xlabel(&#39;distance r&#39;) pylab.ylabel(&#39;T+U&#39;) pylab.title(&#39;Conservation of Energy&#39;) pylab.show() ## Error. Expect to see conservation of energy T+U = constant in the second figure. However the sum T+U appears nonconstant. . Discussion: The horizontal axis is the radial distance $r$ from the origin, and the vertical axis is the energy value. We expect the second figure to be a horizontal straight line. It does appear basically flat except for blow-up behaviour at small distance. Perhaps given the relative size of the interval of integration, namely $0.001$, the total energy $T+U$ is constant within error. . Is this loss/gain of energy a defect from the odeint routine? In otherwords, is energy not conserved because of cumulative errors and approximations in the solution? . for j in range(50): print( U(result[j,0],result[j,1],result[j,2],result[j,3],result[j,4],result[j,5]) + T(result[j,3], result[j,4], result[j,5]) ) . 0.8091666666666666 0.8091694528769837 0.8091722560959542 0.8091750763849768 0.8091779137441633 0.8091807683256578 0.8091836401044897 0.8091865291060831 0.8091894353679061 0.8091923589396186 0.8091952998832174 0.8091982582731821 0.8092012341218966 0.8092042274175154 0.8092072382239642 0.8092102666157523 0.8092133126780814 0.8092163765069865 0.8092194582094769 0.8092225578481547 0.8092256753069924 0.8092288106302348 0.8092319638914872 0.8092351351750412 0.8092383245760144 0.8092415322004902 0.8092447570890509 0.8092480017730359 0.8092512645080137 0.8092545454217692 0.8092578446432682 0.8092611623026272 0.8092644985310876 0.8092678534609862 0.809271227118879 0.8092746194540041 0.8092780305448041 0.8092814604679077 0.8092849092969379 0.8092883771024215 0.8092918639517022 0.8092953699088504 0.8092988950345746 0.80930243938613 0.8093060030172264 0.8093095859779383 0.8093131883146094 0.8093168100697619 0.8093204512819989 0.8093241120128701 . Radius of Weber&#39;s Planetary Atomic Model . Now we consider the diameter of Weber&#39;s critical radius if the test particles $x_1$, $x_2$ are equal to a positron, electron pair. . The mass of the electron is: [ref]. . Lagrangian L=T-U . Given the conservation of energy $d(T+U)=0$ it seems natural to consider the so-called Lagrangian $L=T-U$. But what do we expect from this lagrangian? Is there any reason to believe that a &quot;minimum action&quot; principle holds for two-body systems? Does the above naive expression for $L$ actually correspond to a reasonable action functional on the state space? . L_list=[] for j in range(4000): L_list.append( (rho(result[j,0], result[j,1], result[j,2]), -U(result[j,0],result[j,1],result[j,2],result[j,3],result[j,4],result[j,5])+T(result[j,3], result[j,4], result[j,5])) ) prelist2 = list(zip(*L_list)) pylab.scatter(list(prelist2[0]),list(prelist2[1])) pylab.show() .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/02/10/LetterToAssisWeberPotentials.html",
            "relUrl": "/fastpages/jupyter/2022/02/10/LetterToAssisWeberPotentials.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post28": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jhmartel.github.io/fp/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Welcome to JHM Labs! . The purpose of this website is to broadcast and record our ideas in short articles. This website is powered by fastpages 1. . JHM is me, a mathematician and general good guy from Aylmer, QC, Canada. Our goal is to develop a math-physics lab for 21st century. Our research includes python, jupyter notebooks, optimal transport, topology of singularities, energy and electrodynamics, history of science, computation, mapping class groups, geometric topology, and whatever else we think about. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jhmartel.github.io/fp/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jhmartel.github.io/fp/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}