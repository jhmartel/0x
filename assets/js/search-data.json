{
  
    
        "post0": {
            "title": "Integrating Newton-Weber Equations for Two Body Electrical Systems.",
            "content": ". Newton-Weber Equations in Isolated Two-Body Systems and the Critical Radius: . Our goal is to simulate Weber&#39;s prediction of the stable $(-1)+(-1)$ molecule. . Weber&#39;s electrical force law predicts that it&#39;s possible for two identical negatively charged particles to become a stable molecule of net charge $-2=-1-1$. This distance is the critical radius. It takes $E=mc^2$ much energy to bring these two particles from infinity to within the critical radius and become stable molecule. . This is such a key element of Weber&#39;s force law that we must make it absolutely clear. . Integrating Newton-Weber with Python&#39;s solve ivp: . Initially we were studying implicit ODEs and this required using a DAE solver like GEKKO. However we have successfully isolated all the relevant acceleration terms and have made the Newton-Weber explicit. This means we can either integrate the equations through solve_ivp or we can more directly integrate the equations using kick-drift-kick approximations as we learned from here. . We do both. . # import the packages from scipy.integrate import solve_ivp import numpy as np import matplotlib.pyplot as plt # speed of light constant, set to unity for convenience c=1.0 # norm of vector x .... scalar # softened norm with epsilon factor def norm(x, epsilon=0.0): val = np.dot(x,x)**(0.5) return (val**2+epsilon**2)**(0.5) ## epsilon softened parameter. This definition is discontinuous? # relative distance r01 between x0, x1 .... scalar def rho(x0,x1): r=np.subtract(x0,x1) return norm(r) # returns ((N,3)) array of vectors. # Represents Weber force with only position and velocity terms def F(state, epsilon=0.0): #c=1.0 x, k = state N=len(x) aux = np.zeros((N,3)) aux = np.array(aux, dtype=float) for i in range(N): for j in range(N): if i==j: pass else: r=np.subtract(x[j][0:3], x[i][0:3]) # r_ij, vector. rhat=np.dot(1/norm(r), r) # rhat, unit vector. Coulomb_factor = (-1)*k[i][-1]*k[j][-1]*(norm(r, epsilon)**-2)*(c**-2) # epsilon hard ball Coulomb factor. dv = np.subtract(x[j][3:6], x[i][3:6]) # dv= v_{ij}, vector. r_prime=np.dot(rhat, dv) # r&#39; = rhat cdot dv, scalar. s0 = c**2-(3/2)*(r_prime**2)+(norm(dv))**2 # scalar aux[i] = aux[i]+np.dot(Coulomb_factor*s0, rhat) # vector return aux.reshape(-1) # flatten? # auxiliary function, returns ((N,3)) array of vectors # depending on positions, velocity, and accelerations of the system def P(state,a): x, k = state N=len(x) out = np.zeros((N,3)) out = np.array(out, dtype=object) for i in range(N): for j in range(N): if i==j: pass else: r=np.subtract(x[j][0:3], x[i][0:3]) # r_ij, vector. rhat=np.dot(1/norm(r), r) # rhat, unit vector. Coulomb_factor = (+1)*k[i][-1]*k[j][-1]*(norm(r)**-1)*(c**-2) # scalar, not exactly Coulomb rhat_rescaled=np.dot(Coulomb_factor, rhat) out[i] = out[i]+np.dot(np.dot(a[i], rhat), rhat_rescaled) return out # Q(state, a)[i] returns the component of acceleration due to the environment particles on m_i a_i. def Q(state,a): x, k = state N=len(x) out = np.zeros((N,3)) out = np.array(out, dtype=object) for i in range(N): for j in range(N): if i==j: pass else: r=np.subtract(x[j][0:3], x[i][0:3]) # r_ij, vector. rhat=np.dot(1/norm(r), r) # rhat, unit vector. Coulomb_factor = (+1)*k[i][-1]*k[j][-1]*(norm(r)**-1)*(c**-2) # degree -1 Coulomb factor. rhat_rescaled=np.dot(Coulomb_factor, rhat) out[i] = out[i]+np.dot(np.dot(a[j], rhat), rhat_rescaled) return out # G(state, a) evaluates the acceleration coefficients in Newton-Weber equation. def G(state, a): x,k = state N = len(x) M = [np.dot(k[i][0], a[i]) for i in range(N)] return list(((M - P(state, a) + Q(state, a)).reshape(-1))) # G_m(state) returns matrix of coefficients, obtained by evaluating G(state, b) over a basis of 3N vectors in R^(3N) def G_m(state): x,k = state N = len(x) aux = [] for i in range(3*N): arg = np.identity(3*N)[i].reshape(N,3) row = G(state, arg) aux.append(row) return aux # G_inv(state) returns the matrix inverse of G_m(state) def G_inv(state): return np.linalg.inv(G_m(state)) # Key function: allows us to represent the accelerations terms explicitly in terms of the state input. def H(state, epsilon=0.0): x,k = state N=len(x) return np.dot( G_inv(state), F(state, epsilon)).reshape((N, 3)) # left or right multiplication?? It makes a difference! # Describes state in centre-of-mass reference frame. def cm_frame(state): x, k = state N=len(x) # N-body system N1=len(x[0]) # number of coordinates, expect N1 == 9. output = np.zeros((N,N1)) output = np.array(output, dtype=float) M=0 for i in range(N): M = M + k[i][0] for i in range(N): for j in range(N): if i==j: pass else: dr = np.subtract(x[i], x[j]) output[i] = output[i] + np.dot((1/M)*k[j][0], dr) return output, k # Weber Potential Energy. def U(x): x1,x2 = x dx = np.subtract(x2,x1) rhat=np.dot(1/norm(dx[0:3]), dx[0:3]) # unit vector Weber_term = (np.dot(rhat, dx[3:6]) / c)**2 rho = norm(dx[0:3]) return (rho**-1)*(1-Weber_term/2) # Relative kinetic energy; pecialized to two body states! def T(x, k): factor1 = k[0][0]*k[1][0]/(k[0][0]+k[1][0]) x1,x2 = x dx = np.subtract(x2,x1) rhat=np.dot(1/norm(dx[0:3]), dx[0:3]) factor2 = np.dot(rhat, dx[3:6])**2 return factor1*factor2/2 # Transition matrix describing the explicit ODE. def Phi(t, state_flat, arg_input): k, epsilon = arg_input N = int(len(state_flat)//6) ## we know this is going to be integral. state_arg = [state_flat.reshape((N,6)), k] aux = [] for j in range(N): aux = np.append(aux, state_flat[3+6*j:6+6*j]); aux = np.append(aux, H(state_arg, epsilon)[j]); return np.dot(1.e-0, aux) . . # relative errors and absolute errors. relerr = 1.e-10 abserr = 1.e-10 # input state. s0 = [[[1.1,0,0, 0,0.05,0], [0,0,0, 0,0,0]], [[1.0, -1.0], [1.0, -1.0]]]; s0 = cm_frame(s0); print(&quot;The initial state is: n&quot;, s0) # time points. t_points = np.linspace(0, 30, 60) # Run ODE solver. sol = solve_ivp(Phi, t_span = [0, 30], y0 = s0[0].reshape(-1), t_eval = t_points, dense_output = False, rtol=relerr, atol=abserr, args = [[s0[1], 0.001]]) N= len(sol.y[0]) print(&quot;The initial total energy U+T is equal to: &quot;, U(s0[0])+ T(s0[0], s0[1])) # The following for loop checks the total energy U+T along the orbit. aux = [] for j in range(N): x0 = [[sol.y[i][j] for i in range(6)], [sol.y[i+6][j] for i in range(6)] ] aux = np.append(aux,[[U(x0) + T(x0, s0[1])]] ) print(&quot;The total energy U+T along the solution, :&quot;, aux) N1=len(sol.y[0]) # Plotting the relative distance r=r(t) of an isolated two-body system. r_list = [norm([sol.y[6][j]-sol.y[0][j], sol.y[7][j]-sol.y[1][j], sol.y[8][j]-sol.y[2][j]]) for j in range(N1)] #print(&quot;r :&quot;, r_list) plt.plot(t_points[0:N1], r_list, &#39;r-&#39;, label=&#39;rho(t)&#39;) plt.plot(t_points[0:N1], sol.y[3], &#39;b:&#39;, label=&#39;velocity v0(t)&#39;) plt.ylabel(&#39;relative distance&#39;) plt.xlabel(&#39;time&#39;) plt.legend(loc=&#39;best&#39;) plt.show() . . The initial state is: (array([[ 0.55 , 0. , 0. , 0. , 0.025, 0. ], [-0.55 , 0. , 0. , 0. , -0.025, 0. ]]), [[1.0, -1.0], [1.0, -1.0]]) The initial total energy U+T is equal to: 0.9090909090909091 The total energy U+T along the solution, : [0.90909091 0.90869251 0.90055835 0.90519135 0.9088526 0.90907976 0.90844495 0.88047108 0.90696577 0.90895838 0.90904684 0.9080492 0.06693858 0.90782901 0.90902694 0.90898846 0.90736876 0.84741031 0.90831125 0.90906762 0.90889715 0.90605585 0.89642199 0.9086049 0.90908534 0.90875934 0.90299073 0.90394297 0.90879353 0.90908225 0.90855047 0.89269012 0.90642974 0.90891793 0.90905815 0.90822328 0.79566304 0.90755034 0.90899964 0.90901043 0.90767876 0.69127176 0.90814804 0.90905036 0.90893313 0.90668069 0.88852305 0.9085016 0.90907662 0.90881562 0.90454536 0.90205765 0.9087248 0.90908156 0.90863853 0.89852811 0.90570423 0.90887101 0.9090659 0.9083656 ] . # We make 3D plot of the position of particles x0(t) and x1(t) # also include 3D plot of the velocities of v0(t) of the particle x0. fig = plt.figure(figsize=(10, 10)) ax = fig.gca(projection=&#39;3d&#39;) plt.plot(sol.y[0], sol.y[1], sol.y[2], &quot;g&quot;, label=&#39;position of particle x0(t) with -1 charge&#39;) plt.plot(sol.y[6], sol.y[7], sol.y[8], &quot;b-&quot;, label=&#39;position of particle x1(t) with -1 charge&#39;) #plt.quiver(sol.y[0], sol.y[1], sol.y[2], sol.y[3], sol.y[4], sol.y[5], color = &#39;r&#39;, label=&#39;x0(t) with -1 charge&#39;) #plt.quiver(sol.y[6], sol.y[7], sol.y[8], sol.y[9], sol.y[10], sol.y[11], color = &#39;g&#39;, label=&#39;x1(t) with -1 charge&#39;) plt.legend(loc=&#39;best&#39;) plt.show() fig = plt.figure(figsize=(10, 10)) ax = fig.gca(projection=&#39;3d&#39;) plt.plot(sol.y[3], sol.y[4], sol.y[5], &quot;g:&quot;, label=&#39;velocity of particle x0(t) with -1 charge&#39;) plt.plot(sol.y[6], sol.y[7], sol.y[8], &quot;b:&quot;, label=&#39;velocity of particle x1(t) with -1 charge&#39;) #plt.quiver(sol.y[0], sol.y[1], sol.y[2], sol.y[3], sol.y[4], sol.y[5], color = &#39;r&#39;, label=&#39;x0(t) with -1 charge&#39;) #plt.quiver(sol.y[6], sol.y[7], sol.y[8], sol.y[9], sol.y[10], sol.y[11], color = &#39;g&#39;, label=&#39;x1(t) with -1 charge&#39;) plt.legend(loc=&#39;best&#39;) plt.show() . . Integrating Newton-Weber Following NMoscz&#39; Vectorized Kick-Drift-Kick Pattern: . Reference : Moscz&#39; github repository for nbody-python, which pattern we basically adapted. . # initial input state vector. s0 = [[[1.1,0.0,0, 0,0.05,0], [0,0,0, 0,0,0]], [[1.0, -1.0], [1.0, -1.0]]]; s0 = cm_frame(s0); # Epsilon is &quot;softening parameter&quot; epsilon=.1 # End time. tEnd = 40.0 # Time step dt = 0.001 # Number of time points. Nt = int(np.ceil(tEnd/dt)) # All discrete time points in the interval of integration. t_all = np.arange(Nt+1) # Initialize time at t=0. t=0 # Initializing the saved state vector. state_save = np.zeros((2, 6, Nt+1)) # Setting initial state argument. state_arg = s0[0]; state_save[:, :, 0] = state_arg # Run the main for loop, implement kick-drift-kick approximation. for i in range(Nt): acc = H([state_arg, s0[1]], epsilon) # half kick, velocity update. state_arg[:, 3:6] += acc * dt/2.0 # drift, position update. state_arg[:,0:3] += state_arg[:, 3:6] * dt #update acceleration, re-evaluate according to updated state_arg. acc = H([state_arg, s0[1]], epsilon) # half kick, velocity update. state_arg[:, 3:6] += acc * dt/2.0 #update time t+=dt state_save[:,:,i+1] = state_arg # Relabel the saved state. y = state_save; . . # prep figure fig = plt.figure(figsize=(10, 10)) ax = fig.gca(projection=&#39;3d&#39;) xs = y[0, 0, :] ys = y[0, 1, :] zs = y[0, 2, :] plt.plot(xs, ys, zs, &quot;g&quot;, label=&#39;position of particle x0(t) with -1 charge&#39;) plt.legend(loc=&#39;best&#39;) plt.show() fig = plt.figure(figsize=(10, 10)) ax = fig.gca(projection=&#39;3d&#39;) xs = y[0, 3, :] ys = y[0, 4, :] zs = y[0, 5, :] plt.plot(xs, ys, zs, &quot;g:&quot;, label=&#39;velocity of particle x0(t) with -1 charge&#39;) plt.legend(loc=&#39;best&#39;) plt.show() . . Conclusion: . The above images are pretty good. In fact we&#39;re very pleased with the vectorized implementation since the solution is definitely obtained quickly, and we find the 3D images are quite satisfactory. Having successfully verified the critical radius and the stable molecule consisting of identical negative electric charges $(-1) + (-1)$, we now proceed with the three-body problem! . -JHM. .",
            "url": "https://jhmartel.github.io/fp/weber/newton/two%20body/critical%20radius/2022/11/02/WeberTwoBody_November_Corrected.html",
            "relUrl": "/weber/newton/two%20body/critical%20radius/2022/11/02/WeberTwoBody_November_Corrected.html",
            "date": " • Nov 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Book Review. The Newton Papers by Susan Dry",
            "content": ". Here we write a brief review of the book &quot;The Newton Papers: The Strange and True Odyssey of Isaac Newton&#39;s Manuscripts&quot; by Sarah Dry. . . The story begins with John Maynard Keynes frantically trying to attend a Sotheby&#39;s auction to purchase some of the 300 lots of manuscripts belong to Sir Isaac Newton. . The subject is personally interesting because we ourselves have alot of papers, and we understand their disordered state, but also how much work and creative energy is contained in unpolished form within these papers. When we die, what will happen but all these great ideas will remain hidden. This is why we invest so much energy into writing these posts. We are trying to correct an earlier mistake in our career which involved too much note writing, and not enough synthesis, i.e. actually collecting and editing the notes into a more readable form. Indeed alot of my own manuscripts record my own processes and conversations with myself. Or, and this is why I love writing, the manuscripts record ideas which have freshly arisen in our mind, and we write them to paper to offload them and record them for future study. . Indeed writing gives us great mental relief. Once the ideas are on the page, I don&#39;t need to really keep them in my mind. I can move past them, to the next stage of development. In other words, I write my ideas down so I don&#39;t have to keep them stored in my mind. And once the ideas are outside my mind and recorded on paper, it&#39;s a great relief to my own mental strain. . Now Newton is universally recognized as a great scientific and mathematical genius. However, like most geniuses, he was a peculiar personality. And unfortunately we find the author&#39;s book on Newton rather preoccupied with Newton&#39;s alchemist and theological interests instead of his more scientific or mathematical research. The book contains very few direct quotations or references to Newton&#39;s manuscripts. The book instead makes rather indirect and opaque allusions to certain topics discussed in the manuscripts. We would have enjoyed more direct commentary on specific manuscripts, to hear Newton speak &quot;in his own words&quot; and have commentary to clarify some ideas, expressions, meanings. But the book is again focussed more on the alchemy (and this was JM Keynes&#39; interest in Newton&#39;s manuscripts) and the theology than mathematics or physics. . It&#39;s possible that Newton later in his life, when he was well established as Treasurer of the Royal Mint and personally liable for all coinage in the British kingdom, was not specially preoccupied with scientific researches. We can understand that his life&#39;s work and research in opticks, mechanics, gravitation and motion of the heavens, the calculus, etc., were perhaps less interesting to him in his later years. He surely had a tireless and energetic mind and creative capacity, and it seems natural that he would have wider interests and ambitions. . It seems that Newton&#39;s manuscripts were effectively hidden for many years. Firstly, they were in a very disordered state. Newton wrote and rewrote on many different scraps of paper, folded in different ways, unordered and undated, and often times being recycled. We understand this as a mathematician, sometimes we write very nice notes, but more typically we use paper as a scratchpad for calculations, speculations, ideas, etc., all of which are sometimes very disordered. . There is a second reason for the manuscripts being hidden away, and that&#39;s largely Newton&#39;s extensive theological writings. Basically Newton had a strong anti-Protestant view towards the person of XP. He was a so-called &quot;Anti Trinitarian&quot; arguing that the Son was always subject to the Father, and in no sense &quot;equal with&quot; the Father. Newton also extensively wrote his opinion on corruptions in the translations and transmissions of scripture. In other words, he felt the Bible, say 1611 King James Bible, was corrupted. With Newton being one of the most eminent scientific authorities around the world, but especially on the Continent, these harsh opinions were somewhat scandalous. The trustees of Newton&#39;s manuscripts thought it necessary to suppress the public release of these sharp theological criticisms. . Regarding Newton&#39;s alchemical works, we are not much interested in this subject. We don&#39;t really believe that Newton was dealing with &quot;dark arts&quot; or &quot;magick&quot; of any sort. He was operating in 17th century with very basic chemistry, and he probably had an interest in chemistry and assays being the treasurer of the mint. So our view on his alchemical writings are that they are earnest chemistry researches in a primitive chemical period. If Newton had lived a hundred years later, our opinion is that his alchemical writings would be less speculative. Perhaps we&#39;re wrong. . Further Reading . BBC article . | Wired Q&amp;A with author Susan Dry . | Smithsonian Magazine . | . . . .",
            "url": "https://jhmartel.github.io/fp/newton/manuscripts/book%20review/2022/10/25/NewtonLostManuscriptsBookReview.html",
            "relUrl": "/newton/manuscripts/book%20review/2022/10/25/NewtonLostManuscriptsBookReview.html",
            "date": " • Oct 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Faraday Cages and Non Mathematics",
            "content": ". This has been interesting study, and I think we can clear it up. Yes, and there&#39;s a big difference between doing mathematics in your mind and doing actual physics. . Here is the setting. We imagine we have a solid conductor material, like a hollow spherical shell, where the width of the shell is actually rather thin. So the conductor is very much hollow. To say the shell is a conductor, rather than an insulator, means the shell is made of a metal or aluminum or conductor, something with free mobile electrons. By contrast a conductor would be made of glass or ceramics like porcelain. Or potentially rubber or oil. . But here we have a conductor $C$. It has a volume and it has surface, and it potentially divides space into two volumes ${ bf{R}}^3 - C = V_i coprod V_j$. Here the strange symbol $ coprod$ emphasizes that the volumes are disjoint. In fact, if the conductor $C$ really is a volume than the boundary interesections $ partial V_i$ and $ partial V_o$ are disjoint, and even somewhat partition the proper topological boundary $ partial C$ of the conductor. The reader should contrast the case of a hollow sphere conductor with a solid disk conductor. There is a difference between the two cases. . Remark: This post is somewhat controversial. The trouble is that electrostatics and electricity are not well explained nor understood by most authors. The properties of Faraday cages is one such example, as we&#39;ll describe below. . Faraday Cages . We were brought to this subject reviewing Prof. AKT Assis&#39; book with JA Hernandes &quot;The Electric Force of a Current&quot; pdf available from Prof. Assis&#39; website. The Faraday cage arises in connection with a study of an experiment proposed by R. Sansbury related to &quot;anomalous&quot; electric forces. Assis remarks that Faraday cages have surprising properties which are elementary, but not well known. . Myself I&#39;m realizing that I had a confused idea. It begins with a theorem of Poisson, which is related to the &quot;method of images&quot;. We quote from Weber, writing on this theorem of Poisson (1812): . &quot;When arbitrary electric forces act upon a conductor of arbitrary form from the outside, a distribution of free electricity on the surface of the conductor is always possible -- but only one of them -- for which the electric forces that originate from that distribution of free electricity will likewise be in equilibrium with the electric forces at all points of the interior of the conductor that act from the outside&quot; . In otherwords if we have a conductor $C$, and a net external force $F_{ext}$ acting on $C$, then there exists a unique surface charge distribution $ sigma$ on the boundary $ partial C$ such that $$F_{ext}(x)+F_ sigma(x) =0~~~ text{for every}~~x in C.$$ This theorem represents the observation that a surface charge of free electrons accumulates such that the net force $F_{net}=F_{ext}+F_ sigma$ vanishes everywhere inside the conductor $C$. . EDIT: The equation $F_{ext}+F_ sigma = 0$ is interesting, and we&#39;ve been reviewing Hammond&#39;s textbook &quot;Energy Methods in Electrostatics&quot;, and especially Ch.4 on Energy Theorems. He makes emphasis on George Green&#39;s essays on mathematical analysis of electricity papers, which actually are very readable and very well written. We especially like how Green tackles the question of evaluating divergence theorems when the differentials involved in the potentials diverge to infinity. His attention to singular values is very much interesting! But somehow the zero on the right hand side of the above force equation is represented in Green&#39;s first formula, which is divergence theorem and a product formula: $$ int_V phi . Delta phi~ d text{vol}+ int_} phi . sigma~ dS = int_V nabla phi cdot nabla phi ~d text{vol}.$$ So this is an identity which is valid for all volumes $V$ with boundary. In otherwords, it&#39;s totally general. Hammond calls the right hand term the &quot;field energy&quot; of the system. He emphasizes that the inclusion of the surface charge $ sigma = epsilon_0 frac{ partial phi}{ partial n}$ in the left-hand side integral is necessary for the total energy of the system to be uniquely defined. This total energy is represented by the right hand integral involving $| nabla phi|^2 = nabla phi cdot nabla phi$. . The quantity $ phi. nabla phi$ is called the volume energy density by Hammond. The above Green formula implies it has units of energy density (hence integrating we obtain an energy quantity). It represents $ phi. rho$. But admittedly it&#39;s not clear what the value represents. We prefer to interpret $ phi.~ Delta phi$ as the assembly work required to build the system. I.e. work was required at some point to build the point charges which define the potential $ phi$, and this apparently is represented in $ int_V phi rho$. So there is energy inherent in the system, as represented by this quantity. Apparently William Thompson Lord Kelvin wrote this in his diary in 8 April 1845. . | Thompson made the further observation that the Green identities represent an equilibrium equation, or ground state. This is treated in Hammond&#39;s book. . | . . Useful video on Faraday Cage and EMP Misconceptions. . &quot;How does this relate to Faraday cages?&quot; . In a sense it doesn&#39;t! And i&#39;m guilty myself of making some mistakes in thinking about the question. So let&#39;s be ultra clear. The issue is that the conductor $C$ is a solid material, and the interior of $C$ is hidden from view. For example it is not possible to really place test charges inside the conductor! This would require some material permittivity in the conductor which is not part of our hypotheses, and we are assuming the conductor is a continuum material. . No, instead the practical application of Faraday cages begin with a hollow spherical shell which is constructed from a conductive material. The spherical shell essentially only has a very thin interior, and we will not be placing test charges inside the spherical shell. Rather the spherical shell divides the exterior of the shell into two volumes $V{i}$ and $V{o}$, where the indexes denote &quot;inner&quot; and &quot;outer&quot; volumes. But these volumes are not made of conductive material, in fact they are assumed to be empty spaces which will eventually contain test charges. . So what does Poisson&#39;s theorem say about this conductive shell? Precisely that there exists a surface distribution $ sigma$ which equilibrates the external electrical forces $F_{ext}$. However, and this is the key point, the external electric force $F_{ext}$ is itself a sum of $F_{ext, i}$ and $F_{ext, o}$ depending on whether the test point is in $V_i$ or $V_o$. . The confusion arises between the meaning of &quot;interior of the conductor&quot; and &quot;interior volumes which are bound by the conductor&quot;. As we described above, the interior of a thin spherical shell is a very thin annular type region. But the interior volume bounded by the spherical shell is the rather large interior disk bounded by the shell. This leads to a confusion. . So again: . _The inner volume $V_i$ containing the test points is not(!) the interior of a conductor in the sense of Poisson&#39;s theorem._ . Therefore the surface charge is definitely interacting with the test charges, indeed only the interior of the conductor $C$, which again is not materially accessible. . . So what are we saying, that Faraday cages don&#39;t work? . No, but we are saying that there is an electric field present in the interior volume of a Faraday cage. I.e. $ nabla phi neq 0$ in the interior volume $V_i$. And this is directly contrary to the expectations of Faraday, Feynman, etc., etc.. . But what about experiments? . E.g. Benjamin Franklin 1750s? Or Faraday&#39;s own personal cage? . . . . But what about the experiments, why do they apparently work? Why are we protected from outside lightning when we are sitting in our car? . The above screenshots are from &quot;Prelude to Power&quot; an amusing dramatization of Faraday&#39;s discoveries. . | Simple Faraday Cage Reference . | Katie Loves Physics Explaining Some History of Faraday Cage. Contains useful historical references. . | Physics Forum Question with Very Unsatisfactory Answers. See the erroneous answer invoking Gauss&#39; law. There is no way to deduce a vanishing electric field from a locally vanishing charge density. . | . For example, they say that Faraday built a cubical room lined with metal sheets, like surrounding the walls of your house with aluminum foil, and exposed the room to electrical discharges from the outside volume. Faraday claims to have monitored the interior volume of the cube with an electrometer and detected no electric forces either at the boundary walls or anywhere in the interior of the room. This claim might appear to contradict our arguments above, but let&#39;s reserve our judgement and examine the situation more closely. . Mathematics of Faraday Cage? . Apparently the authors of [this paper] were shocked to find no readily available references or mathematical explanations of Faraday&#39;s cage. Consider the misleading and erroneous answers supplied in the above Physics Form link. . A sketch of the mathematics behind the cage is roughly: we are looking for harmonic functions $ phi$ on the conductor and its surroundings. The assumption is that $ phi$ is constant throughout the conductor and therefore $ nabla phi=0$. And the gradient vanishing says there is no electric force. . So we can phrase everything in terms of harmonic functions. We are given a conductor $C$ and the space complement ${ bf{R}}^3-C$. So we are looking then for harmonic functions $ phi$ defined on the global space ${ bf{R}}^3$ satisfying the condition that $ nabla phi|_C=0$. If the conductor is nontrivial volume, then doesn&#39;t the vanishing of $ nabla phi$ imply $ phi$ is constant, and therefore vanishing everywhere? . Prof. Lewin&#39;s experiment does not seem convincing. He has a conductive ping pong. Are we expected to believe that if his pingpong touches the positively charged surface, then this positive charge will be imparted to the pingpong rod, and eventually ? This is only if the positive charges are mobile and attracted to the rod. . He touches one side of the box and it&#39;s negatively charged. He touches the other side of the box, and he sees that it&#39;s positively charged. (He&#39;s collecting these charges using the condutive ping pong, then discharging at an electrometer to see the polarity, positive or negative charge). But then Prof. Lewin places the pingpong in the volume bounded by the conductor can, and he moves the ping pong around and seems to indicate that he collects no free electric charges. The absence of charges is shown by the electrometer registering no charge. But what does this show? Not that there is zero net force in the interior, but rather that there are no free electrons! . Is this fair critique? There are no mobile positive ions which are collected by the pingpong and then deposited. Rather there&#39;s only mobile electrons. On the negative side, there is an excess of electrons which are possibly attracted to the pingpong. On the positive side there is an absence of electrons. When the pingpong conductor is placed in physical contact with the box, there is a flow of electrons from the pingpong conductor to the box. .",
            "url": "https://jhmartel.github.io/fp/faraday/electricity/faraday%20cage/poisson/harmonic%20functions/2022/10/24/FaradayCages.html",
            "relUrl": "/faraday/electricity/faraday%20cage/poisson/harmonic%20functions/2022/10/24/FaradayCages.html",
            "date": " • Oct 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Andre-Marie Ampere and Magnetism the Disposable Hypothesis",
            "content": ". History of Electricity . Newton (1660s) | Coulomb (1785) | Oersted (1819) | Ampere (1820--1826) | Biot-Savart | Faraday (1831), (1851) | Weber (1840s+) | Maxwell (1860s) | . Since working on our PhD thesis (completed November 2019) we have been thinking constantly about history of electrodynamics. But the 19th century discoveries and experiments in electricity remain extremely interesting today (2022). A very rough sketch of the history is something like this: in 1660s Newton presents his law of gravitation. Eventually Coulomb presents a similar law for electric charges, namely the Coulomb force law (1785). Later Oersted (1819) demonstrated that a constant current in a circuit deflects a compass. This experiment was apparently quite shocking to philosophers of the period. Andre-Marie Ampere was so amazed that he began six intensive years of experiment and investigation, eventually publishing his memoir [insert title]. Now Faraday (1831) demonstrated by experiment effects that he called Volta induction. This induction is frequently called &quot;Faraday induction&quot; in contemporary texts. Eventually Faraday (1851) attempted to explain these effects with his field concept. Faraday used many different expressions for the electric and magnetic fluids, for example, but eventually hypothesized the existence of magnetic field lines. Ampere was always opposed to these hypothetical lines, viewing them as ill defined and disposable, i.e. not necessary for any physical explanations. . Later Maxwell (1860s) more formally developed Faraday&#39;s field concept into his equations relating $E$ and $B$, namely the so-called electric and magnetic fields. Maxwell also introduced another modification, namely his so-called &quot;displacement current&quot; (Interesting derivation here). But we do not address this further here. Needless to say, it&#39;s a strange re-definition of electric current. . Faraday Volta Induction . Eventually we should explain how Wilhelm Weber&#39;s force law successfully unifies the laws of Coulomb, Ampere, and Faraday induction. But this is long story. So we begin with the simplest parts. . The basic example of induction is Faraday&#39;s experiment, as depicted by Prof. Lewin in the following screenshot. Now notice that the magnetic field is materialized by the circuit $C_1$. It is assumed that a magnetic field $B$ is generated by the circuit $C_1$ when the switch in the circuit is closed. Moreover it&#39;s assumed that this switch in the circuit represents a variable magnetic field $ frac{ partial B}{ partial t}$. But again, the magnetic field $B$ is not physical, but rather the circuit $C_1$ is taken implicitly as a proxy representative for the field $B$. . . Likewise when Prof. Lewin illustrates how a magnetic field in motion induces a current in a conductor, a bar magnet is necessary as materialization of the otherwise abstract field $B$. We are emphasizing that the magnetic field $B$ is very abstract and immaterial. Yet in all the motivating experiments, the field $B$ is represented by some material object. In the previous set-up this materialization was given by circuit $C_1$. In this example, the materialization is given by the material bar magnet. . . Mysteries in the Magnets . Now there remains somewhat a mystery hidden in magnets. Faraday in (1851) introduced the idea of the magnetic field, and this was more formally adopted by Maxwell in his equations (1860s). One of the very important equations is the hypothesis of non-existence of magnetic monopoles. This is the important $$div(B) = nabla cdot B = 0$$ law in Maxwell&#39;s equations. . However Ampere much earlier in fact discovered the true reason why magnetic monopoles do not exist. Now we cannot go into detail about Assis&#39; proof of the nonexistence of monopoles from Ampere&#39;s circuital force law. Indeed when the magnetic field is replaced with the electricthe force of a secondary electric circuit, then $$B= nabla times( oint_C bf{F} cdot ds)$$ for a force vector $ bf{F}$ introduced by Ampere [insert reference]. But the trivial algebraic identity $$div(curl)= nabla cdot ( nabla ~ times - )=0, $$ and this trivial identity implies the nonexistence of monopoles. This amazing proof is not well-known even in 2022! . Ampere understood that basic North-South magnetic dipoles can be replaced by small current loops. But here is the mystery of magnetism: are magnetic materials filled with small current loops? If these loops have resistance, then this current must eventually dissipate all its energy into heat and die off. . Is it possible for atoms to carry zero-resistance currents? . This is somewhat the mystery left by Ampere. I.e., magnetism remains ultimately unexplained by Ampere, or at least reduced to the above question. . The critics and Maxwellian field view does not offer any better explanation. . What is the physics of the basic magnetic North-South dipole? . They cannot say. . In the conceptual history, we need to appreciate that Oersted&#39;s experiment (1819) was a catalyst for investigations into electricity. However the idea of magnetic field was not immediately adopted, especially not by Ampere. In the book [ref] we find the controversies between Ampere and most other scientists about the nature of magnetism and electricity, and the proper interpretatinos of Oersted&#39;s experiment. This is somewhat difficult in contemporary research (2022) because the Maxwellian field concepts are deeply implicit in the physicist&#39;s mind. But they are not essential, and as Ampere describes, they are disposable hypotheses. .",
            "url": "https://jhmartel.github.io/fp/ampere/magnetism/currents/magnetic%20monopoles/2022/10/24/AmpereMagnetismErrors.html",
            "relUrl": "/ampere/magnetism/currents/magnetic%20monopoles/2022/10/24/AmpereMagnetismErrors.html",
            "date": " • Oct 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Weber Force Laws and Two-Body Problem. Again. [More Construction]",
            "content": ". Another post on Weber&#39;s force law, and not the last. . We return again to Weber force laws and two body problems, with special attention given to the possible attractive forces between identical negatively charged particles. This is one of the key predictions of Weber&#39;s force law, as addressed previously on this blog. The point is that a large amount of energy and work is required to push the particles through the critical radius r_c where suddenly the sign flips in the force term, or equivalently the object inherits a negative inertial mass. And this is the true meaning of E=mc^2 as we&#39;ve explained. . But to progress this program forward we need to have ready integrated solutions to the equations of state. We also need to know that these integrated solutions are reasonably accurate. Practically we test their accuracy against the conservation of energy. Accurate solutions should maintain a constant total energy, namely $T+U$. Also one has conservation of angular momentum. That Weber&#39;s force law satisfies these conservation properties is proven by Weber himself repeatedly, and by Neumann, and again by AKT Assis in his recent publications. . We&#39;re returning to Weber two-body problem. Our earlier calculations were somewhat incorrect, since we did not account for the centre-of-mass frame properly. This has been corrected below. . It&#39;s curious somewhat that Weber&#39;s potential and force has caused much confusion and errors in physicists and mathematicians alike. It takes some care. Let&#39;s recall the basic derivation of the Weberian mass to borrow a phrase from Prof AKT Assis, especially in the following lecture slide. . . Deriving the Critical Radius . We follow Assis&#39; approach, as described in the above video. The idea is specific to the two-body system, where we have two particles $r_1$ and $r_2$ as an isolated system. The idea is to collect the $a_1$ terms, leaving the $a_2$ term within the force term. Thus Newton&#39;s 2nd Law applied to particle $r_1$ has the following form: . $$ frac{q_1 q_2}} ~ { hat{ bf{r}}}_{12} ~ left(1- frac{r&#39;^2_{12}}{2c^2} - frac{r_{12} a_2}{c^2} right) = (m_1 - frac{q_1 q_2}{r_{12} c^2}) ~a_1 .$$The left hand term is denoted $q_1 E$ in Assis&#39; slide. . The critical radius $r_c$ is derived as the singular point, where we get a sign change, i.e. where $$r_c = frac{q_1 q_2}{m_1 c^2}.$$ . This formulation of the two-body problem is better suited to numerical integration. . Centre-Of-Mass Coordinates . We will be evaluating Weber&#39;s force law using Newton&#39;s 2nd Law in this centre-of-mass frame. This is linear algebra, but we made a mistake in our previous posts, so we here record the idea more formally. . We have $$R= frac{1}{M} sum m_j r_j$$ being the centre of mass in the initial Cartesian coordinate system, and also $$ frac{dR}{dt} = v_{CM} = frac{1}{M} sum m_j v_j.$$ The key observation is that the net force on $R$ is zero, i.e. $${ bf{F}} = M a_{cm} = 0.$$ [Verify this last statement] . Therefore $R(t) = v_{cm}.t$, and the idea of centre-of-mass coordinates is to &quot;subtract&quot; $v_cm$ from the reference frame. Thus we have a change of coordinates $$r&#39;_i := r_i - R,$$ where $r_1&#39;, r_2&#39;, etc.,$ are going to be the position coordinates in the new centre-of-mass reference frame. The main observation is the trivial rearrangement $$r_i&#39; = frac{1}{M} sum_{j} m_j r_{ij}.$$ This follows from the trivial $M-m_i = sum_{j, ~j neq i} m_j$. . Solving with GEKKO: . We would prefer to be integrating ODEs with odeint or more directly. However the form of Weber&#39;s force law implies that acceleration terms arise in the force, and we cannot use the usual tricks to isolate for each acceleration term. . Also since the computation of the Weber force requires relative accelerations between the particles, it has been necessary to record the accelerations in the state vectors. This is also somewhat inconvenient. . For example, say we being with initial state $s=[x,v,a]$. Newton&#39;s 2nd Law implies that the acceleration $a$ of the particle is equal to $1/m$ times the net force acting on the particle, i.e. total electric and non-electric forces. But we also find Weber&#39;s force $F$ depends on the relative accelerations (and distance and velocity) of the particles in the system. Therefore if only electrical forces are acting on the system, then what constraint does the acceleration term satisfy? The issue is that there&#39;s somewhat a circular definition at play which we need to carefully untie. Newton&#39;s law implies that the acceleration of the particle is equal to the net forces which act on the particle. However this net force itself depends on the relative acceleration of this particle with the system. So the question is: for a given time step $Δt$, how should the initial state $s$ propagate to $$s_{new} =s+Δt.(ds/dt)~~~?$$ . By contrast, if we dealing with Newton&#39;s gravitational force with state $[x,v]$ (position and velocity) and law $F=ma$, then the propagation of any initial condition is more obviously $$[x_{new},v_{new}]=[x,v]+ Delta t.[v, frac{1}{m}F].$$ This is the usual trick of setting $$dx/dt=v, ~~~dv/dt=a.$$ The above system can be directly implemented in odeint and numerically evaluated. . However in our case, with Weber&#39;s force law this is not immediately possible. Therefore we have resorted to GEKKO and its differential algebraic equation solver. But we do not like using other methods which offer limited visibility, because we cannot really be certain that the computation is correct! . !pip install gekko . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting gekko Downloading gekko-1.0.5-py3-none-any.whl (12.2 MB) |████████████████████████████████| 12.2 MB 5.9 MB/s Requirement already satisfied: numpy&gt;=1.8 in /usr/local/lib/python3.7/dist-packages (from gekko) (1.21.6) Installing collected packages: gekko Successfully installed gekko-1.0.5 . import numpy as np from gekko import GEKKO import matplotlib.pyplot as plt c=1.0 def rho(x): return np.dot(x,x)**(0.5) # Component of Weber force which contains only position and velocities. def f(state): c=1.0 x, k = state N=len(x) aux = np.zeros((N,3)) aux = np.array(aux, dtype=object) for i in range(N): for j in range(N): if i==j: pass else: r=np.subtract(x[i][0:3], x[j][0:3]) # r_ij, vector. rhat=np.dot(1/rho(r), r) # rhat, unit vector. Coulomb_factor = (+1)*k[i][-1]*k[j][-1]*(rho(r)**-2) # Coulomb factor, only term involving q values. dv = np.subtract(x[i][3:6], x[j][3:6]) # dv= v_{ij}, vector. r_prime=np.dot(rhat, dv) # r&#39; = rhat cdot dv, scalar. s0 = (3/2)*((r_prime/c)**2)+(rho(dv)/c)**2 effective_Weber_factor = 1-s0 aux[i] = aux[i]+np.dot(Coulomb_factor*effective_Weber_factor, rhat) return aux def cm_frame(state): x, k = state N=len(x) # N-body system N1=len(x[0]) # number of coordinates, expect N1 == 9. output = np.zeros((N,N1)) output = np.array(output, dtype=object) M=0 for i in range(N): M = M + k[i][0] for i in range(N): for j in range(N): if i==j: pass else: dr = np.subtract(x[i], x[j]) output[i] = output[i] + np.dot((1/M)*k[j][0], dr) return output, k . import numpy as np from gekko import GEKKO import matplotlib.pyplot as plt c=1.0 # Weber two-body force. def w2b(state): c=1.0 x, k = state N=len(x) aux = np.zeros((N,3)) aux = np.array(aux, dtype=object) for i in range(N): for j in range(N): if i==j: pass else: r=np.subtract(x[i][0:3], x[j][0:3]) # r_ij, vector. rho=np.dot(r,r)**(0.5) # rho = |r_ij|, scalar. rhat=np.dot(1/rho, r) # rhat, unit vector. factor_Coulomb = (+1)*k[i][-1]*k[j][-1]*(rho**-2) # Coulomb factor, only term involving q values. dv = np.subtract(x[i][3:6], x[j][3:6]) # dv= v_{ij}, vector. r_prime=np.dot(rhat, dv) # r&#39; = rhat cdot dv, scalar. #da = np.subtract(x[i][6:9], x[j][6:9]) #da = a_{ij}, vector s0 = 1-(1/2)*(r_prime/c)**2 s1 = np.dot(r, x[j][6:9])*(c**-2) s2 = k[i][0] - (k[i][1]*k[j][1])/(rho*c*c) ## this is the interesting term involving the critical radius. factor_Weber = (s0 - s1)/s2 # scalar aux[i] = aux[i]+np.dot(factor_Coulomb*factor_Weber, rhat) return aux def rho(x): return np.dot(x,x)**(0.5) # main function defining the effective Weber force. def ef(state): c=1.0 x, k = state N=len(x) aux = np.zeros((N,3)) aux = np.array(aux, dtype=object) for i in range(N): for j in range(N): if i==j: pass else: r=np.subtract(x[i][0:3], x[j][0:3]) # r_ij, vector. rhat=np.dot(1/rho(r), r) # rhat, unit vector. Coulomb_factor = (+1)*k[i][-1]*k[j][-1]*(rho(r)**-2) # Coulomb factor, only term involving q values. dv = np.subtract(x[i][3:6], x[j][3:6]) # dv= v_{ij}, vector. r_prime=np.dot(rhat, dv) # r&#39; = rhat cdot dv, scalar. s0 = (3/2)*((r_prime/c)**2)+(rho(dv)/c)**2 effective_Weber_factor = 1-s0 aux[i] = aux[i]+np.dot(Coulomb_factor*effective_Weber_factor, rhat) return aux # main function defining the Weber force. def f(state): c=1.0 x, k = state N=len(x) aux = np.zeros((N,3)) aux = np.array(aux, dtype=object) for i in range(N): for j in range(N): if i==j: pass else: r=np.subtract(x[i][0:3], x[j][0:3]) # r_ij, vector. rho=np.dot(r,r)**(0.5) # rho = |r_ij|, scalar. rhat=np.dot(1/rho, r) # rhat, unit vector. factor_Coulomb = (+1)*k[i][-1]*k[j][-1]*(rho**-2) # Coulomb factor, only term involving q values. dv = np.subtract(x[i][3:6], x[j][3:6]) # dv= v_{ij}, vector. r_prime=np.dot(rhat, dv) # r&#39; = rhat cdot dv, scalar. da = np.subtract(x[i][6:9], x[j][6:9]) #da = a_{ij}, vector s0 = (1-(0.5)*(r_prime/c)**2) s1 = (np.dot(dv, dv)- r_prime**2)*(c**-2) # scalar s2 = np.dot(r, da)*(c**-2) # scalar factor_Weber = s0 + s1 + s2 # scalar aux[i] = aux[i]+np.dot(factor_Coulomb*factor_Weber, rhat) return aux # returns effective Weber mass. Scalar valued function. def wm(state): x,k=state N=len(x) for i in range(N): k[i][0] - k[i]x[i] def enw(state): ## here going to define the Effective Newton Weber Force x,k=state weber_force = f(state) N=len(x) return def cm_frame(state): x, k = state N=len(x) # N-body system N1=len(x[0]) # number of coordinates, expect N1 == 9. output = np.zeros((N,N1)) output = np.array(output, dtype=object) M=0 for i in range(N): M = M + k[i][0] for i in range(N): for j in range(N): if i==j: pass else: dr = np.subtract(x[i], x[j]) output[i] = output[i] + np.dot((1/M)*k[j][0], dr) return output, k def gs(initial_state): s = cm_frame(initial_state) N=len(s[0]) m = GEKKO(remote=True) m.time = np.linspace(0,5, 60) # time points x = m.Array(m.Var,(N,9)) # initializing spatial values for i in range(2): for j in range(9): x[i][j].value = s[0][i][j] # setting constants. c = np.zeros((2,2)) c = np.array(c, dtype=object) for i in range(2): for j in range(2): c[i][0] = m.Const(s[1][i][0]) c[i][1] = m.Const(s[1][i][1]) # is eq0 correct? we see large accelerations, but the velocity is not increasing fast enough, so something seems wrong. # eq0 = [ (c[i][0])*x[i][j+3].dt() == f([x, c])[i][j] for i in range(N) for j in range(3)] eqA = [ x[i][j+3].dt() == w2b([x,c])[i][j] for i in range(N) for j in range(3)] eqB = [ x[i][j].dt() == x[i][j+3] for i in range(N) for j in range(3) ] #eqC = [ x[i][j].dt() == x[i][j+3] for i in range(N) for j in range(6) ] m.Equation(eqA+eqB); m.open_folder() m.options.IMODE = 6 m.solve(disp=False) return x def IW(initial_state, repeat=1): i = 0 input = initial_state while i &lt; repeat: w=gs(input) # total solution integrated over time terminal_state = [[w[i][j][-1] for j in range(9)] for i in range(2)] # this is terminal state of solution # need to build the net_output array for the total integrated solution. net_output = [] ## need initilize net_output to correct variable outside the scope of the if/else part. net_output = np.asarray(net_output, dtype=object) if i == 0: net_output = w else: net_output = np.concatenate((net_output, w)) total_output = [terminal_state] + [input[1]] # i = i+1 input = total_output return net_output def rprime(dx): r = dx[0:3] rho = np.dot(r,r)**(0.5) # rho = |r_ij|, scalar. rhat = np.dot(1/rho, r) dv = dx[3:6] return np.dot(rhat, dv) def U(dx): r = dx[0:3] rho = np.dot(r,r)**(0.5) # rho = |r_ij|, scalar. return (rho**-1)*(1 - (c**-2)*(rprime(dx)**2)) def T(dx): return rprime(dx)**2/2 ## missing a term? #print(rprime([3.0,0.2,0, -1, 0, 0, 0, 0, 0])) #print(U([3.0,0.2,0, -1, 0, 0, 0, 0, 0])) #print(T([3.0,0.2,0, -1, 0, 0, 0, 0, 0])) . File &#34;&lt;ipython-input-1-344b02daef9a&gt;&#34;, line 92 k[i][0] - k[i]x[i] ^ SyntaxError: invalid syntax . s0 = [[[0.2,0,0, -0.6,0.,0.1, 0,0,0], [0,0,0, 0,0,0.1, 0,0,0]], [[1.0, -1.0], [1.0, -1.0]] ]; a = IW(s0, 1); . #print(len(a[0][0])) #print([a[0][0][j] for j in range(len(a[0][0]))]) #print(a[0][0][:]) #print(a[0]) #print(a[0][6][:]) s0 = [[[0.01,0,0, -0.6,0.,0.1, 0,0,0], [0,0,0, 0,0,0.1, 0,0,0]], [[1.0, -1.0], [1.0, -1.0]] ]; # w2b is renormalized vector representing the &quot;effective force&quot; term in Newton&#39;s 2nd Law. # print( w2b(s0),&quot; n n&quot;, f(cm_frame(s0)) ) # Okay, the sign of the force law _is_ correct. # So we see a big difference between Weber&#39;s force, where the sign of the force is always repulsive for equal charge particles. # Yet when we isolate the acceleration term and apply Newton&#39;s 2nd Law, then we find the &quot;effective force&quot; and the &quot;effective mass&quot; # of the particle changes sign. To integrate the equations of motion, yes, we need to compute the &quot;effective force&quot;. Need verify # that our definition of w2b is correct. . [[-82.82828282828284 0.0 0.0] [82.82828282828284 0.0 0.0]] [[8200.0 0.0 0.0] [-8200.0 0.0 0.0]] . fig = plt.figure(figsize=(20, 20)) ax = fig.gca(projection=&#39;3d&#39;) plt.plot(a[0][0][:], a[0][1][:], a[0][2][:], &quot;r&quot;, label=&#39;x0(t) with -1 charge&#39;) plt.plot(a[1][0][:], a[1][1][:], a[1][2][:], &quot;g&quot;, label=&#39;x1(t) with -1 charge&#39;) #plt.title(&quot;Two body system of identical negatively charged particles interacting with respect to Weber&#39;s electrodynamical force law.&quot;) plt.legend(loc=&#39;best&#39;) # Display plt.axis(&#39;on&#39;) #ax.view_init(30, 20) #? Does the rotation on perspective also rotate the axes!? plt.show() . s1 = [[[1.2,0,0, 0,0,0, 0,0,0], [0.0,0,0, 0,0,0, 0,0,0]], [[1.0, -1.0], [1.0, -1.0]] ]; a = IW(s1, 1); ## oscillating position and velocity, but the velocity appears to be diverging. ## is this numerical error or correct integration of the equations of motion? fig = plt.figure(figsize=(10, 10)) ax = fig.gca(projection=&#39;3d&#39;) plt.quiver(a[0][0][:], a[0][1][:], a[0][2][:], a[0][3][:], a[0][4][:], a[0][5][:], color = &#39;r&#39;, label=&#39;x0(t) with -1 charge&#39;) #plt.quiver(a[1][0][:], a[1][1][:], a[1][2][:], a[1][3][:], a[1][4][:], a[1][5][:], color = &#39;g&#39;, label=&#39;x1(t) with -1 charge&#39;) #plt.quiver([a[0][1][0], a[0][1][1], a[0][1][2]], &quot;g&quot;, label=&#39;x1(t) with -1 charge&#39;) # Add Title #plt.title(&quot;Two body system of identical negatively charged particles interacting with respect to Weber&#39;s electrodynamical force law.&quot;) plt.legend(loc=&#39;best&#39;) # Display plt.axis(&#39;on&#39;) #ax.view_init(30, 20) #? Does the rotation on perspective also rotate the axes!? plt.show() . s0 = [[[1.01,0,0, 0,0,0, 0,0,0], [0.0,0,0, 0,0,0, 0,0,0]], [[1.0, -1.0], [1.0, -1.0]] ] w2b(s0) . s0 = [[[0.9,0,0, 0,0.01,0, 0,0,0], [0.0,0,0, 0,-0.01,0, 0,0,0]], [[1.0, -1.0], [1.0, -1.0]] ]; a = IW(s0, 2); # plottting fig = plt.figure(figsize=(10, 10)) ax = fig.gca(projection=&#39;3d&#39;) plt.plot(a[0][0][0], a[0][0][1], a[0][0][2], &quot;r&quot;, label=&#39;x0(t) with -1 charge&#39;) plt.plot(a[0][1][0], a[0][1][1], a[0][1][2], &quot;g&quot;, label=&#39;x1(t) with -1 charge&#39;) # Add Title #plt.title(&quot;Two body system of identical negatively charged particles interacting with respect to Weber&#39;s electrodynamical force law.&quot;) plt.legend(loc=&#39;best&#39;) # Display plt.axis(&#39;on&#39;) #ax.view_init(30, 20) #? Does the rotation on perspective also rotate the axes!? plt.show() . N=len(a[0][0][0]) print(N) diff = [[a[0][0][j][k]-a[0][1][j][k] for j in range(9)] for k in range(N)] print( [dx[0] for dx in diff] ) print( [rprime(dx) for dx in diff] ) # conservation of energy is not well managed. GEKKO is not doing a good job. print( [U(dx)+T(dx) for dx in diff] ) . .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/10/22/Weber_TwoBody_Again.html",
            "relUrl": "/fastpages/jupyter/2022/10/22/Weber_TwoBody_Again.html",
            "date": " • Oct 22, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Colloquium. Welcome to Optimal Transport",
            "content": ". Colloquium Talk: Welcome to Optimal Transport . Here is the rough content of our presentation October 18, 2022 at MRU. EA 1026. . Optimal Transport starts with question: how to get from $x$ to $y$? Or specifically, how to get unit mass $ delta_x$ from a productive source to a unit mass $ delta_y$ at consumption target? Here is instance where we have too many choices! So we begin to consider Hamiltonian Principle, and looking for ground states of energies, or, cost minimizers. . To motivate transport between Borel-Radon measures. We did not emphasize the continuity and stability of OT like we wanted to. But Borel-Radon measures and weak_$*$ convergence and the continuity properties are very important for applications to topology. So we imagine a distribution $ sigma$ of productive sources and a given target $ tau$ of consumptive targets, and the question we are asking is &quot;How to get from $ sigma$ to $ tau$?&quot; . Now as we&#39;ll describe the OT program requires a choice of $ sigma$, $ tau$, and a choice of cost $c: X times Y to { bf{R}} cup { + infty }$. . Monge, Deblais et Reblais . Let&#39;s go through history and start with Monge [(1761)] where he&#39;s literally moving volumes of earth for Napoleon. The Monge Problem is $$(MP) : ~~ inf_{T} int ||x- T(x)||d sigma(x)$$ where the infimum is over all maps $T: { bf{R}}^n to { bf{R}}^n$ satisfying $T # sigma = tau$ (if any such maps exist at all!) Given some basic regularity on $T$, i.e. Lipschitz, then we can infer the Monge-Ampere equation $$det(DT(x)) = frac{g(T(x))}{f(x)}$$ but this equation is effectively useuless in practice. . Okay, so our point is this: that Monge started with a very difficult problem, namely with the unfortunate cost $c(x,y)=d^1(x,y) = ||x-y||$. The fact that this cost is not strictly convex is problematic in practice. Moreover the fact that $x mapsto ||x||$ is not differentiable at $x=0$ is also extremely problematic in practice. In otherwords, the $L^1$ norm is a very difficult choice of cost, especially to begin with. As we will explain below, in a certain sense the most canonical cost is actually the quadratic cost $c(x,y)=d^2(x,y)/2 = ||x-y||^2/2$. . Kantorovich&#39;s Linearization . After Monge, we next consider Leonid Kantorovich, and this is big jump in history. We say that Kantorovich basically linearized the whole problem, replacing the rather intractable space of maps $T: X to Y$ with the space of Borel-Radon measures $ pi$ on the product $X times Y$, and the pushforward condition $T # sigma = tau$ being replaced with two constraints $$pr_X # pi leq sigma, ~~~ pr_Y # pi = tau.$$ Here $pr_X$ and $pr_Y$ are the canonical projections from $X times Y$ to $X,Y$, respectively. Let $SC( sigma, tau)$ represent this set of semicoupling measures from the source $ sigma$ to target $ tau$. The important point is that $SC( sigma, tau)$ is a nonempty, convex, weak-$*$ compact subset of Borel-Radon measures on $X times Y$. . Next Kantorovich observes that for continuous costs $c: X times Y to { bf{R}} cup {+ infty }$ we obtain continuous linear functionals on $SC$ defined by $$ pi mapsto int c(x,y) ~d pi(x,y).$$ So for every choice of source, target, and cost, we find the Monge-Kantorovich program: $$(MK): ~~~ min_{ pi in SC( sigma, tau)} int c(x,y) ~ d pi(x,y).$$ From this program we see that indeed the minimum is attained, being a linear function on a convex compact set. . At this stage we would like to focus on finitely supported source and targets, namely $$ sigma = delta_{x_1} + cdots+ delta_{x_n}$$ and $$ tau = delta_{y_1} + cdots+ delta_{y_n}.$$ . Then the set of measure-preserving mappings from $ sigma$ to $ tau$ corresponds to bijective maps from the n-element sets, i.e. basically corresponding to a permutation from $X$ to $Y$. However Kantorovich introduces the set of bistochastic $n times n$ bistochastic matrices $$BS(n) = {(a_{ij}) } $$ where $a_{ij}$ satisfes $$ a_{ij} geq 0,~~~ sum_i a_{ij} = 1 = sum_j a_{ij}~~ $$ for every choice fo indexes $i,j$. In otherwords all the couplings from $X$ to $Y$ corresponds to an invertible stochastic transformation, where the sum of every column and the sum of every row is equal to unity. . The key point about $BS(n)$ is the so-called Choquet-Birkhoff-von Neumman theorem that the set of extreme points of $BS(n)$ is exactly the $n!$ set of permutation matrices $extreme(BS(n)) approx Sym_n$. This means every linear functional on $BS(n)$ is minimized at a permutation matrix, i.e. represented by a bijection map $X to Y$. Equivalently the optimal coupling will not &quot;split&quot; any mass. The above remarks are intended to give some idea of the structure of the set of semicouplings $SC$. It&#39;s somehow accessible. . So again the Monge perspective is to minimize over the bijection maps $T: X to Y$, i.e. permutations $T: underline{n} to underline{n}$. The linearized Kantorovich perspective to study the convex set $BS(n)$ which consists of multivalued type maps between $ underline{n}$ and $ underline{n}$. Then we seek to minimize a linear function over a convex set (as opposed to minimizing over the discrete set of extreme points). . Kantorovich&#39;s Dual Max Program . The next step is to introduce Kantorovich&#39;s dual max program. How can we motivate this amazing step? Abruptly we can just introduce the transporters point of view, which consists of setting prices $- phi(x)$ on the source $x in X$, and prices $ psi(y)$ on the target $y in Y$. The prices are competitive if and only if they satisfy the important inequality $$- phi(x) + psi(y) leq c(x,y)$$ pointwise for all $x in X$, $y in Y$. The Kantorovich program looks to maximize the surplus over all pairs of potentials $ phi, psi$ on $X, Y$, respectively. Specifically: $$(MAX)~:~~ sup_{ phi, psi~} left( - int phi(x)d sigma(x) + int psi(y) d tau(y) right),$$ where the supremum is taken over all potentials satisfying the above inequality $- phi+ psi leq c$. This inequality implies immediately that (MAX) is less than or equal to (MK), i.e. $$(MAX) leq (MK).$$ However the amazing fact underlying optimal transportation is that the max equals the min, and there is no duality gap. This implies that $c$-optimal semicouplings are supported on the set where equality is attained $$- phi(x)+ psi(y) leq c(x,y)$$ with equality if and only if $(x,y) in spt( pi)$. This motivates the definition of $c$-subdifferentials $ partial^c phi(x)$. . (Twist) Condition and Uniqueness . Basic Regularity . Topics Briefly Touched On: . Kantorovich&#39;s dual program enables a dimension reduction! We don&#39;t really need to search out the $dim(X) times dim(Y)$ space of semicouplings, but instead the optimal transports are defined by basically $dim(Y)$ parameters, or equivalently $dim(X)$ parameters. . | We did not cover McCann&#39;s displacement convexity, the qualitative curvature descriptions, that $R_{ij} geq 0$ if and only if entropy is concave along displacement interpolations. . | The singularity $Z$ and the basic Brouwer No-Retract theorem. We interpret semicouplings from $X$ to $ partial X$ as basically representing deformation retracts. . | We briefly elaborated how $c(x,y)=d^2(x,y)/2=||x-y||^2/2$ is the canonical cost, or equivalently $c(x,y)= langle x ,y rangle$. However there is no real canonical cost from ${ bf{R}}^n to { bf{R}}^k$. . | Challenging to construct costs between arbitrary surfaces, e.g. $T^2 times S^2 to bf{R}$ or $S_g times S^2 to bf{R}$ where $S_g$ is a genus $g$ hyperbolic surface. In fact more interesting is constructing costs from $X$ to $AG_0(S^2)$ using the Dold-Thom theorem. . | .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/10/19/Colloquium_MRU.html",
            "relUrl": "/fastpages/jupyter/2022/10/19/Colloquium_MRU.html",
            "date": " • Oct 19, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Fermat Weber Points and L1 Problems with No Formulas.",
            "content": ". Fermat-Weber Points of Probability Measures &#956; . We&#39;ve been reviewing a Mathoverflow post here about finding formulas for the Fermat-Weber points of a probability measure $ mu$ on ${ bf{R}}^n$. The FW point is that point $ bar{x}$ which minimizes the average $L^1$ distance to the measure, i.e. $FW$ is the minimizing argument of $$ min_x f_ mu(x)= min_{x} int ||x-y|| ~d mu(y),$$ where $x$ ranges over ${ bf{R}}^n$. It&#39;s remarkable that this problem admits no good formula. The problem is that $f_ mu(x)$ defined above is not everywhere differentiable on the domain, specifically being not differentiable along the support of $ mu$. . Now the minimizing program is readily computed approximately, i.e. numerically. This means we can basically start with any initial point $x_0$ and find a type of gradient flow to find sequence $x_1$, $x_2$, $ ldots$ which converges to a minimizer $ lim_{k to + infty}x_k= x_ infty$. In practice, this is good enough. . But what about the formula? This is typical of naive mathematicians, to always seek out a &quot;formula&quot;. But formulas are the exception. There&#39;s probably an easy argument that numbers given by &quot;formula&quot; are exceedingly rare, i.e. measure zero on the real number line ${ bf{R}}$. For example, consider the problem frequently arising in probability: if $y=F(x):=Prob(X leq x)= int_{ infty}^x f(z)dz$ is the cumulative distribution function which is monotonically increasing, then it&#39;s known that the inverse $x=F^{-1}(y)$ exists, but does not typically admit a formula. . Let&#39;s consider the Legendre-Fenchel transform ${f_ mu}^*(n)$ of the function $f_ mu(x):= int ||x-y|| d mu(y)$. Is a formula for $f^*_ mu$ available? Here we must acknowledge the unfortunate fact that Legendre transforms of convex combinations are not immediately computable. That is, we find only the general inequality $$( frac{1}{2}f_0 + frac{1}{2}f_1)^* leq frac{1}{2}f_0^* + frac{1}{2} f_1^*.$$ . This implies ${f^*_ mu}(n)$ satisfies the pointwise inequality $${f^*_ mu}(n) leq nu(n)$$ where $ nu$ is the &quot;naive&quot; convex recombination $$ nu(n):= int langle n, y rangle ~d mu(y)$$ if $||n|| leq 1$, and $$ nu(n)=+ infty$$ otherwise. Notice that the naive transform $ nu$ is basically a linear function on it&#39;s domain, namely $ langle n, bar{y} rangle$ where $ bar{y} := int y ~d mu(y)$. It would be convenient to have a general rule or formula for $f^*_ mu$, but this is not achievable at this stage. . It proves again that the naive $L^1$ cost $c=d^1$ might seem convenient, but it&#39;s really not due to its nondifferentiability along the diagonal. For example, this implies that $f_ mu$ is not differentiable along the support of $ mu$. It&#39;s clear that $f_ mu$ is finite and continuous along $spt( mu)$, however the gradient formula $$ nabla_x f_ mu(x) = int frac{x-y}{|x-y|} d mu(y), $$ and find the integrand undefined whenever $x=y$ for some $y in spt( mu)$. Notice the integral defining $ nabla_x f_ mu$ is a sum of unit vectors. . Here arises a question from first year calculus. What are the critical points of $f_ mu$? Strictly speaking, the critical points consist of those points $ bar{x}$ where $ nabla_x f_ mu( bar{x})=0$ and those points where the function $f_ mu$ is not differentiable. Therefore we find $$Crit(f_ mu) = { bar{x}~|~ nabla f_ mu( bar{x})=0 } cup spt( mu).$$ . This makes the discovery of the minimizer rather difficult, because we also need to compare the critical value at $ bar{x}$ with the values of $f_ mu$ restricted to $spt( mu)$. But this restriction $f_ mu|_{spt( mu)}$ is not differentiable, so there are no critical points to speak of, i.e. every point $x in spt( mu)$ is a critical point according to the strict definition! And here is the basic difficulty in using $L^1$ distances, and the inability to identify the minimizer. . The question arises: &quot;Is the restriction of ${f_ mu}$ to the support $spt( mu)$ really nondifferentiable?&quot; . Here the properties of $spt( mu)$ as a subset of ${ bf{R}}^n$ has to play a role in deciding the behaviour of $f_ mu$ when restricted to the support $spt( mu)$. But this is a question for another time... .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/10/18/FermatWeberPointsNoFormula.html",
            "relUrl": "/fastpages/jupyter/2022/10/18/FermatWeberPointsNoFormula.html",
            "date": " • Oct 18, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "How to Stabilize Blum's Medial Axis Transform.",
            "content": ". How to Stabilize the Medial Axis Transform . We&#39;ve had a draft paper on github for a while related to our study of Blum&#39;s medial axis transform $M(A)$ of open subsets $A subset { bf{R}}^n$. We were seeking an elementary proof of the homotopy-isomorphism between $M(A)$ and $A$, but there were some technical difficulties related to an interesting paper by Nirenberg-Li. Here technical hypotheses on the boundary of $A$ are nececessary. In general, the boundary of an open subset $A$ of ${ bf{R}}^n$ satisfies $$dim_} ( partial A) geq dim(A) -1.$$ This is because there exists continuous surjection of $ partial{A}$ onto $n-1$ hyperplanes in ${ bf{R}}^n$. . In this post we want to propose a solution to the notorious instability of $M(A)$ with respect to perturbations $A&#39;$ of $A$. In practice, what happens is that small perturbations or &quot;bubbles&quot; $A&#39;$ will cause large branches to appear in $M(A&#39;)$. This is a cause of discontinuity with respect to Gromov-Hausdorff distance of the medial axes $M(A)$. . [The following figure is taken from a paper on MAT transform, we don&#39;t recall which one. Apologies!] . . However, what our Reduction-to-Singularity method shows [link to thesis] is that these tendrils are homotopically trivial, and therefore the maximal reduction of $M(A)$ is continuous with respect to GH-topology. . More formally our thesis establishes a maximal reduction $M(A) leadsto W(A)$, where $W(A) hookrightarrow M(A)$ is a homotopy-isomorphism. We are proposing that $W(A)$ is basically $M(A)$ minus the trivial branches. And what we propose is that $W(A)$ is continuous with respect to GH-topology. This means if $A_k$, $k=1,2,3, ldots$ is a sequence of open subsets converging in GH-topology $$ lim_{k to + infty} A_k = A_ infty, $$ then $W(A_k)$ GH-converges to $W(A_ infty)$. By contrast we cannot expect any convergence between $M(A_k)$ and $M(A_ infty)$ because $M$ is generally only upper semicontinuous. . Warning. The above paragraphs are speculative at this stage, and not formally &quot;proven&quot;. .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/10/15/StabilizingMAT.html",
            "relUrl": "/fastpages/jupyter/2022/10/15/StabilizingMAT.html",
            "date": " • Oct 15, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Weber-Kirchoff's Response to Veritasium's Huge Circuit Problem",
            "content": ". Veritasium&#39;s Problem . Wow, I&#39;m impressed by Veritasium&#39;s videos on electricity. The question he poses is so fundamentally interesting that we must review it here. Our goal in this post is to record what we believe Weber and Kirchoff&#39;s answer to this question would be. . Let&#39;s recall Veritasium&#39;s setup: We imagine a huge circuit, where the lightbulb and battery are spatially close, but where the circuit connecting the battery and lightbulb is extremely long. See the screenshot from his video below: . . The question asked by Veritasium is this: &quot;How much time is required for the lightbulb to turn on when the circuit is closed?&quot; . The question is seemingly a contest between two ideas: is the electricity like a fluid that must necessarily travel through and along the wire, therefore traversing a large distance requiring a &quot;large&quot; amount of time. Or is electricity like the Maxwellian field view that the battery somehow transfers energy through the field to the lightbulb. And since the battery and lightbulb are very close (small distance) it&#39;s assumed that the lightbulb turns ON in a very &quot;small&quot; amount of time. . There have been many critical response videos. For example ElectroBoom makes some fair critiques. For example, the threshold for how much energy is required to turn the lightbulb ON needs to be specified, and in practice there is some leakage voltage coming from electrons in the environment. But this is not really the point. . The Maxwellian Field View of Energy Transfer . The following image shows Veritasium&#39;s Maxwellian field-theoretic viewpoint, however ElectroBoom makes a very significant point about the field, specifically about the Poynting vector field $S= frac{1}{ mu}E times B$. What Electroboom argues is that $S$ has a $1/r$ term -- but he does not explain where this $1/r$ term arises, so it&#39;s not clear that ElectroBoom&#39;s point is valid. The issue is that Veritasium is showing the flow of a Poynting field in a situation which is geometrically very different from the original problem. . In this image the yellow lines show the Poynting vector, which is assumed to represent energy flux. But the point is that the circuit depicted in the image is not at all like the original circuit. . . Weber and Kirchoff&#39;s Response: the Surface Charge Distribution . The best reference for Weber and Kirchoff&#39;s work on electricity is AKT Assis&#39; book on &quot;The Electric Force of a Current&quot; available at AKT Assis&#39; webpage. Basically Weber and Kirchoff identify the time required for the lightbulb to turn ON to be equal to the time required for the battery to assemble the surface charge distribution representing the steady state current. This needs be elaborated further. . Briefly, what Weber and Kirchoff discovered is that the purpose of the battery is to work and sustain a surface charge distibution on the wire. Consequently, the time required for the lightbulb to turn ON is precisely the time required for the surface charge distribution to be assembled by the battery. The battery does work using either electrochemical energy or other sources of mechanical or non-electrical energy. . We do not precisely know the time required for this surface charge distribution to be assembled, and it&#39;s very interesting question. However this charge distribution is very quickly attained because the electric charge elements do not need travel very far. The surface charge does not require, for example, any negative charge elements to flow &quot;from the battery&quot;. There is no &quot;flow&quot; from the battery to the lightbulb, contrary to the Maxwellian-Poynting conception. For Weber and Kirchoff, the battery is specifically used to assemble the surface charge distribution. . I suppose the interesting question is: &quot;Why does the battery only do work on the surface charges when the circuit is closed?&quot; In otherwords, &quot;_What is the difference between open and closed circuits?&quot;_ . The Battery . At issue is the question, &quot;What is the battery doing?&quot; The idea of Weber-Kirchoff is that the battery does work, namely assembling and sustaining a surface charge distribution. This is treated in more detail in Assis&#39; textbook cited above. . Now Veritasium are somehwat aware of surface charge distribution in wires. See the image below. Apparently they learned this from Shabay-Sherwood&#39;s textbook &quot;Matter and Interaction&quot;, and this same text is influential on AKT Assis. However Assis demonstrates how Weber and Kirchoff effectively found the correct formulation of Ohm&#39;s Law, which leads to more precise equations for the surface charge distribution. . . Confusion and Uncertainty in the Maxwellian Viewpoint. . We remark that Veritasium phrases his question in classical terms. And it&#39;s very interesting that we are comparing &quot;conjugate variables&quot; in the sense of quantum mechanics, i.e. Energy and Time. This is like asking for the velocity of a particle when it passes through a certain point. Heisenberg&#39;s Uncertainty Principle basically precludes any definite answer! So we must clarify that Veritasium is seeking a classical estimation of the time required. And we do not object to this at all! In otherwords, Veritasium&#39;s question is asking for the time required before a certain amount of energy is attained by the lightbulb. And Heisenberg Uncertainty does not allow us to precisely know both values simultaneously. . This quantum uncertainty is somewhat bypassed because the battery and the lightbulb are proximally &quot;close&quot;, i.e. they are extremely close and not separated by a large distance. This somewhat allows us to bypass the controversial question of whether information is a priori bounded by the speed of light constant $c$. . Ultimately, when Maxwellians look at the power lines, they look at the empty space between the powerlines and imagine there is energy and a field there. But the field is not reified, i.e. it has no substance (no length or mass or area, etc.) The field has no temperature or pressure or anything definable. It is pure mathematical symbolism: $E$ and $B$. For indeed Maxwell must define two fields. And how do these fields interact with matter? . Conclusion . This post argues that Weber-Kirchoff interpret the time required in Veritasium&#39;s huge circuit problem as exactly the time required for the battery to assemble the surface charge distribution representing the stead state current. This is a mathematical problem to be studied further. Using the Weber-Kirchoff equations, this time should be amenable to mathematical analysis, and this to be subject of further study. . [To be continued ... -JHM] . To explain further the idea: the question is somewhat related to the following question: suppose we have a density of positive and negative electric charge distributions $ rho$ on a unit disk $D$. If we let the potential $ rho$ evolve according to Coulomb&#39;s force law to equilibrium, then either the charges neutralize or they are pushed through the boundary $ partial D$. Given that the disk has unit radius, we ask the question: &quot;How much time before the charge distribution is neutralized (either in the interior or at the boundary)?&quot; All this needs to be elaborated in more detail. .",
            "url": "https://jhmartel.github.io/fp/2022/10/08/Veritasium_Response.html",
            "relUrl": "/2022/10/08/Veritasium_Response.html",
            "date": " • Oct 8, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Deciding Injectivity and C.A.Feinstein's Proposed Negative Solution to P vs NP.",
            "content": ". More Remarks on Craig Alan Feinstein&#39;s P versus NP argument. . In previous articles here and here we have been reviewing Craig Alan Feinstein&#39;s proposed solution to the $P$ versus $NP$ problem, and he has been poorly received by the so-called experts. But we continue to find his papers interesting, and this post is mainly about his Dialogue Concerning The Two Chief World Views which contains some interesting remarks on precisely what is the argument being made by CAF that $P neq NP$. . The first point is that given a finite subset $X subset bf{Z}$, there are indeed exponentially many expressions in $2^X$. This is obvious. But generically there are also $2^{ #(X)}$ distinct values in the image $ epsilon: 2^X to bf{Z}$ where $ epsilon(Y) = sum_{y in Y} y$ is the natural addition map defined on subsets $Y in 2^X$. | This point is made as rebuttal to the objection that if $X$ is bounded a priori, then it&#39;s known that there are polynomial many distinct values of the image $2^X to bf{Z}$. [Reference needed]. We think this is important observation, but one which might require more evidence or proof. Can we more formally prove that arbitrary subsets $X subset bf{Z}$ almost surely have exponentially many distinct values in the image $ epsilon: 2^X to bf{Z}$? . If $X$ is in fact a subset of the powers of $2$, namely $$ {1, 2^2, 2^4, 2^8 }$$ then there&#39;s large gaps between the elements, and we think it&#39;s clear that the image is exponential. What&#39;s also interesting in these cases is that it&#39;s also evident that there is a negative solution to the SUBSET-SUM problem. . And this raises the second interesting point in CAF&#39;s paper: . The algorithms which are competing with the &quot;meet in the middle&quot; algorithm (which has $O(2^{n/2})$ time-step complexity) must also have the ability to deterministically return a negative answer to SUBSET-SUM(X) if there are no zero subset sums for the given input $X$. | Perhaps it can be argued that generically the solution to SUBSET-SUM is negative, and therefore the exponential search is almost always required to determine negative solutions. It&#39;s maybe useful to contrast this with algorithms which are known a priori to have positive solutions, for example linear sort of a list of integers. There are surely many more examples of algorithms which are known a priori to terminate with positive solutions. For example, Hilbert Nullstellensatz as described by [Blum-X-Smale-Shub-et.al.] over the complex numbers. . And actually, doesn&#39;t it now appear that: the whole question of SUBSET-SUM(X) is precisely about the injectivity of the map $ epsilon: 2^X to bf{Z}$. This is because $ epsilon$ is basically linear and the difference $$ epsilon(f)- epsilon(g) = 0$$ if and only if $ epsilon(f-g)=0$. But $f-g$ represents a signed symmetric difference, so it&#39;s not exactly a solution of SUBSET-SUM. . Idea: perhaps it&#39;s possible that the signed subset-sum problem is equivalent to unsigned subset-sum. If we return a signed solution $f-g$ then we can examine whether its a positive solution by looking at the signs of the coefficients. This would only add a linear time-complexity. So maybe we should simply focus on the complexity of injectivity of $2^X$ and its relation to negative solutions. Because any algorithms which competes with MITM algorithm must at least solve this decision problem. . On the Injectivity of $ epsilon: 2^X to bf{Z}$ . The idea of the previous section is to actually relax somewhat and consider the Signed Subset-Sum problem, maybe abbreviated $SS_{ pm}(X)$ for a given input $X$ in contrast to $SS(X)$. With this point of view the signed subset sum problem becomes precisely equivalent to the injectivity of the augmentation map $ epsilon: 2^X to bf{Z}$. . This suggests the following question: . _What is the computational time-step complexity of deterministically deciding the injectivity of a linear map $T: V_1 to V_2$._ . What would it mean if the complexity of injectivity of linear maps was essentially $O(dim(V_1))$, or can it be decreased? . What does category theory say about injectivity? . Does the strange Yoneda Lemma have any application? I know the logician&#39;s find that an important representation theorem for certain categories defined over Sets or something, but I never quite understood it. . There are further remarks to be made about CAF&#39;s &quot;Two Worlds&quot; paper. . The critics employ an invalid meta-mathematical critique of CAF&#39;s proof when they inaccurately reason that &quot;if your proof was correct, then [THIS] would be true.&quot; (A kind of reductio ad absurdum) | One such example is this: they say that if we apply CAF&#39;s proof to finding solutions in integers of linear equations $$ sum_i a_i x_i =0 $$ where $a_i in bf{Z}$ and $x_i in bf{Z}$, is asymptotically much faster than $O(2^{n/2})$. This is true, but a trivial example since solutions to such equations are very immediately found. For example, set all unknowns to zero except two. Then it&#39;s trivial to solve over $ bf{Q}$ the linear homogeneous equation $ax+by=0$ when $a,b$ are nonzero constant integers. . However it&#39;s somewhat more interesting for inhomogeneous linear equations over integers. For example, it&#39;s not so easy to solve $$2x_1 + 13x_2 =9.$$ Here we enter into the commutative algebra interpretation. For the range of all possible values obtained by $2x_1 + 13 x_2$ is determined by a unique integer (modulo units), namely the greatest common divisor $gcd(13, 2)$. And the determination of gcd&#39;s is obtained via the Euclidean algorithm. For example, we have $13=2.6+1$. Therefore $1.13 - 6.2=1$ and $(x_1, x_2)=(-6, 1)$ is a solution in integers. . That&#39;s clear, so what do the critics say? Well, the image of $2x_1 + 13x_2$ is indeed large (infinite), yet we have found solutions without searching through this infinite maze. This is true, so what&#39;s the difference? Feinstein argues that the linear equation over integers admits many algebraic identities, or rearrangements. This is somewhat the key of the euclidean algorithm which reduces the coefficients of the equation into essentially equivalent linear expressions, and ultimately into a trivial equation. . Here enters somewhat the idea of the ideal generated by the coefficients over $ bf{Z}$. The image of the linear integer equation is infinite, but it&#39;s highly structured, being determined by the multiples of the generator of the ideal, i.e. the gcd of the coefficients. This structure enables the significant speedup in finding solutions.And the point is that this reduction is not possible in the (signed) subset-sum problem for finite sets. Why? Because the image is much less structured, and not forming any &quot;ideal&quot;. . If we speculate a little, perhaps this is another reason why SUBSET-SUM is so difficult. It&#39;s an extremely constrained version of a much easier problem. I.e. if we are allowed to arbitrarily duplicate elements of $X$, then we could basically realize better approximate solutions to zero subset sum. . Complexity of Deciding Divisibility over $ bf{Z}$ . This relates to a curious point about Euclidean algorithm, namely the backsubstitution that is necessary to return to the first solution. If we are simply studying the decidability problem, then we never need this backsubstitution. This relates to CAF&#39;s observation that the equation $$83x_1 + 18x_2=t$$ for example, has a solution if and only if there exists $z in bf{Z}$ such that $gcd(83, 18).z=t$ where $t$ is the integer target value. With Euclidean we find $gcd(83, 18)=1$ and therefore the equation $83x_1 + 18x_2=t$ is effectively equivalent to the equation $$1.z=t$$ which has trivial solution. So Euclidean algorithm allows us to reduce the equation into a simpler, almost trivial equation. . But the point CAF makes, and perhaps it deserves more elaboration, is that the equations defining SUBSET-SUM are not defined over structured rings with Euclidean algorithms. Maybe it suggests a non-algorithm of trying to perform Euclidean algorithm with only coefficients of $ pm 1$ involved and never really being allowed to re-use an element. It&#39;s highly constrained. . To conclude, the decidability of integer solutions depends on a reduction, depending on Euclidean algorithm finding the primite generator of the ideal generated by the integral coefficients, to a much simpler equation $$gcd(a_i).z=t,$$ and the decidability question becomes reduced to &quot;Is the gcd of the coefficients a divisor of the target?&quot;. . So after Euclid, everything is reduced to the question How is divisibility decided ? I.e., how to decide the function &quot;isDivisible(a,b)&quot;. There&#39;s a discussion of the decidability here. Also interesting is the relation to Newton&#39;s method, sometimes called Newton-Raphson division. . Remark. The efficiency of the Newton-Raphson is not immediately relevant to the question, since it efficiently finds a solution, yet our decidability problem is related to determining whether $x=t.a^{-1}$ is integral. The question would then become whether Newton&#39;s method is sufficiently accurate to distinguish integral versus rational values! . For example, to solve $ax=t$ requires $x=t.a^{-1}$. But the value of $a^{-1}$ can be expressed as finding the zero of the function $0=f(x)= frac{1}{x}-a$, which has derivative $f&#39;(x)= frac{-1}{x^2}$. Applying Newton&#39;s method to finding approximate zeros $x_n$, we find approximate quotient equal to the product $t.x_n$ and we must decide whether $t.x_n$ is integral. . Reviewing the Wikipedia article on Newton-Raphson has interesting remark on the quadratic convergence of Newton&#39;s method. To quote: . &quot;This squaring of the error at each iteration step – the so-called quadratic convergence of Newton–Raphson&#39;s method – has the effect that the number of correct digits in the result roughly doubles for every iteration, a property that becomes extremely valuable when the numbers involved have many digits (e.g. in the large integer domain)&quot;{end quote} . It&#39;s simple enough to recall the idea behind Newton&#39;s method. At an initial point $x_n$ we evaluate the $x$-intercept of the tangent line at $(x_n, f(x_n))$, which tangent line has equation $$ frac{x-x_n}{y-(1/x_n-a)} =-x_n^2. $$ Therefore the $x$-intercept is obtained by $$x_{n+1}:=-x_n^2(0-1/x_n+a)+x_n=x_n(2-ax_n).$$ . We remark that there is something arbitrary in our choice of auxiliary function $f(x)= frac{1}{x}-a$, namely all we really needed was a function $F$ such that $F(a^{-1})=0$) prescribed zero and have well-behaved gradient descent into that zero. That&#39;s what is obtained by the analytic expression $f(x)=1/x-a$, and using Newton&#39;s calculus we find the slope of the tangent line everywhere as $-1/x^2$ by which, again, via calculation that we have recursive formula for a quadratically fast converging approximation. . Here we face the same difficulty as outlined by Blum-Cucker-Shub-Smale in their real complexity book. . [To be continued ... -JHM] . We might compare two algorithms for determining divisibility. The first algorithm is naive, deterministic, and originates in Eudoxus and Archimedes axiom. It runs like this: . def isDivisible(a,b): k=0 while k*a &lt; b: k=k+1 if k*a == b: return True elif k*a &gt; b: return False . print(isDivisible(7, 39)) print(isDivisible(7, 48)) print(isDivisible(7, 49)) print(isDivisible(7, 59)) print(isDivisible(7, 69)) print(isDivisible(3, 2**(12)-1)) print(isDivisible(5, 2**(12)-1)) print(isDivisible(7, 2**(12)-1)) print(isDivisible(3, 2**(13)-1)) print(isDivisible(19, 2**(13)-1)) . False False True False False True True True False False . What&#39;s notable about the function isDivisible(a,b) is that it is guaranteed to return an answer given enough steps or iterates. The Eudoxus-Archimedes method of exhaustion guarantees this fundamental property. It is somewhat exhaustive search, iterating patiently through $k$, $k+1$, $k+2$, etc., but which vanishes at some definite (but arbitrarily large step). . There is a faster algorithm for the floating point approximation. In the following code we implement a faster algorithm, however it&#39;s not necessarily deterministic, and in fact does not really decide whether $a$ divides $b$, but gives a floating point number which is indicative of whether there is divisibility. . def inv(a, initial=0.1, steps=7): x = initial i=0 while i &lt; steps: x = x*(2-a*x) i=i+1 #print(&quot;the&quot;, i, &quot;th approximant is .&quot;, x) else: pass return x #the second function returns the arithmetic quotient b/a. # but the algorithm is not yet deterministic about returning a TRUE when # the resultants are integral. def div(a,b): return b*inv(a) . print(div(3,8)) print(div(9, 81)) print(div(5, 1225)) print(div(3, 2**(12)-1)) print(div(5, 2**(12)-1)) print(div(7, 2**(12)-1)) print(div(3, 2**(13)-1)) print(div(19, 2**(12)-1)) . 2.666666666666667 9.0 244.99999999999997 1365.0000000000002 818.9999999999999 585.0 2730.3333333333335 215.52601618967765 .",
            "url": "https://jhmartel.github.io/fp/2022/10/06/Feinstein_Part2.html",
            "relUrl": "/2022/10/06/Feinstein_Part2.html",
            "date": " • Oct 6, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Computational Complexity of Fibonacci Sequences. Part 1.",
            "content": ". The papers of Alan Craig Feinstein on $P$ versus $NP$ have caused me to previously review some basic questions in computational complexity. This post continues our line of thought on what &quot;computational complexity&quot; means from the pragmatic perspective. This in contrast to &quot;theoretical&quot; computation results, which is a contradiction in terms. . In this post we simply consider the question: What is the computational complexity of computing the $n$-th Fibonacci number? . Let&#39;s recall the basic definition: the fibonacci sequence is a sequence of integers $f_0, f_1, f_2, ldots$ defined recursively by the rules $$f_0=1, ~~ f_1=1, ~~ text{and}~~ f_{n+1}=f_n+f_{n-1}~~ text{for}~~ n geq 1.$$ . Naively to compute $f_{2022}$ would require we apply the definition and find $f_{2021}$ and $f_{2020}$, and then continuing the recursion we need to call all the previous values of $f_k$ for $k &lt; 2022$. What is the precise computation? . Let&#39;s begin with the pseudocode. The idea is memoization. If we apply the above recursive formula to evaluate $f_{2022}$, then we&#39;ll find ourselves reusing values of $f_k$ for $k &lt; 2022$. The memoization is simply a device for recording and reusing these intermediate values. So to compute $f_{2022}$ we will need to store the previous values, although there is a method (Bottom-Up evaluation) which is similar, and which inductively evaluates $f_1$, $f_2$, $f_3$, etc., until we reach $f_{2022}$. So if we compute $f_{2022}$ and memoize all the terms $m= {f_1, f_2, f_3, ldots }$ then we see the memo $m$ grows unbounded in length. Some authors modify the memo to only contain the two &quot;largest&quot; elements, thus reducing the length of the list. This would require updating the memo $$ {f_1, f_2 } leadsto {f_2, f_3 } leadsto {f_3, f_4 } leadsto cdots leadsto {f_{n-1}, f_n } leadsto cdots.$$ But still the number of bytes necessary to represent the fibonacci elements grows exponentially with $n$ since $f_{n+1} approx varphi^n$ where $ varphi$ is the golden ratio $ varphi= frac{1+ sqrt{5}}{2} approx 1.618033$. . The memoization gives a time complexity of $O(n)$ to compute $f_n$. But memoization requires memory to store the memo! We think this needs be accounted for in the computational complexity, since obviously it&#39;s an essential part of the computation. Indeed computation as represented by Turing machines, say, involves motion and read/write/react at different locations on the &quot;tape&quot;. It seems that a proper accounting of complexity should involve every time-step, namely every motion and site operation. In analyzing the complexity of the usual $O(n)$ complexity of computing $f_n$, the action of calling and retrieving the data in the memo are considered to be constant. But is it truly constant in the Turing model? . fib_cache = {0 : 0, 1 : 1} def fib(n): if n in fib_cache: return fib_cache[n] for i in range(100, n, 100): fib(i) fib_cache[n] = result = fib(n-1) + fib(n-2) # print(fib_cache) # &lt;--this line is diagnostic. return result # There&#39;s interesting comment on why the range(100, n, 100) line is important # to avoid a &quot;recursive depth error&quot;. The point is that the memo needs to be filled # as the algorithm develops, otherwise it recurses too far and returns an error! . # fib(1111111) fib(11111) . 515449135231559341621591189426925989418721609167804403107087312453694294479381404009230187092526675635022414542794904158934368158350216751867828729213516067642461147232232268304004580596220514978296704097915617589481297010691749471374967376304898014174716132125169572206688299944881902864940487579850754037243411123276226268998274067232370656873037885028569262362061989878439356579125363739644638605976667733232134130196207453194213358616463005487086631652051025004934485196108344869244852506414543015664379038338857611347469102943415360234480491921571800239803284078147859161629100936007246749423449323827401152142684138017539299637210829955409666781554035555164800825902557894478979141680264821730580654526990976167873657740460977594388216677737796493623940501749951993194553070912364327001564086186444836587037180810655016948562608480145057901528396467327800369394725748856906177005886944277609832795419082424474419033931754123200248752600310587761729439189440527073799320938597514569967706344559861576816209214912702962065352672071494639021231263782338510241176219316180788341549329905272081790433223099472061887224254193326845457247107409050092252994931668934553671721376871177693036352993174131508107261653495025271505086039171034392185521383307925723081097129536244468148375789733582131797176285225457221029865658845503179175504307577930140222583997281098099332145783930418777810346276337273420733754768191403158839413163368990092771464626510432292314209966950363068432367028332284209840897425718364670733733609565321893240873729315360915814803137552560521106490937691421540344502423323064743545226360364012549367167257038202145921861042955299329942301124074181956428710271876930526019606797077558959445434943166179407403375284366340173639269807373108055388080201746447050804598946499248800891171987624229020766742994219485280547337990630263452898332213470171667603200991268579583095661682595442200149483262133621860660302141160974707437100532341443580636798210704649175613121627855118061762876137389590812891131603206815601843823369210865672605256743142632199819790960079549275267815983406188538500072911327187912330503064073869613282412315795790671452556371408354045852898125070873750632615799469070245720036053071341314252092446074260578417947762296896685389368685659291620443322232933074723685001342680075497489 . Wolfram&#39;s (non)Definition of Computational Irreducibility. . In Wolfram&#39;s book &quot;A New Kind of Science&quot;, there is introduced the idea of computational irreducibility. We quote from Chapter 13, pp.739-740. . [Begin quote] . &quot;Computational irreducibility leads to a much more fundamental problem with prediction. For it implies that even if in principle one has all the information one needs to work out how some particular system will behave, it can still take an irreducible amount of computational work actually [sic] to do this. . Indeed, whenever computational irreducibility exists in a system it means that in effect there can be no way to predict how the system will behave except by going through almost as many steps of computation as the evolution of the system itself. . In traditional science it has rarely even been recognized that there is a need to consider how systems that are used to make predictions actually operate. But what leads to the phenomenon of computational irreducibility is that there is in fact always a fundamental competition between systems used to make predictions and systems whose behaviour one tries to predict. . _For if meaningful general predictions are to be possible, it must at some level be the case that the system making the predictions be able to outrun the system it is trying to predict. But for this to happen the system making the predictions must be able to perform more sophisticated computations than the system is trying to predict._ . But the remarkable assertion that the Principle of Computational Equivalence makes is that this assumption is not correct, and that in fact almost any system whose behaviour is not obviously simple performs computations that are in the end exactly equivalent in their sophistication. . So what this means is that systems one uses to make predictions cannot be expected to do computations that are any more sophisticated than the computations that occur in all sorts of systems whose behaviour we might try to predict. And from this it follows that for many systems no systematic prediction can be done, so that there is no general way to shortcut their process of evolution, and as a result their behaviour must be considered computationally irreducible.&quot; . [End quote] . Here&#39;s what I like about Wolfram&#39;s idea of &quot;computational irreducibility&quot;. Firstly it aligns with experience. Mathematicians who actually compute things know that there&#39;s no shortcuts for alot of operations. It&#39;s very difficult to add two numbers $a+b$ without actually &quot;adding&quot; them arithmetically. In otherwords, mathematicians don&#39;t expect shortcuts. This relates to Craig Alan Feinstein&#39;s arguments that $P neq NP$, and specifically that the subset-sum problem is computationally irreducible. To decide whether a set of signed integers $S$ contains a zero subset-sum, there really is nothing we can do except exhaustively search through all the subsets and add the elements. But most theoretical computer scientists &quot;dream&quot; of somehow there existing a shortcut, a black box, a sophisticated magical algorithm that will compute the sums without actually computing them. So Wolfram&#39;s idea of &quot;computational irreducibility&quot; is in opposition to the theoretical computer scientists&#39; optimism. . But what&#39;s lacking is any rigorous proof or definition of computational irreducibility. If I had to make a pseudo-definition (intuitive) I would first say: . a computation is the evaluation of a function $f: X to Y$, . | and the evaluation $f$ is computationally irreducible if the complexity of any equivalent composition $g: X to Z$ and $h: Z to Y$ with $f = h circ g$ satisfies $$c(f) leq c(h) + c(g),$$ where $c(f)$ represents the &quot;computational complexity&quot; of evaluating a function $f$. . | . However the complexity $c(f)$ is not well-defined at this point. But the idea is that a function $f$ always admits (nonunique!) compositions $f=h circ g$, but the question would be whether these compositions $h, g$ are any simpler than the evaluation of $f$ itself! . The above is not sufficiently rigorous since we have not defined $c(f)$. Now it must be clear that $c(-)$ is not defined on the category of functions, since compositions $f=h circ g$ are identical functions, but not necessarily identical computations. . For example, Wolfram would argue that the Collatz function arising in the Collatz conjecture $3x+1$ problem is computationally irreducible. There are no shortcuts for evaluating the Collatz function, and therefore there will never be a proof of Collatz&#39; conjecture. This is why no mathematician should study the problem, because the results/outputs of the Collatz function cannot be logically established, they can only be empirically established by directly evaluating the Collatz function using the definition. . Topological Irreducibility . Let&#39;s examine the word irreducibility somewhat more. Wikipedia has many references, but the primary definition (for our perspective) is the topological irreducibility. Here is the definition from Wikipedia . Def. (Reducible and Irreducible Topological Spaces) . _A topological space $X$ is reducible if it can be written as a union $ X= X_1 cup X_2$ of two closed proper subsets $X_1, X_2$ of $X$._ | A topological space $X$ is irreducible (or hyperconnected) if it is not reducible. Equivalently, if all non empty open subsets of X are dense, or if any two nonempty open sets have nonempty intersection. | . So the computational aspect of irreducibility would involve computing intersections of nontrivial open subsets of the space. Formally it would be better to provide the relative definition of irreducibility for subsets $A subset X$, but we just use the subspace topology on $A$ in $X$. I.e., open subsets of $A$ are defined as intersections $U cap A$ of open subsets $U$ of $X$. It&#39;s useful to introduce the relative definition because our first idea for defining computational complexity is to consider the topology of the graph of the function in question, i.e. $graph(f) subset X times Y$. Now if we have a composition $f = h circ g$, then we obtain two graphs(!) namely $$graph(h) subset Z times Y$$ and $$graph(g) subset X times Z.$$ . I suppose it would be interesting to determine whether the graph of the Collatz function is an irreducible topological subset of the product $ bf{N} times bf{N}$. And here we need specific the topology of $ bf{N}$, which is not so obvious, although perhaps the discrete topology is the most natural. . To this point we haven&#39;t proved anything new, we&#39;ve only conceptualized the problem, trying to more formally define &quot;irreducibility&quot;. We have yet to define the complexity $c(f)$ of a function although our idea is that $c(f)$ should reflect a property of $graph(f)$. Dependancy on the graph of $f$ allows us to compare compositions $f=h circ g$ and maybe develop comparisons between $c(f)$, $c(g)$, $c(h)$, etc.. . But the point is that we have a strategy now, and we can consult the literature by looking for techniques to prove the irreducibility of topological spaces. This is a general problem, and we can even begin with the more specialized case of algebraic geometry. . In algebraic geometry, if a variety is irreducible then what does that mean in concrete terms for the equations defining the variety? For hypersurfaces it means the variety can be described by two equations instead of one. For example a polynomial $p(x)$ is reducible if $p(x)=q_1(x) . q_2(x)$ for nontrivial polynomials $q_1, q_2$. What&#39;s interesting for polynomials is that the degrees of the products are strictly smaller, namely $$deg(p) = deq(q_1)+deg(q_2).$$ . But let&#39;s hypothesize that the complexity of evaluating a degree $d$ polynomial is $d$. Then the complexity of computing $p=q_1 q_2$ is not $deg(p)$ but actually $$ max { deg(q_1), deg(q_2) }.$$ This is assuming that the product $p=q_1 . q_2$ has constant time complexity, which is reasonable for real or complex numbers. So this is elementary examples of how topological reducibility relates to computational complexity. And this seems to be the beginning of maybe something more interesting. . Complexity of Exponentiation . There is a recurring idea in evaluating the complexity of exponentiations, and it&#39;s based on taking successive squares based on the binary representations. This is where the $O(log_2 n)$ complexity algorithm for evaluating $f_n$ is derived. This speedup depends on the fact that the rule for defining the fibonacci sequence is linear and static, namely the infinite sequence of rules $f_{n+1}=f_n+f_{n-1}$ for $n geq 1$ is almost only one rule $$f_{*+1} = f_* + f_{*-1}.$$ . This relates to the observation that iterated matrix powers of $$Q= begin{pmatrix} 1 &amp; 1 1 &amp; 0 end{pmatrix} $$ generate the Fibonacci sequence. Specifically we have the identity $$ begin{pmatrix} f_{n+1} &amp; f_n f_n &amp; f_{n-1} end{pmatrix} = begin{pmatrix} 1 &amp; 1 1 &amp; 0 end{pmatrix}^n. $$ . This identity has alot of different explanations, but it suggests the following &quot;speedup&quot; for computing $f_n$ for $n geq 2$. For example, to compute $f(16)=f(2^4)$ we can evaluate $$Q, ~~~~~Q^2, ~~~~ Q^4=(Q^2)^2, ~~~~ Q^8=(Q^4)^2.$$ And this successive squaring of the previous result allows a $log_2(n)$ speedup of the Fibonacci computation. . Likewise $f(15)$ requires we compute all the powers $Q, Q^2, Q^4, Q^8$ and then take the product $$Q.Q^2.Q^4.Q^8=Q^{15}.$$ This represents somewhat the &quot;worst case&quot;. Notice that here we need to store the powers $Q^{2^k}$, $k&gt;0$, in memory and then multiply these powers together. So what precisely is the complexity? We find it curious how the complexity of matrix multiplication remains an open question in computer science. There are various speedups, although the naive complexity is $O(n^3)$ although speedups to $O(n^{2+ epsilon})$ are possible for some specific small values of $ epsilon$. We&#39;ll let $ tilde{m}$ represent the complexity of multiplication of two matrices. . So the total complexity is counted like: . Complexity of evaluating $log_2(n)$ matrix powers is $ tilde{m}. log_2(n)$ time steps. | Complexity of $log_2(n)$ matrix multiplications is $ tilde{m}. log_2(n)$ time steps. | Total Complexity is the sum of the complexities in 1. and 2., and therefore equal to $ tilde{m}. log_2(n)$, or $O(log_2n)$ | Are the methods of computing the fibonacci sequence really different? Obviously one appears to be faster, requiring nominally less time steps or elementary operations. However there are increasing memory constraints arising from the matrix form, having to store the powers $Q^{2^k}$, and where of course the entries are becoming super exponentially large. . So What? . Fair question. But as we said, this is the beginning of maybe something deeper. So we need to compare the two algorithms, and perhaps address whether the final $O(log_2n)$ is really irreducible in Wolfram&#39;s sense. . [To be continued ... -JHM] .",
            "url": "https://jhmartel.github.io/fp/2022/10/04/Fibonacci_Complexity.html",
            "relUrl": "/2022/10/04/Fibonacci_Complexity.html",
            "date": " • Oct 4, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "[Update] Weber Two Body Problem with Python and Gekko",
            "content": ". In previous posts we have been focused on modelling N-body systems under Weber&#39;s force law. Concretely we have taken $N=2,3$. This post focusses on $N=2$. The initial challenge of these simulations was finding a suitable DAE (Differential Algebraic Equation) solver. This is because Newton&#39;s 2nd Law and Weber&#39;s force lead to implicit ordinary differential equations (nonlinear, second order). We have found GEKKO to be suitable for our purposes, being easy to install, well maintained by Prof. Dr. Scott Hedengren, and easy to use. Of course GEKKO has limitations based on the input size. In our case even two particles involves 18 variables and the ODE system consists of 12 equations. This is somewhat demanding for usual computers to handle. . !pip install gekko . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: gekko in /usr/local/lib/python3.7/dist-packages (1.0.5) Requirement already satisfied: numpy&gt;=1.8 in /usr/local/lib/python3.7/dist-packages (from gekko) (1.21.6) . import numpy as np from gekko import GEKKO import matplotlib.pyplot as plt # main function defining the Weber force. def f(state): c=1.0 N=len(state) x = state aux = np.zeros((N,3)) aux = np.array(aux, dtype=object) for i in range(N): for j in range(N): if i==j: pass else: r=np.subtract(x[i][0:3], x[j][0:3]) # r_ij, vector. rho=np.dot(r,r)**(0.5) # rho = |r_ij|, scalar. rhat=np.dot(1/rho, r) # rhat, unit vector. factor_Coulomb = x[i][-1]*x[j][-1]*(rho**-2) # Coulomb factor dv = np.subtract(x[i][3:6], x[j][3:6]) # dv= v_{ij}, vector. r_prime=np.dot(rhat, dv) # r&#39; = rhat cdot dv, scalar. da = np.subtract(x[i][6:9], x[j][6:9]) #da = a_{ij}, vector s1 = (np.dot(dv, dv)- r_prime**2)*(c**-2) # scalar s2 = np.dot(r, da)*(c**-2) # scalar factor_Weber_1=(1-(0.5)*(r_prime/c)**2 + s1 + s2 ) # scalar aux[i]=aux[i]+np.dot(factor_Coulomb*factor_Weber_1, rhat) return aux #returns a force vector, i.e. three-vector. # returns the centre of mass of the initial state. def cm(state): xx = state N=len(state) M=np.zeros(N) state_cm=[0,0,0,0,0,0,0,0,0] for i in range(N): M[i] = np.abs((xx[i][-1])**-1) state_cm= np.add(state_cm, np.dot(M[i], xx[i][0:9])) return np.dot(state_cm, (sum(M))**-1) . # better results obtained when remote=True m = GEKKO(remote=True) # the following line needs be adjusted frequently # if m.solve() doesn&#39;t successfully terminate. m.time = np.linspace(0,15, 250) # time points #print(&quot;m.time returns: n&quot;, m.time) # create GEKKO variables x = m.Array(m.Var,(2,10)) # # initial positions x[0][0].value = +0.2 x[1][0].value = -0.2 #x.value[:][0:3]=terminal_state #x[2][2].value = +1.1 # initial velocities x[0][3].value=-0.6 x[0][5].value=0.1 x[1][3].value=+0.7 x[1][5].value=-0.1 # defining the charges and inertial mass x[0][-1].value=-1.0 # q0/m0 x[1][-1].value=-1.0 # q1/m1 #x[2][-1].value=+0.01 # q2/m2 print(&quot;The initial states are: n&quot;, x, &quot; n&quot;) # equations defining the ODE system eq0 = [ x[i][j+3].dt() == f(x)[i][j] for i in range(2) for j in range(3)] eq1 = [ x[i][j].dt() == x[i][j+3] for i in range(2) for j in range(3)] print(len(eq0+eq1)) m.Equation(eq0+eq1); . The initial states are: [[0.2 0 0 -0.6 0 0.1 0 0 0 -1.0] [-0.2 0 0 0.7 0 -0.1 0 0 0 -1.0]] 12 . Run gekko solver: . m.open_folder() m.options.IMODE = 6 m.solve(disp=False) . # the centre of mass position and velocity. s_init=[] N=2 # two body system. for i in range(N): s_init=s_init + [[x[i][j][0] for j in range(10)]] cm_s = cm(s_init) v_cm = cm_s[3:6] RR_cm = [np.add(np.dot(v_cm, time), cm_s[:3] ) for time in m.time ] Rt_cm=np.transpose(RR_cm) print(&quot;the initial state is:&quot;, s_init) ## returns initial state from solution. print(&quot;cm of initial state is:&quot;, cm_s) ## returns cm from initial state. This function is defined earlier. print(&quot;velocity of cm is:&quot;, v_cm) ## v_cm is constant with respect to time (verify with computation) . the initial state is: [[0.2, 0.0, 0.0, -0.6, 0.0, 0.1, 0.0, 0.0, 0.0, -1.0], [-0.2, 0.0, 0.0, 0.7, 0.0, -0.1, 0.0, 0.0, 0.0, -1.0]] cm of initial state is: [0. 0. 0. 0.05 0. 0. 0. 0. 0. ] velocity of cm is: [0.05 0. 0. ] . # and subtracting the centre of mass to prepare for the plotting. x0_cm = np.subtract(x[0][0], Rt_cm[0]) y0_cm = np.subtract(x[0][1], Rt_cm[1]) z0_cm = np.subtract(x[0][2], Rt_cm[2]) x1_cm = np.subtract(x[1][0], Rt_cm[0]) y1_cm = np.subtract(x[1][1], Rt_cm[1]) z1_cm = np.subtract(x[1][2], Rt_cm[2]) l = len(x[0][0]); vx0 = [x[0][3][j]-v_cm[0] for j in range(l) ] vy0 = [x[0][4][j]-v_cm[1] for j in range(l) ] vz0 = [x[0][5][j]-v_cm[2] for j in range(l) ] vx1 = [x[1][3][j]-v_cm[0] for j in range(l) ] vy1 = [x[1][4][j]-v_cm[1] for j in range(l) ] vz1 = [x[1][5][j]-v_cm[2] for j in range(l) ] # Creating new plot. fig = plt.figure(figsize=(30, 30)) ax = fig.gca(projection=&#39;3d&#39;) #ax0 = fig.add_subplot(111, projection=&#39;3d&#39;) #plt.gca().patch.set_facecolor(&#39;black&#39;) plt.plot(x0_cm, y0_cm, z0_cm, &quot;r:&quot;, label=&#39;x0(t) with -1 charge&#39;) plt.plot(x1_cm, y1_cm, z1_cm, &quot;g:&quot;, label=&#39;x1(t) with -1 charge&#39;) plt.plot(x[0][0], x[0][1], x[0][2], &quot;r&quot;) plt.plot(x[1][0], x[1][1], x[1][2], &quot;g&quot;) # Add Title plt.title(&quot;Two body system of identical negatively charged particles interacting with respect to Weber&#39;s electrodynamical force law.&quot;) plt.legend(loc=&#39;best&#39;) # Display plt.axis(&#39;on&#39;) ax.view_init(30, 20) plt.show() .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/09/13/Weber_TwoBody_Ctd.html",
            "relUrl": "/fastpages/jupyter/2022/09/13/Weber_TwoBody_Ctd.html",
            "date": " • Sep 13, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "[Under Construction] Weber Three Body Problem.",
            "content": ". Remark: . This article is not yet readable. We&#39;ve hit a technical difficulty which need to first solve, namely how to integrate ODEs of the form $x&#39;&#39;=f(x,x&#39;, x&#39;&#39;)$ on python. Typically odeint integrates equations of the form $x&#39;&#39;=f(x)$ or $x&#39;&#39;=f(x,x&#39;)$. But the Weber force involves interaction between all the particles their relative accelerations, so applying Newton&#39;s 2nd Law gives us equations of motion of the form $$m_i a_i = m_i v&#39;_i= sum_j F_{ij}$$ where the force depends on the acceleration of $a_i$ and the particles $a_j$. In the literature this is known as a &quot;Differential Algebraic Equation&quot; (DAE). So we&#39;ve been looking for python packages which have DAE solvers. It does not appear to be well-known subject. . Introduction: Weber&#39;s Force. . Our goal is to study the $N$-body problem on python according to Weber&#39;s electrodynamic law. In fact $N=3$ would be good start, and many author&#39;s have studied the $2$-body problem, even us in our first post on this blog. . The Weber potential belongs to relational mechanics, being given by a force $F_{12}$ between two charged particles satisfying Newton&#39;s action-reaction principle, so $F_{12}=-F_{21}$, and the force is also radial being along the straight line segment joining the centre of the particles. However what is further strange about Weber&#39;s potential is that the force $F$ depends on the relative positions, velocity, and accelerations of the particles! . So let us consider a three body system with particles having inertial masses $m_1, m_2, m_3$ and such that they have positions $r_1, r_2, r_3$ relative to the centre of mass $O=O_{CM}$ of the system. We will assume that the calculations are performed in the centre of mass frame. . The total force acting on particle $m_1$ is the sum $F_{12}+F_{13}$, where for example $$F_{1j}= frac{q_1 q_j}{r^3_{1j}} hat{ bf{r}}_ left( 1- frac^2}{2 c^2} + frac{r_{1j} r_{1j}&#39;&#39;}{c^2} right) $$ for $j=2,3$. . Now if we directly apply Newton&#39;s 2nd Law we get: $$F_{ text{net} 1}=F_{21}+F_{31}=m_1 frac{dv_1}{dt}.$$ Here we want to be extremely careful in interpreting these terms. The velocity $v_1$ is the velocity of the particle $m_1$ with respect to the centre-of-mass, i.e. we have $v_1=v_{1O}$. The difficulty with the second order differential equation obtained from Newton&#39;s 2nd Law is that $r_{12}&#39;&#39;$ is an acceleration term that has some component in common with $ frac{dv_1}{dt}$. The term $r_{12}&#39;&#39;$ is very different from $r_1$. Indeed we view $r_1$ and $dr_1/dt=v_1$ as vectors (a position and velocity vector). However the radial $r_{12}$ and $r_{12}&#39;$, $r_{12}&#39;&#39;$ are scalar-valued functions! . To use python&#39;s odeint solver, we either need to rewrite the expression defining Newton&#39;s 2nd Law, or we need to directly define an integration method which integrates the equation as is. . Formulas and Computation: . What are the terms that enter into the ODE? . In this setting a state has the form $$ {(x_i, v_i, m_i, q_i) }_i$$ for a finite collection of particles having position $x_i$, velocity $v_i$, and mass $m_i$ and electric charge $q_i$. In our computations it&#39;s convenient to separate the spatial variables $[x_i, v_i]$ from the mass charge $[m_i, q_i]$ for each $i^{th}$ particle in the system. . We suppose our system is isolated, and therefore the net force acting on the $i^{th}$ particle is the sum of the forces $F_{ij}$ over all indexes $j$. The pairwise particle force $F_{ij}$ is evaluated by Weber&#39;s force law. The basic notation is: . ${ bf{r}}_{ij} := r_i - r_j ~~~......... ~ text{vector}$ | $r_{ij}:=|r_i - r_j| ~~~......... ~ text{scalar}$ | $v_{ij}:=v_i - v_j ~~~......... ~ text{vector}$ | $a_{ij}:=a_i -a_j ~~~......... ~ text{vector}$ | $r&#39;_{ij}:= { hat{ bf{r}}}_{ij} cdot v_{ij} ~~~......... ~ text{scalar}$ | $r_{ij}~r&#39;&#39;_{ij}:={v_{ij} cdot v_{ij}} - ({ hat{r}}_{ij} cdot v_{ij})^2 +{ bf{r_{ij}}} cdot a_{ij}. ~~~......... ~ text{scalar}$ | . This last identity for $r&#39;&#39;_{ij}$ is important for our calculations. If we directly substitute this formula into Weber&#39;s force law and Newton&#39;s 2nd Law, then the net force on the $i^{th}$ particle yields: . $$m_i a_i= f_i(r_{ij}, v_{ij}) + sum_{j} frac{q_i q_j}{r_{ij}^2 c^2} ~ { hat{ bf{r}}}_{ij}~ ( r_{ij} cdot a_{ij}), $$where $f_i$ is defined as: . $$f_i( {r_{ij}, v_{ij} }):= sum_j frac{q_i q_j}{c^2 r^2_{ij}} { hat{ bf{r}}}_{ij} left( c^2- frac{3}{2}({ hat{ bf{r}}}_{ij} cdot v_{ij})^2 + |v_{ij}|^2 right) $$for $1 leq i , j leq 3$. . For every $i$, we want to isolate the acceleration terms $$m_i a_i - sum_{j} frac{q_i q_j}{c^2r_{ij}^2} ~ { hat{ bf{r}}}_{ij}~ ( r_{ij} cdot a_{ij}) $$ as much as possible, and this leads to our solving Implicit ODEs or so-called DAEs. Ideally we would avoid the DAE solvers as much as possible, and would prefer to have all acceleration terms $ {a_i }_i$ isolated. But it&#39;s not clear a priori whether this is possible. . Using the above formulas we rewrite $r&#39;_{ij} = hat{ bf{r}}_{ij} cdot v_{ij}$, and therefore everything in $f_i$ can be computed directly from $ { { hat{ bf{r}}}_{ij} }$ and $ {v_{ij} }$. . Moreover the acceleration term $r_{ij} cdot a_{ij} = r_{ij} cdot(a_i - a_j)$ does involve the vector $a_i$ directly, but only the image of $a_i$ under the projection. Thus we could write the Newton Law as $$m_i a_i = F_{ text{effective}}+ sum_j frac{q_iq_j}{r_{ij}c^2} { hat}}_{ij} (}}_{ij}} cdot a_i), $$ and where $F_{ text{effective}}$ is the effective force. (We&#39;re being somewhat vague here). . These are all the terms that enter into the calculation of Weber&#39;s force law. It&#39;s more complicated than Newton&#39;s Gravitational equations, which depends only on the relative positions $ {r_{ij} }$. Notice that $r_{ij}$, $r&#39;_{ij}$, $r&#39;&#39;_{ij}$ are scalar-valued, and the terms $r_i$, $v_i$, $v_{ij}$, and $a_{ij}$ are vector-valued. . Now the auxiliary function $f_i$ is ready-made for python&#39;s odeint. But the second right hand term is more interesting, as it involves the relative acceleration term directly, where $a_{ij}:=a_i - a_j$ by definition. We are looking to solve for $a_i$, for all $i$. The challenge is to: . decouple the equations into independant ODEs | try to rearrange the terms, see what symbolic manipulation can yield. This means writing it out in full (or in Wolfram Mathematica) and staring at it for a good while. Time for experimentation with the symbols. | Remark. An initial approximation might involve ignoring this second term. In fact we should rewrite our python code to reflect this direct factorization. Note that $r&#39;_{ij}$ is still present in our formula, but we could directly use formula $r&#39;_{ij}= hat{ bf{r}}_{ij} cdot v_{ij}$. .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/09/11/WeberThreeBody.html",
            "relUrl": "/fastpages/jupyter/2022/09/11/WeberThreeBody.html",
            "date": " • Sep 11, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "[Under Construction] Weber Sansbury's Structured Planetary Model of the Atom",
            "content": ". Here&#39;s a confession: our ultimate research goal is to complete Weber&#39;s proposal on the molecular structure of atoms according to his proposed electrodynamics. . This would mean classifying the semistable states of various $N$-body electrical systems. This was outlined by Wilhelm Weber in one of his final posthumously published memoirs. Insert reference. . We can numerically simulate these solutions using odeint on python. We fix the centre of mass of the system as the origin reference point, and can integrate the equations of motion using Weber&#39;s force law. This is complicated $N$-body system, so what can we reasonably expect to prove ? . When we write $-2=-1-1$ we mean considering $-2$ as a two-body system which is stable, i.e. two identicaly negative charges have been forced through the critical radius and stably attracted to each other. . We should classify the $(-2) + (+1)$ system, which consists of three particles. In other words, does the system collapse? Or is it a stable planetary type system, i.e. the two-body system $(-2)$ rotates with precession about the $(+1)$ charge. This would require a simulation. | Classifying the stable orbits of the above three-body system, which has net charge $-1$ (!) would be very interesting starting point. In Ralph Sansbury&#39;s model, the above system represents the electron, and in this sense Sansbury speaks of the structured electron. The question which needs be considered is what is the internal potential energy of the above system, i.e. at what energies can it be separated ? Because we do not typically see the electron disintegrating into smaller subsystems. But it&#39;s possible that the system has an extremely large cohesion energy, and that if the system ever degenerated, then particles travelling at velocities greater than $c$ would be observed. And this possibility is not ruled out a priori by the Weber electrodynamics. . There is an obvious multiplicity in the possible configurations once you allow the possibility of the electron as being structured of smaller systems. And this is where we need simulation, because I&#39;m not sure we can work out the theory a priori. That is, the energy landscape of the Weber potential is not well known for many-body systems, and simulation is a good way to start learning. . For example the four-body systems with zero net charge could have various configurations. Likewise the six body systems with zero net charge, etc.. . Here we implicitly assume a zero net charge to represent the stable molecular atoms. This implies an even number of particles. However we do not require the inertial masses $m_+$ and $m_-$ to be equal a priori. And in practice we find the proton significantly heavier than the conventional electron. In light of Sansbury&#39;s proposal, we need a heuristic to find the order of magnitude of the indivisible negative charges $(-1)$ and positive charges $(+1)$. . There is very interesting idea of Ralph Sansbury which calls &quot;metastable orbits&quot;. This is Sansbury&#39;s explanation for the apparent nonradiation of the hydrogen atom. In fact, there is radiation between the subsystems, but there is zero net radiation into the environment! That&#39;s Sansbury&#39;s brilliant idea. (He has others.) . . .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/09/04/WeberPlanetaryModel.html",
            "relUrl": "/fastpages/jupyter/2022/09/04/WeberPlanetaryModel.html",
            "date": " • Sep 4, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "E=mc^2 but what does it mean?",
            "content": ". Ok we apparently all know that $E=mc^2$ is one of the great achievements of Albert Einstein. He first published the idea in 1905 in a paper called &quot;Does the inertia of a body depend upon its energy content?&quot; . But what does the formula mean? . Supposedly it&#39;s one of the greatest formulas in history, yet what is the content of this supposed formula? . We give the reader a few minutes to consider what they understand about this formula. For example, what can you say about it? . The standard explanations are not so useful. They say that a body of mass possesses some intrinsic energy which is independant of the observer frame. And this intrinsic energy is computed -- according to Einstein&#39;s suggestion -- as $m_0c^2$, where $m=m_0$ is the inertial rest mass of the object. . Now here enters another idea, that the inertial mass of an object increases with its velocity, i.e. we have $E=mc^2 = beta m_0 c^2$ where $ beta$ is one of the Lorentz-type beta factor arising so frequently in special relativity, that is $ beta = 1 / sqrt{1-v^2/c^2}$. . Now look, for most students in university, or even adults, this is the end of the story. You can take physics in undergraduate college or university, and I think this is the standard treatment. . You can ask &quot;Why is the formula true?&quot;, and the teacher might say &quot;You won&#39;t need to prove it on the exam, don&#39;t worry about it!&quot;, or &quot;Well, I don&#39;t know, nobody ever asked me, but I guess you can read Einstein&#39;s work&quot;, or &quot;I don&#39;t know, but look at wikipedia.&quot; . And reading Einstein&#39;s original paper is a good idea, although it won&#39;t likely be satisfying. Another good reference is Levi-Civita&#39;s &quot;Absolute Differential Calculus&quot; textbook which has very detailed mathematical review of Einstein&#39;s &quot;physical principles&quot;. . In Levi-Civita&#39;s textbook, the formula is derived via an argument using Hamilton&#39;s principle. It&#39;s something like this: we start with the usual Lagrangian $L$, and find the equations of motion are $ delta int L = 0$, where $ delta int$ is the variational derivative of the functional $ alpha mapsto int_ alpha L$. (Here $ alpha$ denotes a path, not the earlier Lorentz gamma factor.) . Actually in Einstein and Levi-Civita&#39;s approach, which is based on Hamilton principle. The starting point is a Lagrangian $L$ defined on the statespace, and then with Hamiltonian $H=L^*$. The Hamiltonian principle says $$ delta int L=0.$$ But Einstein introduces the variational equation $$ delta int c^2 = 0.$$ And he says this equation is trivial like $$ delta int dt =0 .$$ . So Einstein introduces the Lagrangian $c^2 - L$ and the Hamilton principle becomes $$ delta int (c^2 - L) = 0.$$ . This apparently trivial modification of the Lagrangian is what introduces (imports) the constant $c^2$ energy term in the Hamiltonian. . We omit the details from Levi-Civita, but the main formula obtained is $$H^*=c^2 - L = c^2 sqrt{ 1- beta -2U/c } = c^2 + v^2/2 - U,$$ since $L = v^2/2 + U$. Einstein remarks that if $v=0$ and $U=0$, i.e. if the potential energy and kinetic energy vanish, then there still remains an intrinsic energy represented by $c^2$, i.e. the Hamiltonian $H^*$ does not vanish. . In the above argument, we remark that it&#39;s not always clear that $U=0$ is meaningful or nonarbitrary. As well known, it&#39;s the gradient of the potential $ nabla U$ which enters into the dynamical equations, and not the scalar values of $U$. So on this point we are not entirely persuaded that $v=0, U=0$ represents an intrinsic energy. . So in summary, what&#39;s the story behind this amazing formula $E=mc^2$ ? In the above argument, there is no story except trivial mathematics. There is no physics! . Levi-Civita makes this same remark in his textbook, but points a posteriori to radioactive substances. . So Where&#39;s the Physics in E=mc^2 ? . Wilhelm Weber (1804 -- 1891) was a great physicist mathematician who succeeded C.F. Gauss as directory of the Gottingen Observatory. Weber was the first physicist to define $c$, with Kirchoff using Weber&#39;s force equations, as the velocity of electrical impulses in a material wire. . In his works on particle electrodynamics, he discovered in 1860s an amazing physical model of $E= mu c^2$ where $ mu$ is the reduced mass of the system of electric particles. We have written on this amazing fact in a previous post &quot;Weber&#39;s Critical Radius, Work, and E=mc2 Formulas&quot; . We repeat the fundamental physical idea (and this is what is totally absent in Einstein&#39;s derivation). . Consider two equal and identical unit electric charges $q_1$ and $q_2$ which are separated by some relative distance $r&gt;0$. The Couloumbian motto is that &quot;like charges repel, and opposite charges attract&quot;. So a force is required to push the charges $q_1$ and $q_2$ closer together. In otherwords, there is work that needs be done to push the identical charges $q_1, q_2$ together. . What Weber discovererd in 1860s (maybe earlier) was that there is a critical distance, where if a sufficient amount of work is performed, and the charges $q_1, q_2$ become sufficiently close $r&lt; r_c $ (passing through Weber&#39;s critical radius $r_c$) then there is a sign in Weber&#39;s force law which predicts that the equal charged particles become attractive within the critical radius $r&lt; r_c$. In otherwords it&#39;s possible for two-body system of net charge $-2$ $(=-1 -1)$ to be a stable system with large potential energy! There is a large amount of potential energy within this system because alot of work was performed to push the particles through the critical radius. . More specifically, Weber&#39;s force predicts that the amount of work required to push the particles through the critical radius is $E= mu c^2$, where $ mu$ is the reduced mass of the system. . This is amazing fact! This is a physical explanation of what the formula means. The formula represents an amount of work which has been invested into the system. Moreover, Weber&#39;s formula predicts that this potential energy is stored in surprisingly stable many body systems, e.g. two-body systems of the form $-2=-1-1$. . I don&#39;t know if the reader can appreciate how EPIC this idea of Weber&#39;s is. It&#39;s the beginning of something incredible, basically returning physics to classical paradigm, and appreciating Wilhelm Weber&#39;s tremendous contributions. Here I have been much influenced by AKT Assis&#39; recent translation into English (and Spanish, Portugese) of Wilhelm Weber&#39;s collected works (in four volumes). . .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/09/03/mc2_part2.html",
            "relUrl": "/fastpages/jupyter/2022/09/03/mc2_part2.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "Remarks on Craig Alan Feinstein's papers and P versus NP.",
            "content": ". P versus NP has long complicated backstory, see Stephen Cook&#39;s official statement of the problem for the Clay Millenium Problems. https://www.claymath.org/millennium-problems/p-vs-np-problem . We have also recently been reviewing Craig Alan Feinstein&#39;s papers on the problem, especially his article https://arxiv.org/abs/1605.08639. . Now must record that Feinstein&#39;s work is somewhat controversial, although we find it quite interesting. For example here is a critique of one of Feinstein&#39;s earlier papers, notice the year 2007, https://arxiv.org/abs/0706.2035. Let us remark that controversy is very important for science, and we strongly believe in the need for creative controversy. So we follow the debate with much interest. . [Insert description of Subset-Sum Problem over an ordered set of length N] . [Insert Feinstein&#39;s argument to reduce the complexity to $O(2^{N/2})$] . Here Feinstein claims (and his critics argue that he asserts without proof) that $O(2^{N/2})$ is a lower bound for the worst-case complexity of the SUBSET-SUM problem. The problem however is that computer science and complexity ``theorists&quot; have no strategies for proving lower bounds! . So it appears to us that if $P neq NP$ (as is widely expected), then it&#39;s formally unprovable (because of the unprovability of lower bounds). In other words, as Feinstein argues, the critics can always say &quot;Well what if one day an algorithm is discovered by some new method that is much faster $ ldots$.&quot; And what&#39;s the answer to such a &quot;what if?&quot; statement? . If we review S. Cook&#39;s statement of the P vs NP, we see very few references to lower bounds, and it&#39;s indeed the central problem in the area. . There is another comment to make regarding the effect of so-called quantum computers and quantum algorithms. With QCs, complexity is measured by the number of quantum gates in the circuit realizing the algorithm. But is the complexity in QC really comparable to complexity in classical algorithms? . Firstly, the quantum algorithms are not deterministic, i.e. they have a nontrivial probability of giving the wrong answer as output. For example, Grover&#39;s search algorithm is quite famous (almost as famous as Schor&#39;s factorization algorithm), and given a function $f$ on an unstructured set of $N$ elements, finds the input which satisfies a given output in $O( sqrt{N})$ evaluations. We are being vague here, but the wikipedia article on Lov Grover and his algorithm has useful links: . e.g. https://arxiv.org/abs/quant-ph/0109116 and https://en.wikipedia.org/wiki/Grover&#39;s_algorithm . . So we can finish this article here. What&#39;s our main point? That IF $P neq NP$, then there will never be a proof, because lower bounds on complexity are not possible. Critics will always say there is a possibility of a magic algorithm that speeds up the process. And the confusion being imported by quantum computers encourages the magical optimism, &quot;Maybe there will even be a quantum algorithm tomorrow which is much faster than anything known!&quot; . So meanwhile we focus on other problems and would not recommend anybody work on P versus NP. .",
            "url": "https://jhmartel.github.io/fp/p%20vs%20np/subset%20sum/complexity/worst%20case/algorithms/caf/2022/09/03/PversusNP.html",
            "relUrl": "/p%20vs%20np/subset%20sum/complexity/worst%20case/algorithms/caf/2022/09/03/PversusNP.html",
            "date": " • Sep 3, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "An Idea on Constructing Counterexamples to Larry Guth's Sponge Problem using Jammed Packings.",
            "content": ". In this post I&#39;m simply calling attention to a possible counterexample to Larry Guth&#39;s Sponge Problem. I have other interests at the moment, so I know I won&#39;t be working on this problem for awhile and I leave my notes here in case somebody else wants to complete it. This same idea, in even less developed form, was posted on Mathoverflow here: https://mathoverflow.net/questions/381172/status-of-larry-guths-sponge-problem. . See also: https://www.github.com/jhmartel/Sponge/ . The idea is how to use jammed disk packings to construct counterexamples to Larry Guth&#39;s Sponge Problem. These will be packings which have a large linear diameter but have arbitrarily small volume (i.e. arbitrarily low &quot;density&quot; in the terminology of disk packings). The most interesting constructions of arbitrarily small density packings are constructed by Werner Fischer (see references in the above Mathoverflow post). Our idea is that from these packings we can construct open subsets $V$ with arbitarily small volume and which do not admit any expanding-embeddings into the unit disk. . A negative answer to Guth&#39;s Sponge Problem implies that Guth&#39;s proof of the width-volume inequalities [ref] cannot be simplified or deduced from width-volume inequalities on the unit disk. In otherwords, Guth&#39;s very interesting proof [ref] remains also a necessary proof. . The above idea is not rock solid. What we have not proven is the following statement: . Unproven Lemma: Let $P$ be a disk packing of an open subset $U$ of ${ bf{R}}^n$. If $P$ is rigid relative to $U$, then $diam(f(U)) geq diam(U)$ for all expanding-embedding maps $f: U to { bf{R}}^n$. . Here $diam$ refers to the linear diameter of $U$ as a subset of ${ bf{R}}^n$. We say $P$ is rigid relative to $U$ if for every force load $b$, the packing cannot be rearranged without some outward-expansion on the boundary $ partial U$. . So the rough idea is that : force loads $b$ cause a reaction in the packing, and what&#39;s interesting is whether the load forces the domain $U$ containing $P$ to increase in volume. . However what can possibly happen is that the force load $b$ causes an expansive rearrangement of the disks in the packing, and where the volume of $U$ increases, but potentially the disks can then be rearranged in a configuration with smaller linear diameter. . Perhaps the correct lemma is this: If $P$ is a disk packing in $U$, and $P$ is rigid relative to $U$, and if $U$ has minimal possible linear diameter containing $P$ (this implies $U$ is a subset of the smallest disk containing $P$), then $diam(f(U)) geq diam(U)$ for every expanding-embedding $f$ of $U$. . What we are seeking is some sort of monotonicity property. However it&#39;s possible that monotonicity can hold only for limited types of open subsets, for example those $U$ containing the packing $P$ and the minimal disk containing $P$. (I know there&#39;s alot of definitions here, some implicit, and I&#39;m rushing.) . These ideas are related to the recent results that &quot;Dionysian packings of arbitrarily small density can be mechanically rigid&quot; [ref]. [https://www.youtube.com/watch?v=M62Fewh-dHw] .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/08/31/GuthSpongeCounterexamplesViaPackings.html",
            "relUrl": "/fastpages/jupyter/2022/08/31/GuthSpongeCounterexamplesViaPackings.html",
            "date": " • Aug 31, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "Preprint on Closing Steinberg Symbols in Mapping Class Group with Applications to Constructing Spines.",
            "content": ". Today we announce that our preprint on &quot;Closing Steinberg Symbols in Mapping Class Groups&quot; has achieved a basically stable form. The article is available at [https://www.github.com/jhmartel/MCG]. . There is a proof missing in one of the lemmas, but otherwise the paper can be read as is. We don&#39;t solve all the problems we wanted to, and we cannot yet produce an explicit spine for the genus two mapping class group, for example, but we&#39;ve written what we can. . Comments are welcome at jhmartel {at} proton {dot} me. .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/08/30/MCG_CS_Article_Available.html",
            "relUrl": "/fastpages/jupyter/2022/08/30/MCG_CS_Article_Available.html",
            "date": " • Aug 30, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "Summer 2022 is over. That was fast.",
            "content": ". Wow, I&#39;ve been busy in real world and have paused some writing projects. Of course there is still ALOT to be said. But I&#39;m searching for opportunities, and have travelled to Calgary, AB, to see what&#39;s happening here. Again, I am travelling to make a decision about where to reside. . What&#39;s been on my mind? . The Qiskit and quantum computing has reached the following stage: the mathematical theory is perfect but incomplete, unsatisfactory, and the physical interpretation is not sufficient. [Insert comment: there is tensorial categorical issue with the quantum definitions of observables and projections of variables, e.g. the Einstein-Podolsky-Rosen (EPR) paradox and interpretations. This tensorial definition is provided by Gromov in his ERGO brain papers and specifically his paper &quot;In Search of Structure Part 1: Entropy&quot;.] | Again there is alot to say. . The further logical incompleteness of quantum is reflected in Neil Bohr&#39;s famous proclamation that &quot;Those who are not shocked when they first come across quantum theory cannot possibly have understood it.&quot; . Or Richard Feynman &quot;If you think you understand quantum mechanics, you don’t understand quantum mechanics. &quot; . Or Werner Heisenberg&#39;s &quot;Not only is the Universe stranger than we think, it is stranger than we can think.&quot; . Now let me say that these statements are True, but they are also True evidence that Quantum is unacceptable. It is admittedly inconsistent in its very foundations, at its root, genesis, and zero block. So why is quantum accepted at all? Because there is something useful about it. So let us continue to be guided by pragmatism. What&#39;s useful? . Another remark on Bohr&#39;s claim. There is something shocking in quantum physics, and this is well documented in the historical accounts of Bohr, Planck, and Einstein. And what is the shock? It is found in Bohr&#39;s planetary two-body model of the hydrogen atom which violates classical physics. More specifically, the apparent non-radiation of Bohr&#39;s planetary model of the hydrogen atom is the shocking postulate. But the apparent nonradiation is an ad hoc postulate, and which is contradictory to the classical dynamics. Quantum is shocking for those who try to explain the atom via classical electrodynamics, e.g. the Maxwell field theory and Lorentz force laws. However as Weber and Gauss understood from Ampere&#39;s Newtonian theory of electricity, there are many more dynamic and stable configurations in Weber&#39;s relational action-at-a-distance model than the standard Lorentz-Maxwell field framework. . So we view Niels Bohr&#39;s above statement as True. Indeed for the student to accept the mathematical premise of Bohr&#39;s quantum theory is to positively reject the classical physics, because Bohr does not resolve the problem of the apparently non-radiating hydrogen atom . Rather he takes this apparent non-radiation as a fundamental postulate, regardless of its apparent irreconciliability with the basic classical theory of two-body electronic systems. . What is the alternative? . The alternative was proposed by Wilhelm Weber before Planck, and before Bohr. The Weber planetary model of the atom is stable, and does resolve the inherent contradictions of Bohr&#39;s model. In otherwords, Weber restores the planetary atom to classical physics. . Update: I&#39;ve been looking for work since around November 2021. Today is June 2022, so that makes approximately seven months unemployed. I work labour jobs on the side, but that&#39;s just to make ends meet. I will say I consider it quite demoralizing and humiliating to be ready, willing, and able to work a job, and yet having no job. I&#39;m sorry my excuse is I&#39;m too talented and too skilled and too good to be working labour like a beast. The consequences of this choice are this: instead of readily earning 32$/hour as entry level wage, I&#39;m poor and effectively homeless. But I&#39;m a dreamer, and i&#39;m called to do physics and mathematics, and I&#39;ll do it until I die. . I have recently updated my special relativity paper titled &quot;On Critical Foundations of SR and Light Propagation in Vacuum&quot;. I actually think the paper is quite readable at this stage. The main idea is simply a modification of Fizeau&#39;s spinning wheel experiment following a suggestion of Ralph Sansbury which -- at least to my mind -- would be decisive in demonstrating that &quot;light&quot; is really something that &quot;travel through space&quot;. | Remark. In Maxwell&#39;s treatise, especially the final chapters of Vol II, and his review of action-at-a-distance theories of Gauss, Weber, Neumann, etc., Maxwell is clear that it&#39;s inconceivable for him to imagine light as anything other than a &quot;particle&quot; which necessarily travels through space. This presumption then leads to the confused issue of whether light is particle or wave or somehow both? (I.e. Bohr complementarity). . My new goal is to acquire the IBM Qiskit Developer Accreditation. This involves a 60 question multiple choice exam in August. . Why? . Qiskit and Quantum Computing (QC) seems like perfect opportunity to prove both my math and programming skills. Honestly I don&#39;t think anybody can understand Qiskit or the quantum gates without strong linear algebra. . | Qiskit and especially Qiskit Pulse are excellent testing grounds for fundamental physics. Indeed my hypothesis is that decoherence and quantum errors are going to remain a persistent problem in QC. It would be amazing to actually derive/prove the persistence of these errors via Weber&#39;s electrodynamics. . |",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/08/29/LongLostUpdate.html",
            "relUrl": "/fastpages/jupyter/2022/08/29/LongLostUpdate.html",
            "date": " • Aug 29, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Remarks on Ahlfor's Measure Conjecture.",
            "content": ". A recent MO post has prompted us to express something about our views regarding $ Gamma$ actions and $ Gamma$-invariant Borel measures and proper discontinuous actions, $E Gamma$ spaces, etc.. The conjecture is rather famous, especially in three dimensions. But since our PhD thesis (2019) we have some nonstandard approaches to the problem, which we will elaborate below. . It&#39;s not easy to describe the current state of the conjecture, depending on several implications and reductions to other conjectures, especially in three-dimensions. A good reference is here. . The above MO question is interesting in that it&#39;s asking about the corresponding conjecture in higher dimensions. I sent Alex Kolpakov a message asking him about the question because he&#39;s the person who I know knows the most about four-dimensional manifolds. . The setting for Ahlfor&#39;s problem begins with a discrete finitely generated group $ Gamma &lt; Isom( bf{H}^{n+1})$ of some hyperbolic space. The discreteness of the group representation implies that $ Gamma$ acts proper discontinuously on the hyperbolic space. The hyperbolic space also has a canonical volume measure. We say the representation has finite covolume if the quotient $ bf{H} / Gamma$ has finite volume. In otherwords $ Gamma$ represents the fundamental group of a hyperbolic orbifold $X:= bf{H}^{n+1} / Gamma$, and the action $$ Gamma times bf{H}^{n+1} to bf{H}^{n+1}$$ is the universal covering action $$ pi_1 times tilde{X} to tilde{X} $$ where $ tilde{X} approx bf{H}^{n+1}$. . Now given an action of $ Gamma$ on $ bf{H}$ we next need to define the topology of the &quot;boundary at-infinity&quot;. . To review: geodesics in hyperbolic space $ bf{H}^{n+1}$ diverge exponentially in every direction. The hyperbolic space is expansive at-infinity. And even though the Poincare disk might naively appear to be a small bounded disk, this disk in fact contains an infinitely large expansive world. . . We need carefully define the topology to properly define the limit points $L_ Gamma$ of a $ Gamma$-orbit $ Gamma.pt$. In fact some confusion arises because the topology on the visual sphere at-infinity is not always made explicit by authors. Naively the majority definition of the topology on the visual sphere consists of open subsets which are presumed to contain ideal points $ xi$ at infinity. This perspective implicitly assumes that the boundary disk is the natural compactification of the hyperbolic space $ bf{H}$. . But actually the correct definition of these ideal points is via horoballs and the horofunctions. A point at-infinity $ xi$ really corresponds to a $1$-Lipschitz function $b_ xi: bf{H} to bf{R}$, and specifically the pole at negative infinity $- infty$ of $b_ xi$. The level sets of the horofunction $b_ xi$ are horospheres centred at $ xi$. . So we have two definitions of the points at-infinity, and two different topologies. For example: . Horofunction definition of limit points at-infinity: the point $ xi$ is a limit point of the orbit $ Gamma .pt$ with respect to the horoball topology if the values of the orbit $b_ xi( gamma.pt)$, $ gamma in Gamma$, accumulate near the negative infinity pole. I.e. iff there exists an infinite subsequence such that $b_ xi( gamma.pt)$ diverges to $- infty$. . | &quot;Open&quot; topology definition of limit points: the visual point $ xi&#39; in partial_ infty bf{H}$ is a limit point of the orbit $ Gamma .pt$ w.r.t. the open topology if every open neighborhood $U$ of $ xi&#39;$ in $ bf{H} cup partial_ infty bf{H}$ has a non trivial intersection with the orbit $U cap Gamma.pt neq emptyset$. . | The problem with the second definition, which admittedly the majority topology in the literature, is that the point at-infinity has not been defined, and no criteria is given for testing whether the &quot;point&quot; $ xi&#39;$ is contained or not contained in the open subset $U$ of $ bf{H}$. . Therefore in the horofunction topology, the ideal points at-infinity are not &quot;points on a sphere&quot; they are instead $1$-Lipschitz functions obtained as a natural limit of the distance functions $d(x,y)$. The point at infinity $ xi$ is instead the negative infinity pole of the corresponding horofunction $b_ xi$. . Naively one says there is a circle at infinity, namely the visual sphere $ partial_ infty bf{H} approx bf{S}^1$. With respect to the hyperbolic metric, of course the boundary is infinitely far away. Now we need to topologize the visual boundary such that the $ Gamma$-action on $ bf{H}$ extends to a continuous topological action on $ bf{H} cup partial bf{H}$. . One method of topologizing the boundary is via the visual angle metric. Another interesting topology on the boundary is the Busemann topology. The key fact necessary is that the topology of $ partial_ infty$ needs be defined such that the group $ Gamma$ acts continuously on $ bf{H} cup partial_ infty bf{H}$. . [Edit] If we fix a basepoint $pt$ in $ bf{H}$, then one naturally defines the boundary at-infinity $ partial_ infty bf{H}$. The boundary at-infinity is topologized with the visual angle metric, and it&#39;s a standard fact that the boundary at-infinity is a topological sphere $ partial_ infty bf{H}^{n+1} approx bf{S}^n$. The sphere at-infinity does not have a $ Gamma$-invariant metric since $ Gamma$ actions by translating the basepoint $pt$. It is a standard fact that the sphere at-infinity has a well-defined $ Gamma$-invariant metric modulo quasi-isometries. The definition of quasi-isometries will not be further developed in this article, except to say this: that the topological sphere at-infinity is a difficult object to study with respect to the $ Gamma$ action. . Standard fact: Under assumptions [insert] the topological action $ Gamma times bf{S}^n to bf{S}^n$ is ergodic with respect to the class of $n$-dimensional Lebesgue measures on $ bf{S}^n$. In other words, if $ lambda$ is the $n$-dimensional Lebesgue measure on the topological sphere at-infinity $ bf{S}^n$, then the only $ lambda$-measurable $ Gamma$-invariant functions $f: bf{S}^n to bf{R}$ are constant a.e.. [Insert reference to Mostow&#39;s proof, or Thurston&#39;s book]. Roughly speaking, the topological $ Gamma$-action on $ bf{S}^n$ is very complicated, acting with sources and sinks and strange dynamics which are not measured by the $n$-dimensional Lebesge measure $ lambda$. . To finally state Ahlfor&#39;s measure conjecture we need to define the limit set of a $ Gamma$-orbit. Informally we can define $L_ Gamma subset partial_ infty bf{H}$ as the accumulation points of the orbit $ Gamma. pt$. . Ahlfor&#39;s Measure Conjecture: If $ Gamma, X$ satisfy the assumptions (i) and (ii), namely having finitely many ends and finite volume, then the limit set $L_ Gamma$ is either a zero or full measure subset of $ partial_ infty tilde{X}$. In otherwords, the limit set $L_ Gamma$ is either equal to $ bf{S}^n$ or has vanishing $n$-dimensional measure. . In most circumstances, the action of $ Gamma$ on the visual sphere is ergodic with respect to the natural Lebesgue measure on the sphere. However the example in the above MO post shows the action is not necessarily ergodic in higher dimensions for certain groups. However in this counter-example to ergodicity the group $ Gamma$ is finitely-generated but not finitely-presentable. . Now we must admit that the structure of the limit points $L_ Gamma$ is very difficult to comprehend. Many experts have been involved in producing graphics and simulations of these limit sets, especially for Kleinian and Fuchsian groups. Mumford&#39;s book &quot;Indra&#39;s Pearls&quot; is also relevant here. . Now our approach to Ahlfor&#39;s Measure Conjecture is to first realize that the visual boundary is not the correct boundary for the group action $ Gamma times bf{H} to bf{H}$. We view the visual boundary as extremely unnatural with respect to the group $ Gamma$. Specifically, the problem is that the $ Gamma$ action is not proper discontinuous on the visual sphere. This means that point orbits on the sphere are not well separated by the $ Gamma$ action. In otherwords, the visual boundary is very confusing because group elements which otherwise are very distinct can have similar actions on the visual boundary, and their distinct identities become confused. . # generated by the letters, a, A, b, B. Eventually we use a dictionary to convert these # letters and words into numerical matrices. import numpy as np #from numpy.linalg import matrix_power gen=[&#39;a&#39;,&#39;A&#39;,&#39;b&#39;,&#39;B&#39;] inverse_list=[[&#39;a&#39;,&#39;A&#39;], [&#39;A&#39;, &#39;a&#39;], [&#39;b&#39;, &#39;B&#39;], [&#39;B&#39;, &#39;b&#39;]] def test(x): a, b = x if (x in inverse_list): return False else: return True def f(x): l=[] level, word_list, tail = x new_level=level+1 for letter in gen: if test([tail, letter])==True: new_word_list=word_list+[letter] new_tail=letter l=l+[[new_level, new_word_list, new_tail]] else: pass return l print(f([1,[&#39;B&#39;, &#39;a&#39;],&#39;a&#39; ])) # Below we print the ends. seed=[0, [], &#39;&#39;]; def ends(x): m, seed = x sample_list=[seed]; for n in range(m): new_list=[] for x in sample_list: new_list=new_list+f(x) sample_list=new_list return [len(sample_list), sample_list] print(ends([4, seed])) . [[2, [&#39;B&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [2, [&#39;B&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [2, [&#39;B&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;]] [108, [[4, [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;a&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;a&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;A&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;a&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;b&#39;, &#39;b&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;a&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;b&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;b&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;b&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;A&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;a&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;a&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;a&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;A&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;A&#39;, &#39;b&#39;], &#39;b&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;A&#39;, &#39;B&#39;], &#39;B&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;B&#39;, &#39;a&#39;], &#39;a&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;B&#39;, &#39;A&#39;], &#39;A&#39;], [4, [&#39;B&#39;, &#39;B&#39;, &#39;B&#39;, &#39;B&#39;], &#39;B&#39;]]] . # into a matrix product. We evaluate these products over the ends of the tree # generated by the words in the letters a, A, b, B. import numpy as np from numpy.linalg import matrix_power as mp # S is a typical pseudo-anosov type transformation # T was randomly chosen, and might not be a good example. S=np.array([[2,1], [1,0]]); T=np.array([[-1,-1], [2,1]]); Id=np.array([[1.0,0.0], [0.0,1.0]]); # matrix_power is abbreviated mp. data={&#39;a&#39; : S, &#39;A&#39; : mp(S, -1), &#39;b&#39; : T, &#39;B&#39; : mp(T, -1) }; def bracket(w): x, y = w return (x@y)@(mp(x,-1)@mp(y, -1)) ## the @ symbol represents matrix multiplication, i.e. np.matmul() print(&quot;The commutator of S and T is:&quot;, bracket([S,T])); print(&quot;The commutator of S and S is trivial:&quot;, bracket([S,S])) # matmul is the numpy function for matrix multiplication. def product(word_list): m=Id; for word in word_list: m=data[word]@m; return m sample_word_list=[&#39;a&#39;, &#39;B&#39;, &#39;A&#39;, &#39;b&#39;]; swl1=[&#39;a&#39;, &#39;b&#39;, &#39;A&#39;, &#39;b&#39;] # print(product(swl1)); # print(product(sample_word_list)) . The commutator of S and T is: [[-5. -3.] [-3. -2.]] The commutator of S and S is trivial: [[1. 0.] [0. 1.]] . e=ends([5, seed]); max_index=e[0]; matrix_list=[]; def maxFunc(x): return np.abs(x).max(); for n in range(max_index): matrix_list=matrix_list+[product(e[1][n][1])] matrix_list.sort(key=maxFunc, reverse = True) def rpl(x): return (maxFunc(x)**(-1))*x def quad(x): return np.matmul(x.transpose(), x) # looks at the first largest elements. We should keep those terms which # are of similar order of magnitude as det(rpl(x)). for x in matrix_list[0:10]: print(&quot;The matrix product is:&quot;, x); # print(&quot;The sup norm of the matrix product is:&quot;, maxFunc(x)); # print(&quot;Renormalized projectivization:&quot;,rpl(x)); # print(&quot;The determinant of the renormalized projectivization is:&quot;, np.linalg.det(rpl(x))); print(&quot;The log-absolute-determinant of the renormalized projectivization is:&quot;, np.log( np.abs( np.linalg.det(rpl(x)) ) ) ); print(&quot;The quadratic form Transpose(x)*x of x=rpl(x) is:&quot;, quad(rpl(x)) ); eigen=np.linalg.eig(quad(rpl(x))) # print(&quot;The eigendistribution: is:&quot;, eigen); print(&quot;The log-absolute value of the near zero eigenvalue is:&quot;, np.log(np.abs( eigen[0].min() )) ) min_index=eigen[0].argmin() max_index=eigen[0].argmax() print(&quot;The approximate limit point is:&quot;, eigen[1][max_index] ); # print(&quot;The rank-one quadratic form represented by the limit point is:&quot;, (eigen[1][max_index])@(eigen[1][max_index].transpose()) ) print(&#39; n&#39;) ## N.B. the limit point is the eigenvector corresponding to the nonzero eigenvalue. ## we would like to retain only those elements such that floor(log|det(x)|) is maximized. ## this means considering only those words whose renormalized projectivizations are of the same ## order of magnitude, i.e. of the same maximal &quot;dimension&quot;. ## TO DO: quantify the uncertainty in the approximate limit point. . The matrix product is: [[ 29. 12.] [-75. -31.]] The log-absolute-determinant of the renormalized projectivization is: -8.634976227072784 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.14951111 0.4752 ] [0.4752 0.19644444]] The log-absolute value of the near zero eigenvalue is: -17.567056648013256 The approximate limit point is: [ 0.92414737 -0.38203618] The matrix product is: [[-29. -12.] [ 75. 31.]] The log-absolute-determinant of the renormalized projectivization is: -8.634976227072784 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.14951111 0.4752 ] [0.4752 0.19644444]] The log-absolute value of the near zero eigenvalue is: -17.567056648013256 The approximate limit point is: [ 0.92414737 -0.38203618] The matrix product is: [[ 31. 12.] [-75. -29.]] The log-absolute-determinant of the renormalized projectivization is: -8.634976227072784 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17084444 0.4528 ] [0.4528 0.17511111]] The log-absolute value of the near zero eigenvalue is: -17.567056648013256 The approximate limit point is: [ 0.93268339 -0.36069612] The matrix product is: [[-31. -12.] [ 75. 29.]] The log-absolute-determinant of the renormalized projectivization is: -8.634976227072784 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17084444 0.4528 ] [0.4528 0.17511111]] The log-absolute value of the near zero eigenvalue is: -17.567056648013256 The approximate limit point is: [ 0.93268339 -0.36069612] The matrix product is: [[-31. -13.] [ 74. 31.]] The log-absolute-determinant of the renormalized projectivization is: -8.608130186408527 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17549306 0.49251278] [0.49251278 0.206355 ]] The log-absolute value of the near zero eigenvalue is: -17.539682136925283 The approximate limit point is: [ 0.92231631 -0.38643579] The matrix product is: [[ 31. 13.] [-74. -31.]] The log-absolute-determinant of the renormalized projectivization is: -8.608130186408527 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17549306 0.49251278] [0.49251278 0.206355 ]] The log-absolute value of the near zero eigenvalue is: -17.539682136925283 The approximate limit point is: [ 0.92231631 -0.38643579] The matrix product is: [[70. 29.] [29. 12.]] The log-absolute-determinant of the renormalized projectivization is: -8.496990484098841 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.17163265 0.48530612] [0.48530612 0.20102041]] The log-absolute value of the near zero eigenvalue is: -17.310726354950265 The approximate limit point is: [ 0.92387953 -0.38268343] The matrix product is: [[-41. -17.] [ 70. 29.]] The log-absolute-determinant of the renormalized projectivization is: -8.496990484098706 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.34306122 0.55653061] [0.55653061 0.23061224]] The log-absolute value of the near zero eigenvalue is: -17.447393627661846 The approximate limit point is: [ 0.92382689 -0.38281051] The matrix product is: [[ 41. 17.] [-70. -29.]] The log-absolute-determinant of the renormalized projectivization is: -8.496990484098706 The quadratic form Transpose(x)*x of x=rpl(x) is: [[1.34306122 0.55653061] [0.55653061 0.23061224]] The log-absolute value of the near zero eigenvalue is: -17.447393627661846 The approximate limit point is: [ 0.92382689 -0.38281051] The matrix product is: [[-12. 29.] [ 29. -70.]] The log-absolute-determinant of the renormalized projectivization is: -8.496990484098877 The quadratic form Transpose(x)*x of x=rpl(x) is: [[ 0.20102041 -0.48530612] [-0.48530612 1.17163265]] The log-absolute value of the near zero eigenvalue is: -17.31072635129126 The approximate limit point is: [-0.38268343 -0.92387953] . # To do: are two-generator subgroups sufficient to enumerate all the limit points? # To do: need collect approximate limit points and attach a confidence value, i.e. rank by how small the smallest eigenvalue is in quad(rpl(x)). . What&#39;s the python code say? . First we need to generate the ends of a tree. That is, the ends of a Cayley graph. Why? Because the limit points correspond to divergent sequences of group elements. We see that a sequence diverges iff the sup norm of the matrices diverges to infinity, i.e. some element must be diverging. Here we generate the ends of the tree &quot;abstractly&quot;, and then replace with actual matrix elements $S, T$ using a python dictionary. . Second, given divergent sequences we take the renormalized projective limit (RPL) of this sequence. This is inspired by H. Furstenberg&#39;s paper &quot;On Borel&#39;s Density Theorem&quot; The idea of taking the RPL is based on the natural ambient compactification of $ bf{H}$ when viewed in Voronoi&#39;s Cone model of hyperbolic space. That is, the RPL naturally defines the limit point as lying on the visual sphere at-infinity. . When we study RPL&#39;s with near zero determinants, then we find approximate limit points of the orbit. In the above computations, we restrict ourselves to two-generator subgroups of $ Gamma$. Note: a ping-pong argument should tell us that the limit points of a two-generator subgroup accumulate in a Cantor set at-infinity. .",
            "url": "https://jhmartel.github.io/fp/2022/06/16/AhlforsConjecture.html",
            "relUrl": "/2022/06/16/AhlforsConjecture.html",
            "date": " • Jun 16, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Ampere Birkeland Currents.",
            "content": ". This article is an extension of Don Scott&#39;s work on modelling Birkeland currents. Search for his articles here. These Birkeland currents (BCs) are cosmic low beta plasma discharges delivering electric current along filaments in space. We recall that $ beta$ is the ratio of the plasma thermal pressure $p=Nk_B T$ with the so-called &quot;magnetic pressure&quot; $B^2/2 mu_0$. So low beta means that so-called magnetic pressure is the domainant force in the interaction. . Professor Scott&#39;s model is derived from Maxwell&#39;s equations and Lorentz&#39; force law. Our goal is to reconsider his model in light of Ampere&#39;s electrodynamics. Thus the subject of this article is ABC, or Ampereian Birkeland Currents. . Ampere&#39;s Force Law . Ampere&#39;s investigations in 1820-1826 led to his proposing a force law between two current elements $I_1 ds_1$ and $I_2 ds_2$. The current element has a scalar $I$ which represents the net current intensity, and $ds$ is a differential vector element representing arc length along the circuit. The magnitude of $I$ typically represents the current intensity, while the magnitude of $ds$ represents the current velocity. . Alternatively we could follow Weber and write $I ds = q dv$. That is, an electric current element is essentially equivalent to charge in motion, i.e. a charge $q$ in velocity $v$. . Ampere determined that the force between two current elements was radial and proportional to the current intensity and to current velocities. Thus the force $F$ was proportional to $I_1 I_2 ds_1 ds_2$. Ampere discovered the inverse square distance proportionality. And moreover determined the precise coefficient of proportionality. In cgs units Ampere&#39;s force law is expressed: . $$ F_{~I_2 ds_2 ~~ text{on} ~~ I_1 ds_1 } =F_{21}= - I_1 I_2 frac{ hat{r}_{12}}{r_{12}^2} [ 2(ds_1 cdot ds_2) - 3 ( hat{r}_{12} cdot ds_1) ( hat{r}_{12} cdot ds_2) ] =-F_{12} .$$ . The following code uses Ampere&#39;s formula: . def dot(v1, v2): vx1, vy1, vz1 = v1 vx2, vy2, vz2 = v2 return vx1*vx2+vy1*vy2+vz1*vz2 def scalar(a, v1): vx1, vy1, vz1 = v1 return [a*vx1, a*vy1, a*vz1] ## Ampere&#39;s Force def F(state1, state2): x1, y1, z1, vx1, vy1, vz1 = state1 x2, y2, z2, vx2, vy2, vz2 = state2 v1=[vx1,vy1,vz1] v2=[vx2,vy2,vz2] I1=dot(v1,v1)**0.5 I2=dot(v2,v2)**0.5 rho_12 = ((x2-x1)**2+(y2-y1)**2+(z2-z1)**2)**0.5 rhat12 = scalar(rho_12**-1, [x1-x2, y1-y2, z1-z2]) coefficient = -I1*I2*(2*dot(v1,v2)-3*dot(rhat12, v1)*dot(rhat12, v2))*(rho_12)**-1 return [x1,y1,z1]+scalar(coefficient, rhat12) s0=[0,0,0,.1,.7,0] s1=[1,0,0,0.9,0,0] print(F(s0,s1)) print(F(s1, s0)) . [0, 0, 0, -0.05727564927611038, 0.0, 0.0] [1, 0, 0, 0.05727564927611034, 0.0, 0.0] . Ampere&#39;s force law has several predictions. . Parallel collinear current elements repel; | Reversing a parallel colinear current attracts by 2/3rd of the repelling force; | Parallel adjacent current elements attract; | Parallel oppositely oriented current elements repel. | . These can all be tested via various samples. . s0=[ 0,0,0, 0,0,1 ] s1=[ 0,0,2, 0,0,1 ] s3= [ 0,0,3, 0,0,-1 ] print(F(s0,s1)) print(F(s1,s0)) print(F(s0,s3)) print(F(s3,s0)) # Parallel Adjacent Current Elements Attract s2=[ 1,0,0, 0,0,1] print(F(s0, s2)) . [0, 0, 0, 0.0, 0.0, -0.5] [0, 0, 2, 0.0, 0.0, 0.5] [0, 0, 0, -0.0, -0.0, 0.3333333333333333] [0, 0, 3, -0.0, -0.0, -0.3333333333333333] [0, 0, 0, 2.0, -0.0, -0.0] . What is a Birkeland current? . Here we imagine a direct current through space which is being conducted in a plasma cylinder. We cannot assume that the cylinder is rigid and strictly right angled throughout, i.e. the Birkeland currents will possibly be radially contracting. We do not wish to assume rotational symmetry around the central axis, although it is convenient in some settings. For example, two doubly infinite parallel filaments will attract/repel depending on their relative orientations. This interaction contributes to the internal potential energy, and the filaments will attract/repel towards a lower energy state. . . Note: in the above right hand image, the force between the Birkeland currents is probably computed according to Coulomb&#39;s formula, and not Ampere&#39;s. So we would expect the force to differ from that graph. . We are trying to find the mathematical equations for the Birkeland currents. We model the currents as a cylindrical configuration of plasma ions in motion, and deliverying a sustained direct current. Therefore we want to know something about the distributions $j$ of electrical charges in motion satisfying: . $$ int_{area} mathbf{j} cdot mathbf{n} dS=I=constant,$$ where the integral is defined over a two-dimensional cross-section of the cylinder. . Now we follow Scott&#39;s analysis, namely that the Birkeland currents are ideally suited to satisfy the minimum total potential energy principle. In otherwords, the stable ground state of a Birkeland current is that with minimal internal energy. In the Ampere perspective, we have charges in motion and the primary dominant force is Ampere&#39;s. Now we must proceed cautiously: if we follow Scott, then the primary force is $F=q(E+v times B)$ and the internal potential energy is measured by the force $j times B$. Thus Scott&#39;s interpretation of &quot;force free field aligned currents&quot; leads to the equation $$j times B =0$$ throughout the current. Moreover Maxwell&#39;s equation says $ nabla times B = mu j$, and therefore $$( nabla times B) times B =0.$$ This leads to the hypothesis (&quot;ansatz&quot;) that $$ mu j = alpha B.$$ Moreover Scott discovered that if we assume $B_r=0$, then we find $B_z$ satisfies a zero order Bessel equation in the variable $r$. . The above gives an interpretation of the Birkeland current from the standard Lorentz force and Maxwell equation perspective. But we are proceeding with strictly Ampere&#39;s force law as the only force acting on the current. Therefore we must carefully think about the meaning of minimal total potential energy in the current. . [To be continued] .",
            "url": "https://jhmartel.github.io/fp/2022/05/23/Weber_Birkeland_Currents.html",
            "relUrl": "/2022/05/23/Weber_Birkeland_Currents.html",
            "date": " • May 23, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "Eddington's May 29, 1919 Eclipse.",
            "content": ". As the reader of this blog will know, the author has an interest in fundamental physics, and is especially critical of the standard early 20th century methods. Our purpose in this article is to review some basic elements of Eddington&#39;s 1919 expedition to observe the possible gravitational deflection of starlight. The 1919 eclipse is a key experimental pillar of general relativity (GR). . Here is an interesting physics stackexchange question about the correct procedure used in the Eddington 1919 observation. . It was argued by the late Nasa Engineer and physicist Edward Dowdye Jr. (RIP) that Eddington&#39;s assumption of there being no refractive medium near the solar &quot;surface&quot; (which is a questionable assumption) is incorrect and invalidated by experiment. As known today, the Sun has a plasma limb (of approximately [insert ref] dimension). . Modern GR apologists would argue that many more experiments have been performed, and that we need judge the theory by it&#39;s best experiments. Then proceeds a list of complicated papers, e.g. as provided by Prof. Levraz in this thread. We reproduce his list below. . E. B. Fomalont and R. A. Sramek, Phys. Rev. Lett. 36, 1475 – Published 21 June 1976 . Shapiro, S. S., Davis, J. L., Lebach, D. E., &amp; Gregory, J. S. (2004). Measurement of the solar gravitational deflection of radio waves using geodetic very-long-baseline interferometry data, 1979–1999. Physical review letters, 92(12), 121101. . Fomalont, E. B., &amp; Kopeikin, S. M. (2003). The measurement of the light deflection from Jupiter: experimental results. The Astrophysical Journal, 598(1), 704. . The key prediction we are interested in is that reported by Edward Dowdye Jr., which is summarized in the following gif taken from his former website. . . Now my question is how to independantly perform this experiment. Dowdye comments that delays, like the so-called &quot;Shapiro effect&quot;, were typical, but no bending outside of the plasma limb was observed, i.e. no apparent bending in vacuum. Dowdye appears to have had access to NASA&#39;s Voyager data. [Need elaborate]. . In practice we assume that the light rays are arriving in the &quot;far field limit&quot;, i.e. the incoming rays are parallel at infinity, but their trajectories are effected by their interaction with the Sun. In GR this interaction is continuous with respect to the distance of the ray from the Sun. So the incoming rays are initially parellel, but will develop curvature as they approach the Sun (in the GR model). . So how do we test Dowdye&#39;s claim? According to Dowdye&#39;s arguments about the Gauss sphere, we need not wait for another solar eclipse. If we are $kR$ solar radii distance from the Sun, then the expected angle of deflection is -- according to Dowdye&#39;s reasoning -- equal to $ frac{1}{k} delta theta_R$. Here $ delta theta_R$ is Dowdye&#39;s notation for the expected deflection at the edge of the Sun, i.e. at distance equal to one solar radii. . [To do: check that Einstein&#39;s GR also predicts $ frac{1}{k} delta theta_R$ deflection] . Dowdye suggests that we don&#39;t need to wait for another solar eclipse, since there should be measurable deflection for starlight which passes several solar radii near the Sun. I.e., we need study constellations which appear &quot;near&quot; the Sun. We might take regular photographs of stars near the Sun, and compare with photographs of these same star systems when the Sun is far from these stars, say, six months later. . Such an elementary test... has it not been performed? . Remark. Testing the optical position of these stars is perhaps too simplistic. More modern experiments would use radio waves, or radiation at different wavelengths beyond the visible light spectrum. We are not sure how the modern experiments use &quot;quasar&quot; radio sources in the sky instead of optical light. Again, following Dowdye, we might expect delays in the radio transmissions when they pass near the Sun. However we do not expect displacement of the radio sources. . [To be continued...]. .",
            "url": "https://jhmartel.github.io/fp/2022/05/19/EddingtonEclipse.html",
            "relUrl": "/2022/05/19/EddingtonEclipse.html",
            "date": " • May 19, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "Physics is a calling, and I want to run a physics lab.",
            "content": ". So i tell people that i want to run a physics lab in five years. The typical response is &quot;But what do you know about physics or running a physics lab?&quot; And that&#39;s a fair question for the general population. But what don&#39;t i know about physics that i need to know? Surely plenty, but why should that stop me from running a lab? . I look to Ernest Rutherford, or Kristian Birkeland, or Ampere. These were great physicists in the 19th and early 20th century. You might not know this, but I know that i am them. Sort of a &quot;cut from the same cloth&quot; idea. . My background -- what i always tell people -- is that in high school i decided i wanted to be a mathematician. It was a calling, and it was a dream that i pursued for 12+ years, and that dream continues and has brought me here. So here i am now telling you my dream is to run a physics lab. Do you doubt it? . I am formally trained in mathematics, which is a different discipline i admit, but very relevant to physics. And i don&#39;t mean modern theoretical physics, but very relevant to classical physics. Let us remember that all the great physics was performed in 19th and 18th century by gentleman and gentlewomen of modest means, but of extraordinary mind and ability. We are especially mindful of Faraday, Ampere, and Weber. Moreover Gauss and Riemann themselves were especially physically oriented. . Are &quot;trained physicists&quot; the only persons qualified to run physics labs? No, but they are sometimes qualified to operate in a lab. Good, i might need some physicists in the lab depending on their specialties. An electrician is always useful, or somebody trained in electrochemistry or optics, for example. Moreover experiments also need calibrations, and here sometimes experienced specialists are required. . But none of that is any reason why ii can&#39;t run a physics lab. . Why do i want to run a physics lab? because i want to be captain of the ship, because new programs are going to be needed, new discoveries and results are going to be obtained. This is the Thomas Kuhn type of paradigm shift, where the previous generation will be the last to know that the revolution has begun to turn over. . From our view we see classical physics being recalled to life, and being properly rejoined with the experimentalists of 18th, 19th century. We see finally a solution to the same problems that Bohr to Sommerfeld to Heisenberg to Schrodinger all confronted but counld not resolve. In this path the quantum is unnecessary hypothesis. For recall that Planck introduced the quantum as a hypothesis to explain his formula for relating temperature to the spectrum of blackbody radiatio, and he had no explanation for it And its caused no small amount of trouble for the 20th century. What if the quantum discontinuity could finally be understood in the classical continuity? . We see the opportunity for this now, presently in 2022. . So why do i want to run a lab? Because i want to be positioned to lead the way, be among the shock troops of this new frontier of electrical physics. But it remains something of an obscure path, but which is becoming clearer. There is opportunity to become a pioneer and build the trail, and to document the route. And i&#39;m not the first, but i feel like the first wave who&#39;ve learned from Electric Universe and Thunderbolts Project, and picking up the tradition left from Wilhelm Weber. . The next step is building a team. At this stage, I really ought to have graduate students or collaborators. My greatest fault is that I&#39;ve always worked independantly. But now I need teammates, because the task i want to now undertake is too big for me alone. . There&#39;s alot of experiments I would like to do. The simplest experiments would begin with Weber&#39;s potential = $$U(r,r&#39;)= frac{q_1 q_2}{r^2}(1- frac{r&#39;^2}{2 c^2})$$ and the basic predictions of Weber electrodynamics. These are explained in the Brazilian AKT Assis&#39; work on Weber. . We are tempted by the possibility of explaining the relatively short lifetimes of positronium. We are also interested in resolving all the classical objections to Bohr&#39;s model of the atoms, e.g. hydrogen, helium, etc.. . Theoretical calculations with Weber&#39;s potential applied to the Weber-Bohr-Sansbury model of the electron and &quot;positron&quot; could yield comparisons with the relatively short lifetimes of para-positronium and ortho-positronium (approx. 0.12 ns and 138.6 ns, respectively according to the Wikipedia reference). | This would be an interesting test of Weber versus QED. . How difficult is it to create positronium in the laboratory? There is possibility of using a radioactive source that ejects positrons. Specifically, Carl Andersen discovered that alpha particles (helium nuclei) bombarding beryllium (atomic number 4 Be !) emitted positrons. Beryllium Be is rather little known atomic element, but now appears very interesting from the Weber viewpoint. . How does the Weber potential model the emission of positrons when beryllium is bombarded by helium nuclei? | [To be continued...] .",
            "url": "https://jhmartel.github.io/fp/2022/05/17/iWantToRunAPhysicsLab.html",
            "relUrl": "/2022/05/17/iWantToRunAPhysicsLab.html",
            "date": " • May 17, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "Fizeau and Sansbury",
            "content": ". As described in previous posts, our critical essay on foundations in SR is basically stable and readable. Here we begin to elaborate on the concluding section, which briefly mentions Ralph Sansbury&#39;s proposed experiment. Sansbury&#39;s experiment is this. We take $c$ the speed of light as equal to $1$ foot per nanosecond $[ mu s]=10^{-8}$ seconds. . Sansbury&#39;s Experiment . We quote from Sansbury&#39;s paper and his book &quot;Faster than Light&quot;. . (Case 1) A $15$ nanosecond light pulse from a laser was sent to a light detector, $30$ feet away. When the light pulse was blocked at the photodiode during the time of emission, but unblocked at the expected time of arrival, $31.2$ nanoseconds after the beginning of the time of emission, for $15$ nanosecond duration, little light was received. (A little more than the $4mV$ noise on the oscilloscope). This process was repeated thousands of times per second. . (Case 2) When the light was unblocked at the photodiode during the time of emission ($15$ nanoseconds) but blocked after the beginning of the time of emission, during the expected time of arrival for $15$ nanoseconds, twice as much light was received ($8mV$). This process was repeated thousands of times per second. . Sansbury&#39;s conclusion? That this indicated that light is not a moving wave or photon, but rather the cumulative effect of instantaneous forces at a distance. That is, undetectable oscillations of charge can occur in the atomic nuclei of the photodiode that spill over as detectable oscillations of electrons after a delay. . Sansbury found the equipment necessary for the experiment too expensive to rent for an extended period of time, and he was possibly not a sufficient expert in calibrating the equipment. So Sansbury&#39;s experiment appears to have not been sufficiently investigated, and we would argue that the experiment has neither been reproduced nor properly reviewed. Thus we turn to the classical Fizeau experiment, and consider its similarities to Sansbury&#39;s setup. . Fizeau&#39;s Saw Tooth Experiment (1849) . Now Sansbury&#39;s apparatus has some similarity with Fizeau&#39;s sawtooth apparatus, as used circa 1849 to prove some of the first sensible measurements of luminal velocity. . Here is a blog which inlcudes a transcription of Fizeau&#39;s original 1849 paper in Comptes Rendus. . Here is an interesting youtube video en francais sur la mesure de Fizeau. Around the 4-5 minute mark is the most interesting. While the speed of the wheel is increased, the received light signal becomes increasingly erratic and intermittent, until a sufficiently high speed of rotation is achieved and the received light signal becomes eventually null and no light is received. . Here is a useful physics stackexchange answer. . Sansbury vs. Fizeau . Now Fizeau&#39;s original setup is a type of Sansbury test where the wheel is in uniform motion. Sansbury&#39;s setup involves either a wheel in nonuniform motion, or equivalently a wheel in uniform motion with a nonuniform sawtooth distribution. . We remark that Fizeau was historically looking to estimate the luminal velocity $c$. Sansbury&#39;s experiment assumes $c$ as given, estimated at $1$ foot per nanosecond. Sansbury&#39;s goal is to distinguish light propagation from particle model, and his setup is meant to test whether light is even something that travels at all. . Let&#39;s review the basic math of Fizeau&#39;s apparatus, it&#39;s very simple. No matter how we setup the mirrors, we suppose light has some total travel time. In Fizeau&#39;s original setup, light travelled a total path of $ approx 16 km$. With an expected speed of light $c=3 times 10^8 km$, then the expected travel time is . begin{equation} frac{16 times 10^3 [m]}{3 times 10^8 [m]/[sec]}=5.333 ldots times 10^{-5} [sec]. end{equation} . Now consider the wheel with angular velocity $ omega$ having units of $[degrees]/[sec]$ and having a toothlength equal to $1/720$ degrees. The time required to turn one toothlength is therefore $ frac{1/720}{ omega}= frac{1}{720 omega} [sec]$. Thus we find that the expected travel time is equal to the time to rotate one toothlength if the following equality holds $$ frac{16 times 10^3 }{3 times 10^8} = frac{1}{720 omega}, $$ which implies $$ omega approx 26 ~~~ frac{[rotations]}{[sec]}.$$ . Sansbury&#39;s (Case 1) could be realized if the wheel was allowed to rotate nonuniformly, i.e. if the wheel could be accelerated in &quot;impulses&quot; something closer to the actual discrete motions of a clock. For example, if the wheel is initially opened at the time of emission, then immediately rotated one saw tooth length (to the closed position) during the expected time of travel, and just prior to the expected time of arrival is rotated another tooth length (to the open position), then Fizeau would predict that the receiver would observe a strong light source. However Sansbury predicts that the receiver would observe rather a very weak signal. Notice here we require the wheel to move twice as fast as Fizeau&#39;s angular velocity. In otherwords the wheel must rotate two complete tooth lengths before the estimated arrival time. . Sansbury&#39;s (Case 2) could be realized if the wheel was rotating nonuniformly. For example, if we keep the wheel fixed during the expected time of flight of the light particle, and turn the wheel one complete toothlength at the expected time of arrival, then Sansbury would predict a relatively strong signal would be received. Fizeau and the particle model would however predict no light would be received, since in the model it would be blocked by the sawtooth at the expected time of arrival. . N.B. Fizeau&#39;s conception of the sawtooth wheel is classical. But what happens if we retrospectively apply the SR methodology to the experiment, what results are obtained? It appears that SR has a null effect on the entire experiment, i.e. returns the same results as the classical case. While the sawteeth lengths are contracting in SR, this effects the circumference of the wheel but not the angular velocity. Thus Fizeau experiment appears insensitive to any Lorentz SR effects and an experiment which cannot prove SR in contrast to the classical mechanics. . We do not require mathematics at this stage, but rather to perform an experiment. However there is an intersting math aspect to the question, &quot;How to keep a nonuniform wheel in uniform motion?&quot;. . Fizeau&#39;s original wheel was materially balanced: the distribution of teeth was equidistant and regular. The centre of mass corresponded with the axis of rotation. . But if we begin to study nonuniform wheels, then the behaviour becomes more difficult depending on, say, whether the centre of mass coincides with the axis of rotation. . Some comparisons between Fizeau and Sansbury: . Fizeau&#39;s involves several reflecting mirrors (beam splitters). Therefore there is more interaction involved in Fizeau&#39;s setup than with Sansbury&#39;s. For Fizeau&#39;s is a type of two-way trip of light, where the source and receiver are space-coincident. But Sansbury&#39;s is a one-way trip, requiring some electronics at the receiver namely a photodiode, to measure the amount of electrons released by the light emission. . | If the phase of Fizeau&#39;s wheel could be controlled, then we could compare the behaviour of the experiment when the wheels differ by one saw tooth length. The trouble in Fizeau is that, because the source and receiver coincide, it&#39;s evident that no light is emitted when the phase is shifted one tooth length. Sansbury&#39;s experiment however does emphatically require the apparatus to be open at the moment of emission. . | Faster Fizeau Wheels . We could test some of Sansbury&#39;s ideas if we could increase the angular velocity of the Fizeau wheel by factor of $4$, i.e. we need a wheel of roughly $100$ revolutions per second instead of $20$ revolutions per second. . Given such a revolution speed, then we could change the sawtooth pattern of the wheels, having some that are $1/4$ closed, $1/2$ closed, and $3/4$ closed wheels. For example, we could have the alternating sawtooth $$ ldots 0101010101 ldots$$ or we could have $$ ldots 001100110011 ldots$$ both of which are $1/2$ closed but having different patterns. And these patterns would have different predictions depending on the photon model or Sansbury&#39;s cumulative action-at-a-distance. Likewise it would be interesting to compare the predictions given a wheel having a $1/4$-closed sawtooth pattern $$ ldots 0001000100010001 ldots$$ versus a $3/4$-closed pattern $$ ldots 0111011101110111 ldots.$$ . If we could get the Fizeau wheel to spin $200$ revolutions per second, then we could test the theories according to $8$-periodic patterns, i.e. with sawtooth patterns being $1/8, 2/8$, $ ldots$, $7/8$ths closed. . If we could build a larger wheel with more teeth, say, $1440$ teeth, then $2880$ teeth, then basic gear ratio would increase the speed of the initial pinion wheel by factor of $ times 2$, $ times 4$, etc.. . The heuristics by which we can determine reasonable revolutions per second depends probably on some energy estimates and would require smaller and smaller radii. . [To be continued...] .",
            "url": "https://jhmartel.github.io/fp/fizeau/sansbury/c/sr/2022/04/29/FizeauSansbury.html",
            "relUrl": "/fizeau/sansbury/c/sr/2022/04/29/FizeauSansbury.html",
            "date": " • Apr 29, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "GR, OT. Part 2.",
            "content": ". Costs and Optimal Transport (OT) . What is OT about? Everybody knows its about costs, and specifically trying to minimize the expected cost of a coupling or semicoupling or correlation $ pi$ between a source $(X, sigma)$ and a target $(Y, tau)$. The coupling $ pi$ is a measure on $X times Y$ which correlates $ sigma$ with $ tau$. . But what is the cost $c(x,y)$ of transporting a unit mass from $x$ to a unit mass at $y$? . Here we assume that the measures $ sigma, tau$ represent matter which needs be conserved. So if a unit mass at $x$ is transported to a unit mass at $y$, then the intermediate masses, say $ mu_s$ for some parameter $s$, needs to have total mass $ int mu_s$ equal to $1$ for all parameters. We will comment below on the possible confusion arising from the use of the term &quot;mass&quot; in applications of OT to GR. . A priori, it is difficult to construct costs $c$ between spaces $X, Y$ which occupy different spaces, having no spatial relationship between points $(x,y)$, and having no measure of either near or far, or hot and cold. In case $X,Y$ occupy different universes, then they might be said to be infinitely far apart and the only canonical cost appears to be a constant zero (or constant infinity) cost. Therefore insofar as optimal transport is studying geometric transport, it is necessary that the source and target spaces $X,Y$ have some spatial relations between the various elements. As trivial as this sounds, it is very difficult and important problem to construct interesting geometric costs. . In practice the author finds best results are obtained when the target $Y$ is given as a subset of the source $Y hookrightarrow X$. Interesting topological applications arise when $Y= partial X$ is a type of rational bordification of $X$, e.g. $Y= partial X[t]$ where $X[t] subset X$ is a $ Gamma$-rational excision like Borel-Serre bordifications of locally symmetric spaces. See our thesis for such examples. . The author is also not convinced one can invent en abstracto interesting geometric costs. . So rather we turn to physical models for inspiration. In our view, cost always represents a cost of energy in Joules, and not necessarily of dollars, or kilometers. . So a cost $c(x,y)$ measures an interaction between a unit mass at $x$ and a unit mass at $y$. . In our minds, this tells us that costs $c(x,y)$ represent interaction energies. . In our thesis [Ibid] we compared the properties of attractive costs, e.g. the quadratic geodesic cost $c(x,y)=d(x,y)^2/2$ when $Y subset X$, with the class of so-called repulsive costs. These costs were interesting because the geometry of the singularities of $c$-optimal transports had very different structures. . We were motivated by electrodynamics. Heuristically, the attractive costs represents interaction energies between oppositely charged positive source and and negatively charged target configurations. The repulsive cost represents interaction energies between, say, positive source and positive target configurations. We recall that opposite charges attract and like charges repel, hence the terminology. This final idiom that like charges repel has a very interesting modification when interaction energy is measured via Weber&#39;s potential, but we leave this for future topic. This is briefly discussed here. . GR and OT . We would like to continue to develop our ideas on GR and OT. This seems popular subject, and I know we have an interesting viewpoint on the whole situation. Where are we? In previous section we were discussing cost as energy. This implies that the expected cost, or integral $ int c(x,y) d pi(x,y)$ also has well-defined units of energy. Bearing this in mind, let us now continue our discussion of GR and OT. . As we discussed here what Robert calls the Lorentz distance $ ell( sigma, tau)$ between measures $ sigma, tau$ on $ mathbf{R}^{3,1}$ and which he interprets as the &quot;expected maximal proper time between the events $ mu, nu$&quot; is not readily interpreted as an energy, and so we find it difficult to accept $ ell$ as a suitable cost in the classical sense. . And so we return to the question &quot;what does $ ell(x,y)$ represent given events $x,y$? &quot; Given the above discussion, let us then also ask: &quot;what does the additive expected value represent $ int ell (x,y) d pi(x,y)$ when $ pi$ is a coupling measure between events $ sigma, tau$? Our point here is that the cost has definite energy units in the classical setting, and these energy units are additive, and therefore the integral representing the expected value again has well-defined energy units. However with the Lorentzian distance $ ell$, there are no units to justify or confirm that the additive average is well-defined. Of course, the numerical integral value is well-defined, but there are many other possible modifications. Robert&#39;s paper has anticipated this objection somewhat in his general treatment of $q ell(x,y)^q$ for $0 &lt; q leq 1 $. . Is the above question irrelevant? Some might rationalize it away, and dismiss the objection. They might ask &quot;why should $ ell$ need an interpretation?&quot; or &quot;why should $ ell$ require units?&quot;. . Our response would be, &quot;well, are we looking for something to compute, or are we looking for something to experimentally verify?&quot; If we are just computing values, and if that is considered physics, then okay, we don&#39;t need units. But if physics is to relate to observation, then we necessarily need units. Why? Because units are used to quantify uncertainty. Again, this is the classical viewpoint. . Remark. If the Lorentz-Minkowski metric $ds^2$ was positive definite, then we could happily represent $ ell$ as a sum of positive squares, in which case we have a formula in the units of energy, i.e. kinetic energy assuming that one can define the inertial mass. But in the SR spacetime formulation, there seems no opportunity to introduce inertial mass, and the sum of signed squares has no Riemannian metric meaning. . Problem: Construct Interesting Energetic Costs . So before we digress into a question about GR and OT, let&#39;s pose some problems. Basically the usual quadratic cost $c(x,y)=d(x,y)^2/2$ is taken as the canonical cost on a Riemannian manifold. Thus we witness the same thing with proposals for applying OT to SR and GR, and this is indeed natural. . Our question here is where to find more examples of costs. For as we developed in our thesis, for one example, different costs can generate singularities of very different homotopy type. From our point of view, this depends on whether costs are attractive or repulsive. . For example with an attractive cost, one is typically looking at ground states which collect near the target. However for repulsive costs, the ground states are typically deeply nested in the source domain, i.e. states are being repelled from the target, and look to escape as far away as possible. . We continue to use electrodynamic energies as the basic supply of interesting costs. The author would be open to hearing other recommendations for interesting costs. . m, matter, mass, Mach. . I can&#39;t help myself from making another comment on the challenge of applying OT to GR. In OT one often speaks about mass transport, where it&#39;s always assumed that a continuity equation holds. Thus when the measures are transported there is conservation and nothing lost or gained along the way. . So if OT is to study &quot;mass transport&quot; in the setting of GR, are we to assume that mass also will satisfy local conservation ? In otherwords, what are the measures $ sigma, tau$ actually representing on the spacetime ? . Wal Thornhill has made this point himself, that among the greatest hazards and sources of confusion in physics is the unfortunate coincidence that both mass and matter begin with the same letter &quot;m&quot;. . Really no joke. That&#39;s the cause of all the trouble. What happens is mathematicians and physicists both get lazy and begin to interpret &quot;m&quot; for matter and mass as if they are equivalent or interchangeable. This possibly originates in Newton&#39;s own non-definition of mass as simply the presence of immediate ponderable matter, and Newton assumed that there was some unspecified constant of proportionality between mass and matter and so &quot;up to a constant which we can set to $1$&quot; both &quot;m[atter]&quot; and &quot;m[ass]&quot; became confused with the letter $m$. . Einstein&#39;s Equivalence Principle is another source of confusing the gravitational mass $m_g$ of an object (which following Newton is something vaguely defined like the quantity of gravitational charge, or in otherwords quantity of matter) with the inertial mass $m_i$. Thus Einstein&#39;s argument that $m_g=m_i$ is another source of confusion. The argument against Einstein&#39;s equivalence principle is elementary, and recognized by Einstein himself that rotating bodies do not admit global inertial frames! Therefore the inertial frames used to convert the gravitational potential into an inertial reference frame is only defined locally on the tangent space. It is very limited first-order observation. . Amazing there is another &quot;m&quot; that enters the problem of &quot;matter&quot; and &quot;mass&quot;, namely Mach! Because Mach proposed that the inertial mass $m_i$ must be defined as the potential energy of the body relative to the fixed stars at infinity. What are the implications? Namely that . matter can neither be created nore destroyed . while inertial mass is variable depending on the interaction of the matter with the matter of the fixed stars at infinity. . This is essentially my understanding of AKT Assis&#39; development of Relational Mechanics. This is also Halton Arp&#39;s interpretation of the observed intrinsic red shifts of quasars which are visibly interacting with nearby systems. Arp&#39;s idea was that, quasars are creation hotspots in the universe, where newly created atoms have less mass because they have been interacting with only a limited part of the universe for a short period of time. Therefore their inertial mass is much smaller, and therefore the wave lengths emitted by the atoms is increased. This increased wave length is caused not by usual red shift velocity mechanism, but by the Machian dependance of inertial mass with the other matter in the universe, and these matter-to-matter signals take time. This is the meaning of the intrinsic red shift, as opposed to the Hubble-Einstein velocity red shift. Further details can be found in Arp&#39;s book &quot;Seeing Red&quot;. .",
            "url": "https://jhmartel.github.io/fp/einstein/sr/ot/lorentz/2022/04/27/GR_OT_Part2_Costs.html",
            "relUrl": "/einstein/sr/ot/lorentz/2022/04/27/GR_OT_Part2_Costs.html",
            "date": " • Apr 27, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "Einstein and Maxwell",
            "content": ". We&#39;ve been working on an essay on the foundations of special relativity (SR). Why do we invest so much time and effort into SR foundations ? Well honestly because i think the foundations are never taught, so there&#39;s alot to say. Moreover the SR converts tend to ignore foundations, evading them and jumping ahead to their conclusions. So the task of foundations is typically neglected by adherents, and it&#39;s left to the skeptics to develop the foundational issues. And the student of the history of SR will know that there has always been a strong skeptic school in SR (and GR) and this school frequently included Einstein himself at various times in his life, and for good reason. There is much to critically examine in the SR theory, and this the purpose of our essay. . Einstein SR and Maxwell . Einstein&#39;s theory of special relativity (SR) arose from Einstein&#39;s study of Maxwell&#39;s equations (ME) circa 1870 AD. But what is the logical and mathematical relation between SR and Maxwell? This is interesting question because they are essentially antagonistic. Although Einstein was much influenced by the Maxwellian field theory viewpoint, his own early work (1905) was based on the antithetical photon model of light. . Briefly, by Maxwell equations we understand that in a given reference frame $K$ we have the existence of electric and magnetic fields $E,B$ satisfying the four equations on $div(E), div(B)$ and $curl(E)$ and $curl(B)$ relative to a charge volume density $ rho$ and electric current density $J$. In vacuum where $ rho=0$ and $J=0$, composing the first-order Maxwell equations together yields the second-order fact that the coordinate components of $E,B$ satisfy wave equations with speed of propagation $c= sqrt{ epsilon_0 mu_0}$. This usually leads to the idea that electromagnetic field disturbances travel at the speed of $c$ in aether. And indeed Maxwell&#39;s equations expressly assume an aether as the medium by the which the electromagnetic radiation travels. . Now this author does not really accept Maxwell&#39;s field equations as being satisfactory. For example, the magnetic field $B$ is not a rectifiable or reifiable field, meaning it has only potential and not any material substance. The same could be said for Maxwell&#39;s electric field, which again is a potential field describing the force experienced by a charged test particle. This is the so-called continental field-theoretic viewpoint after Maxwell, etc. . However Maxwell&#39;s equations were not satisfactory in their predictions on the photoelectric effect. For example, is light a disturbance in the electric or the magnetic field? If light is such a disturbance, then Maxwell equations predict the interaction of the $E$-wave (or is it $B$-wave) with charged test particles. . Here it&#39;s interesting to compare Einstein&#39;s 1905 explanation of the photoelectric effect using the photon particle theory of light. Thus we tend to interpret Einstein&#39;s developments of SR from a photon or corpuscular point of view. . Problem: The classical homogeneous wave equation has the property of the velocity being dependant on the receiver velocity relative to the medium. But SR argues that the assumption on the &quot;rectilinear uniform propagation of light&quot; somehow yields a wave equation where velocity is receiver independant. But how? [We do not address this important issue here]. . SR and Lorentz Groups . The null result of the Michelson-Morley experiments led to Einstein&#39;s postulating the Lorentz transformations relating space and time variables $x,t$. Undoubtedly the theory of SR is summarized in the representations of the Lorentz group of linear transformations, namely the isometry group designated $O(ds^2)=O(3,1)$ and its standard linear action on ${ bf{R}}^{3,1}$. . For the mathematician, once a single linear representation is given, there are many algebraic constructions possible to obtain further representations, for example the symmetric and alternating representations. We develop this idea further to try and bridge the assumptions of SR to Maxwell&#39;s equations, and especially the wave equation. . Now we discuss several group representations (i.e. linear group actions). . First we begin with the standard linear representation $$ rho_0:{ bf{R}}^{1,3} times L to { bf{R}}^{1,3}$$ which is the linear representation $ rho_0$ represented by left matrix multiplication $(v, lambda) mapsto lambda.v$. . Next we dualize. . Let $C({ bf{R}}^{3,1})$ be the space of polynomial functions on the space. Abbreviate $C:=C({ bf{R}}^{3,1})$. Naturally we assemble $C$ from the dual functionals $ lambda in {({ bf{R}}^{3,1})}^*$. Taking products and polynomials in the dual functions $ lambda$ we obtain the contragradient represention $$ rho_0^*:C( { bf{R}}^{3,1}) times L to C({ bf{R}}^{3,1}). $$ . The idea is that the vector spaces $V$ and $V^*$ are isomorphic (non canonically) in finite dimensions. Moreover the algebra generated by $V^*$ yields an (infinite-dimensional) space of polynomial functions on $V$. . Now what are vector fields? . In differential topology, the vector fields $ frac{ partial }{ partial x}$ act on functions as derivations, i.e. as linear maps $$ frac{ partial }{ partial x}: C to C $$ satisfying Liebniz product formula. Iterating these linear maps generates an algebra of operators on $C$, namely the operators polynomial in $ partial / partial x$. On the other hand, the differential $dx$ itself as contained in the cotangent space is not an algebra. . Iterating the derivations $ frac{ partial }{ partial x} circ frac{ partial }{ partial y}= frac{ partial^2 }{ partial x partial y}$ leads to the usual linear differential operators on $C$. We are specially interested in d&#39;Alembert&#39;s operator $$ square:= frac{-1}{c^2} frac{ partial^2}{ partial t^2} + frac{ partial^2}{ partial x^2}+ frac{ partial^2}{ partial y^2}+ frac{ partial^2}{ partial z^2} .$$ . Our main proposal, and this is not yet altogether rigorous, is to identify the Minkowski squared line element $ds^2$ as dual in a certain yet-to-be-defined algebraic sense to the d&#39;Alembert operator $ square$. The difficulty is that the symmetric product of the differential operators $ partial/ partial x$ and $ partial / partial y$ is distinct from the composition of the differential operators $ partial^2 / partial x partial y$. . The term $dx^2$ in Minkowski&#39;s line element is formally a section of the $(T^*)^{ otimes 2}$ bundle over the manifold space, here ${ bf{R}}^{4}$. So here is the informal computation. Let us formally relabel the variables $$x_0, x_1, x_2, x_3 = t,x,y,z, $$ respectively. Now the choice of Lorentz metric $h$ can be represented as a square symmetric matrix $$[h]= begin{pmatrix} -c^2 &amp; 0 &amp; 0 &amp; 0 0 &amp; 1 &amp; 0 &amp; 0 0 &amp; 0 &amp; 1 &amp; 0 0 &amp; 0 &amp; 0 &amp; 1 end{pmatrix}.$$ . The choice of $h$ allows us to define an isometry between the differential forms and vector fields, i.e. secord order linear operators. Specifically, $h$ allows us to define explicit isometry between symmetric $(2,0)$ tensors and symmetric $(0,2)$ tensors. . Lemma: The metric $h$ identifies the dual of $ds^2$ with d&#39;Alembert&#39;s wave operator $ square$. That is $(ds^2)^* = square.$ . Proof. The proof is basic linear algebra. First one needs prove that the $h$-dual of $dx_0$ is $dx_0^* =-c^{-2} frac{ partial}{ partial x_0}$. Likewise we find $dx_i^*= frac{ partial}{ partial x_i}$ for $i=1,2,3$. This is all in the tangent space, i.e. between $(1,0)$ and $(0,1)$ tensors. Now we consider the squares, i.e. the symmetric $(2,0)$ tensors. We find that $$(ds^2)^*=-c^2 (dx_0^*)^2 + (dx_1^*)^2 + (dx_2^*)^2 + (dx_3^*)^2, $$ and which is equal to $$-c^{-2} ( frac{ partial}{ partial x_0})^2 + ( frac{ partial}{ partial x_1})^2 +( frac{ partial}{ partial x_2})^2+( frac{ partial}{ partial x_3})^2, $$ which is equal to d&#39;Alembert&#39;s square operator $ square$ as desired. [See our remarks above on the nonrigorous nature of this argument, and is the subject of investigation.] . The Lorentz invariance of $ square$ shows the solutions to the homogeneous wave equation (HWE) are Lorentz covariant and $ square phi =0$ if and only if $ square lambda cdot phi =0$ for every Lorentz transformation $ lambda in L$. This is the wave equation version of the fact that the null cone $ds^2=0$ is Lorentz covariant. . Now Einstein&#39;s (A12) postulates the uniform rectilinear propagation of light in vacuum. This would suggest a corpuscular model of light, being represented as affine parameterized lines $$s mapsto (s, x(s), y(s), z(s))=(s, gamma(s)) $$ in ${ bf{R}}^{3,1}$ satisfying $D^2_{ss} gamma =0$. . Is the equation $D^2_{ss} gamma=0$ Lorentz covariant? (Yes?) . But what are the corresponding &quot;uniform rectilinear&quot; solutions $ phi$ for the dual HWE: $~~ square phi=0$ ? Compare this. . An idea: there has always been correspondance between lines in $V$ (one-dimensional linear subspaces) and quadratic functionals via the Segre embedding, or $ lambda mapsto lambda^2$ where $ lambda in V^*$ is a linear functional. . The following questions will be answered below: . Are the quadratic functions $q(x)=h(v,x)^2/2$ solutions to $ square =0$ for null vector $v in N$? (Yes, we prove below). . | Can we find quadratic functions $q$ whose level sets are everywhere orthogonal to the null cone $N$ ? . | . The idea would be to derive some canonical solutions $ square q=0$ from quadratics arising from vectors on the null cone. . If $v$ belongs to null cone, then $q(x):=h(v,x)^2/2$ for $x in V$ defines a quadratic function on $V$ with $q(v)=h(v,v)^2=0$. . It&#39;s clear that $q$ is minimized along $v^ perp$, i.e. $q(x,v)=0$ for all $x in v^ perp$ and $v in v^ perp$. Here $v^ perp$ consists of all $u$ such that $h(u,v)=0$. . Lemma. For every vector $v in { bf{R}}^{3,1}$, let $q(x):=h(v,x)^2/2$ be the quadratic form defined by $v$. Then $ square q=0$ if and only if $v in N$ and $h(v,v)=0$. . Proof. We claim that $ square q=h(v,v)$ when $q(u)=h(v,u)^2/2$. If the vector $v$ has coordinates $v= langle v_t, v_x, v_y, v_z rangle$, then $h(v,x)^2$ is equal to $$(-c^2 v_t t + v_xx+ v_yy+ v_zz)^2/2,$$ which is equal to $$c^4 v_t^2 t^2 +v_x^2 x^2 + v_y^2 y^2 + v_z^2 z^2 + (mixed~ terms).$$ Applying d&#39;Alembert&#39;s operator we find $$ square q =2( -c^2 v_t^2+v_x^2 + v_y^2 + v_z^2)=2 h(v,v),$$ since $ square(mixed~~terms)=0$ and the claim follows. . Thus we find that null vectors $v in N$ yield solutions $q_v$ to HWE. . A superposition principle also applies, where any signed measure $ mu in mathscr{M}(N)$ yields a $ mu$-averaged solution $q(x):= int_N q_v (x) d mu(v)$ to the HWE. Here it would be useful to have a representation theorem, something like, if $ phi$ is any solution of HWE, then $ phi$ can be represented as a $ mu$-average of the $h_v$ as described above. . [To do: establish the conservation of energy for the HWE from the same principles.] .",
            "url": "https://jhmartel.github.io/fp/einstein/maxwell/wave%20equation/lorentz/sr/2022/04/26/Einstein_Maxwell.html",
            "relUrl": "/einstein/maxwell/wave%20equation/lorentz/sr/2022/04/26/Einstein_Maxwell.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "GR, OT. Part 1.",
            "content": ". I recently watched Yann Brenier&#39;s YT lecture on Optimal Transport (OT) and General Relativity (GR). He mentioned Professor McCann&#39;s GRO paper. The question ``How to apply OT to GR ?&quot; seems to be gaining attention. But as Brenier suggested, there possibly appears some arrogance in the OT theorists who believe that OT is readymade to make a breakthrough in the GR field. . I have my own thoughts on the problem.T he trouble is that GR is a very radical paradigm for thinking about physics, especially about energy which is notoriously undefined (and undefinable!) in GR. Likewise work is undefined in GR. Thus it&#39;s basically impossible to properly relate thermodynamics to GR. See mathoverflow, physics.stackexchange for numerous questions about the impossibility of really defining energy tensors which satisfy a conservation law, e.g. 1, 2. These difficulties gave Einstein alot of grief throughout his life. . In OT, everything is generally controlled by the geometry of the cost $c: X times Y to mathbf{R} cup {+ infty }$, where $(X, sigma)$ and $(Y, tau)$ are the source and target measure spaces, respectively. But cost is really more properly understood as energy. . What is the cost of transporting/correlating a unit source mass $dx$ to a unit target mass $dy$ ? Well... isn&#39;t it really the energy required to transport/correlate? . Thus when OT studies couplings and semicoupling measures $ pi$ between the source measure $ sigma$ and target $ tau$, the optimization problem is really about the min energy states, i.e. ground states. So OT is already positioned to contribute to GR, as soon as GR can define energy! . For example, if we consider Professor McCann&#39;s paper, we see that path integrals of $ds&#39;:= sqrt{-ds^2}$ along so-called timelike curves lead to his definition of $ ell(x,y;q)$ for a parameter $0 &lt; q leq 1$. At $q=1$ the function $ ell(x,y)$ is like the Lorentz &quot;distance&quot; (caution this is misnomer, since $ ell$ satisfies a reverse triangle inequality!). The function is frequently related to the so-called proper time function $d tau$ defined by $ds&#39;=cd tau$. Since we do not readily admit that $ds&#39;$ has units of $[length]$, we likewise believe it&#39;s an error to generally refer to $d tau$ as having units of $[time]$. While $ds&#39;$ and $ ell$ do represent covariant scalars, we consider it antithetical to the premise of GR to consider these absolute scalars as having any physical units. . So Prof. McCann&#39;s interpretation of the &quot;Lorentz cost&quot; $$ ell( mu, nu)= sup_{ pi} [ int ell(x,y)^{1/q}~ d pi(x,y)~]^q, ~~0 &lt; q leq 1$$ as representing the quote &quot;maximum expected proper-time which can elapse between the distribution of events $ mu, nu$ &quot; is an interesting heuristic, but perhaps not a strictly proper GR interpretation. . But another trouble with the differential geometry of GR is that the Minkowski-Lorentz quadratic forms $g=ds^2=-c^2dt^2 + dx^2+dy^2+dz^2$ is a unit-less number. This is where the absolute part of the absolute differential calculus makes itself known: whatever real number is being represented by $ sqrt{-ds^2}$, it has no ``physical meaning&quot;. . Where does the proper time interpretation come from? The interpretation comes from the use of a so-called &quot;instantaneous rest frame&quot;. But how much information can a particular choice of coordinate system provide? This requires the observer to find coordinates $( tau, xi, eta, zeta)$ where $ tau$ represents ``time &quot; and all the partial derivatives vanish $$ frac{ partial xi}{ partial tau}= frac{ partial eta}{ partial tau}= frac{ partial zeta}{ partial tau}=0.$$ In this particular coordinate system one finds $ds^2=-c^2d tau^2$ and $ds&#39;=c ~ d tau$. But what does this computation really show? We consider it less persuasive than it might appear : can we really conclude that $ds&#39;$ has units of $[length]$ or that $ds&#39;/c$ has units of $[time]$, based on the form of the equation in one coordinate system ? . Conclusion. . Our point is that the tensor calculus approach to GR requires users to basically &quot;surrender their units, rigid rods and rulers at the door&quot;. Once inside the covariant category, the rods no more represent objective lengths. Irrespective of their material composition, the Lorentz transformation formulas take over and contract what was otherwise incontractible. The users themselves will see no contraction because also their corresponding &quot;proper times&quot; will be contracted. So when the Lorentzian scalar $ds&#39;$ is used by the Riemannian geometer, a careful mind needs to not readily confuse the units of $ds&#39;$ as representing $[length]$ in the timelike future directions. . These opinions originate from around year 2013--2018, when I really spent alot of time in symplectic geometry, almost-complex structures, pseudo-Riemannian and Lorentzian geometry, and took long road to realize that importing Riemannian definitions into pseudo-Riemannian structures does not yield metric Riemannian results. For example, a function $ ell(x,y)$ which satisfies a reverse triangle inequality $$ ell(x,y) geq ell(x,z)+ ell(z,y)$$ can not really correspond to a units of $[length]$. Nonetheless it&#39;s common to see triangles with relabelled edges and where the lengths in no way correspond to the length in the image. . Here looks to be very thorough introduction to the current differential geometric approach to energy in GR. I&#39;m vaguely aware of the ADM definition, originating with Weyl I believe, and which converts the covariant divergence free $ nabla_i T^{ij}=0$ into a local integral conservation equation. .",
            "url": "https://jhmartel.github.io/fp/einstein/gr/ot/units/tensors/2022/04/24/GR_1.html",
            "relUrl": "/einstein/gr/ot/units/tensors/2022/04/24/GR_1.html",
            "date": " • Apr 24, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "Alexandrov and Souls. Part 1.",
            "content": ". Here is available a very rough work-in-progress. I&#39;ve let the idea rest for seventeen months. It&#39;s difficult to illustrate the idea in python, and there appears no application of python to Alexandrov geometry yet. . What is Alexandrov geometry? . Introductions and references are available from Alexander, Kapovitch, Petrunin. The subject is notoriously difficult. The original paper of Burago, Gromov, Perelman is still useful. . An Alexandrov space $(X,d)$ is a fat and round metric space, like a sphere or cube but possibly with corners and sharp angles, and where triangles are fat. Examples include: convex sets $C$, the boundaries of convex sets $ partial C$, affine space $ mathbf{R}^d$. There are several important metric constructions by which spaces with curvature bounded below ($CBB[ kappa]$ in the notation of AKP). . We are interested in the singular mm-spaces, so we use the synthetic definition of sectional curvature and not necessarily the Gaussian definition of differential Riemannian geometry. Alexandrov spaces have sectional curvature $ kappa geq 0$, which implies that every geodesic triangle $ Delta(a,b,c)$ in $X$ is fatter than the comparison triangle $ tilde{ Delta}$ in $ mathbf{R}^2$. . In Alexandrov geometry, geodesics frequently focus and collide. Initially geodesic rays might appear to be orthogonal, but at a not-too-faraway time, the rays will focus and converge back to a point. After this refocussing the geodesics might or might not diverge again to infinity. Positive sectional curvature tends to manifest in general relativity spacetimes, where the presence of mass (attractive matter) tends to cause light rays to focus and converge. . Our goal is a general method for constructing the souls of singular finite-dimensional Alexandrov spaces. . The original result on souls was achieved by G. Perelman (1994), building on the work of Gromoll, Meyer, Cheeger. Perelman&#39;s result in the Riemannian setting was that the souls $S$ of Riemannian Alexandrov spaces $(X,g)$ had the property that: $X$ was diffeomorphic to the normal bundle of $S$. . So what remains to be proved in the singular setting? . On a singular Alexandrov space, the space of directions at every point is isometric to an Alexandrov space of curvature $ kappa geq 1$. The metric distance on the space of directions $ Sigma_p X$ at a point $x$ is defined by the angle between the directions. Alexandrov spaces (curvature bounded below by zero) have well-defined angles and directions. For almost all points, this space of directions is isometric to a $d-1$ dimensional sphere $ mathbf{S}^{d-1}$. This is well-known regularity property of Alexandrov spaces, namely that almost all points are regular. This is analogous to the fact in convex analysis that proper lower semicontinuous convex functions are differentiable almost everywhere. . The real challenge in the singular setting is to define gradient flows which are naturally defined and extend through the singular points. . Python and Alexandrov? . Our approach to mathematics is based on discovery. So instead of trying to prove everything a priori, we prefer to experiment and discover what is true. But how to use python to study Alexandrov spaces? It&#39;s not clear... . There are many analogies between lsc (lower semicontinuous) proper convex functions $f: mathbf{R}^d to mathbf{R} cup {+ infty }$ and Alexandrov spaces. For example, the regularity of Alexandrov spaces follows from the fact that: lsc proper convex functions are differentiable almost-everywhere on their domain. This is because the gradient $D f$ is Lipschitz on its domain, and therefore differentiable almost-everywhere. Therefore $f$ is even twice-differentiable almost everywhere on its domain. . Similarly, if an Alexandrov space contains a doubly-ended geodesic ray, then the Splitting theorem says $X= mathbf{R}^1 times X_0$ where $X_0$ is again Alexandrov. For convex lsc proper functions, the analogous fact is this: if the graph of the derivative $Df$ contains a double ended straight line, then up to a change of variable, $f$ can be factored as $f= ell(x_1) + f_0(x_2, ldots, x_d)$ where $ ell$ is affine and $f_0$ is a convex function which only depends on the remaining variables $x_2, ldots, x_d$. (There may be slight error here, but this is essentially the idea as i learned from Prof. R.J. McCann.) . But the proper lsc convex functions are something like the local theory of Alexandrov spaces. For example, a difficult result in the foundations of Alexandrov geometry is Toponogov&#39;s Globalization theorem, namely: if a space satisfies Toponogov comparison everywhere on small neighborhoods, then the space satisfies Toponogov comparison globally. . Although we should note that the convex analysis analog does not readily lead to a proof of Toponogov&#39;s Globalization theorem, which is very difficult result in foundations of Alexandrov geometry. For functions $f$ on $ mathbf{R}^d$ the question is: if $f$ is locally convex, then prove $f$ is globally convex. .",
            "url": "https://jhmartel.github.io/fp/alexandrov/souls/singular/2022/04/22/Alexandrov.html",
            "relUrl": "/alexandrov/souls/singular/2022/04/22/Alexandrov.html",
            "date": " • Apr 22, 2022"
        }
        
    
  
    
        ,"post28": {
            "title": "Yao's Millionaire's Problem. Part 1.",
            "content": ". The purpose of this article is to investigate whether there is strategy or skill possible in the following variation of Yao&#39;s &quot;Millionaire Problem&quot;. . Here is the game. We have a huge grid $ mathbf{R}^2$. Now let two players $A,B$ have secret locations $s_A=(x_A, y_A)$ and $s_B=(x_B, y_B)$. These secrets are points in the euclidean plane $ mathbf{R}^2$. . Now the players $A, B$ are going to take turns guessing affine functions (or affine lines in $ mathbf{R}^2$) and the first player to guess an affine function which separates the secrets wins! . The gameplay is something like this: The players $A,B$ take turns. If player $A$ goes first, then player $A$ chooses an affine function $ ell$ on $ mathbf{R}^2$, and asks player $B$ to reply with the sign of $ ell(s_B)$. We require that $B$ replies honestly with $sgn( ell(s_B))$. This is the end of player $A$&#39;s turn. If $ ell$ separates the secrets, then player A wins. Otherwise it&#39;s player B&#39;s turn. Next player $B$ chooses an affine function $ ell&#39;$, and asks player $A$ to reply with the sign of $ ell&#39;(s_A)$. Once player $A$ replies, then this is the end of player $B$&#39;s turn. Again, if $ ell&#39;$ separates the secrets, then player B wins. Otherwise it&#39;s player A&#39;s turn. . The object of the game is to determine an affine function $ ell$ which separates the secrets, i.e. for which $ sgn( ell(s_A)) neq sgn( ell(s_B)).$ The first player to demonstrate an affine function which separates the secrets wins! . Our interest is to find optimal strategies for this game. Firstly, we have to consider whether any strategy is even possible. For example, can player $A$ use the cumulative history of both player $A$ and $B$&#39;s affine guesses to better inform their next guess? For example, if player $A$ guesses an affine function $ ell$ which does not separate, then player $B$ can use the knowledge of their own private secret to determine which halfspace contains $s_A$. And indeed, by the same reasoning player $A$ can use their knowledge of $s_A$ to likewise determine which halfspace contains $s_B$. So obviously the initial distribution $d lambda$ is updated to the restricted distribution $d lambda cdot 1_H$, where $H$ is the halfspace defined by $ ell$ and containing $s_A, s_B$. With successive guesses, the distribution becomes a descending chain of closed convex sets, namely the intersection of successive halfspaces, having the form $$d lambda leadsto d lambda cdot 1_H leadsto d lambda cdot 1_H 1_{H&#39;} leadsto d lambda cdot 1_H 1_{H&#39;} 1_{H&#39;&#39;} leadsto cdots. $$ . The notation is somewhat strange, but simply expresses that we remain uncertain of the specific location of the secrets $s_A, s_B$, except we know the possibly location is becoming more restricted. . In the millionaire game, the players $A,B$ have an interest in privacy. Their secrets $s_A, s_B$ are intended to be secret. This means the players $A,B$ might not choose affine functions which potentially reveal information about their own secrets. In practice this means players determined to maintain their privacy will always choose affine functions which do not bound compact convex sets. Similarly, an opponent will not readily choose affine functions which separates the domain into a bounded component, since the probability that the opponent&#39;s secret lies in the bounded component is relatively small, while the probability of its lying in the unbounded component is much greater. . The subject of so-called zero knowledge proofs in cryptography is related to the millionaires problem. Here we try to find a balance where the players can choose to reveal as much as they wish of their own balances, while their own guesses are signals/indications in-themselves of the secret balance. . Our question is whether there is any strategy or skill in this game. What is the optimal strategy? Can the player use the knowledge of the opponent&#39;s affine functions to improve their own selection of affine function?? . import numpy as np import matplotlib.pyplot as plt # Now we simulate the millionaire problem on the euclidean two-dimensional plane. # For convenience we rename the players $A,B$ as players $+1, -1$, respectively. # for testing purposes we suppose the players A,B have secrets below: #s_A=input(&quot;What is player A&#39;s secret position?&quot;) s_A=[16,0] s_A=np.array(s_A) #s_B=input(&quot;What is player B&#39;s secret position?&quot;) s_B=[0, 0.2] s_B=np.array(s_B) # now we define some basic functions, i.e. to compute affine functions based on # their normal n and height b. def affine(n,x,b): n=np.array(n) x=np.array(x) # return n.dot(x)+b return n[0]*x[0]+n[1]*x[1] + b # to protect the secret we really only need the sign of the affine function. def sign(x_Real): if x_Real&lt;0: return -1 else: return +1 # here t defines the test function, which returns True iff the affine function # separates the secrets. True is returned if the signs of the affine function # evaluated on the secrets are not equal. def t(n,b): n=np.array(n) if sign(affine(n,s_A,b)) != sign(affine(n,s_B, b)): return True else: return False . #initial conditions. outcome=False history=[] vector_history=[] player=+1 i=0 color=[] while outcome == False: print(&quot; n Player &quot; + str(player) + &quot;&#39;s turn to play:&quot; ) print(&quot;Given the history &quot; + str(history) + &quot; choose your affine function:&quot;) n0 = float(input()) n1 = float(input()) b = float(input()) history = history + [[n0, n1, b]] vector_history=vector_history + [[n0, n1]] i=i+1 if t([n0, n1], b) == True: outcome = True print(&quot;Winner! Player &quot; + str(player)+ &quot; has separated the secrets with &quot; + str([n0, n1, b]) + &quot;. End of Game!&quot;) else: print(&quot;Fail! Player &quot; + str(player) + &quot; has failed to separate the secrets... End of turn.&quot;) player=player*(-1) # the following plots the various normals chosen by the players, but we would # prefer to have the half spaces. V=np.array(vector_history) origin=np.array([[0]*i, [0]*i]) plt.quiver(*origin, V[:,0], V[:,1], scale=21) plt.show() . Player 1&#39;s turn to play: Given the history [] choose your affine function: 1 1 0 Fail! Player 1 has failed to separate the secrets... End of turn. . Player -1&#39;s turn to play: Given the history [[1.0, 1.0, 0.0]] choose your affine function: 3 1 0 Fail! Player -1 has failed to separate the secrets... End of turn. . Player 1&#39;s turn to play: Given the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0]] choose your affine function: 1 4 0 Fail! Player 1 has failed to separate the secrets... End of turn. . Player -1&#39;s turn to play: Given the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0], [1.0, 4.0, 0.0]] choose your affine function: -3 0 -5 Fail! Player -1 has failed to separate the secrets... End of turn. . Player 1&#39;s turn to play: Given the history [[1.0, 1.0, 0.0], [3.0, 1.0, 0.0], [1.0, 4.0, 0.0], [-3.0, 0.0, -5.0]] choose your affine function: -1 -4 2 Winner! Player 1 has separated the secrets with [-1.0, -4.0, 2.0]. End of Game! . The above is a very simple gameplay, where it happens by chance that the two points can be separated by a flat strip, namely the space between two parallel halfspaces. We included the above simply as an example. . [To Do:] . Use matplotlib to plot the halfspaces, and not simply the normal vector, which is what we have above. . | Determine some automatic routine to compete with a human opponent. . | The millionaire&#39;s problem implicitly assumes the players $A,B$ have large funds, i.e. enough to pay for a dinner! Therefore we might need assume our secrets $s_A, s_B$ are sufficiently far from the origin. (?) . | If the domain is essentially infinite, then a certain amount of privacy will always be maintained, because it&#39;s better to bisect the unknown into two halfspaces of equal (possibly infinite) area. If the affine function indeed separates the secrets, then the position of that secret is only known to occupy an infinite area domain, and thus essentially remains private in a restricted sense. Although of course the direction of the secret, and not necessarily its magnitude will be better known to the opponent, i.e. there will be a definite reduction of uncertainty in the direction of the opponents secret, but not necessarily a reduction in uncertainty in its magnitude. . | If an opponent proposes an affine function which separates the domain into a bounded and unbounded component, then that is huge risk for the player, i.e. it&#39;s unlikely that the small bounded domain (chosen at random) will contain the secret as opposed to the infinite domain. At the risk of belabouring the point: a random infinite domain is more likely to contain an unknown secret than a compact domain. We find this an interesting point... . from scipy.spatial import HalfspaceIntersection prehistory = history[:-1] signs=[] sph=[] for x in prehistory: epsilon=sign(affine([x[0], x[1]], s_A, x[2])) signs=signs+[epsilon] sph=sph+[epsilon*np.array(x)] sph=np.array(sph) # for illustration we have the secret s_A as feasible_point. # its interesting question to select a feasible point which # does not reveal too much information about the secrets... # but obviously any point on the convex hull formed by the secrets s_A, s_B # will be a feasible point. But there are many more choices, so which choice reveals # the least information about the secrets s_A, s_B ? I.e. which feasible point can be chosen # which reveals the least information about s_A, s_B? feasible_point = np.array([16.0, 0.0]) halfspaces = sph*(-1) # we need reverse-signs to align with the convention in qhull that # the halfspaces are defined by the inequality Ax+b &lt;= 0. hs = HalfspaceIntersection(halfspaces, feasible_point) import matplotlib.pyplot as plt fig = plt.figure() ax = fig.add_subplot(&#39;111&#39;, aspect=&#39;equal&#39;) xlim, ylim = (-100, 100), (-100, 100) ax.set_xlim(xlim) ax.set_ylim(ylim) x = np.linspace(-100, 100, 1000) symbols = [&#39;-&#39;, &#39;+&#39;, &#39;x&#39;, &#39;*&#39;] signs = [0, 0, -1, -1] fmt = {&quot;color&quot;: None, &quot;edgecolor&quot;: &quot;b&quot;, &quot;alpha&quot;: 0.5} for h, sym, sign in zip(halfspaces, symbols, signs): hlist = h.tolist() fmt[&quot;hatch&quot;] = sym if h[1]== 0: ax.axvline(-h[2]/h[0], label=&#39;{}x+{}y+{}=0&#39;.format(*hlist)) xi = np.linspace(xlim[sign], -h[2]/h[0], 1000) ax.fill_between(xi, ylim[0], ylim[1], **fmt) else: ax.plot(x, (-h[2]-h[0]*x)/h[1], label=&#39;{}x+{}y+{}=0&#39;.format(*hlist)) ax.fill_between(x, (-h[2]-h[0]*x)/h[1], ylim[sign], **fmt) x, y = zip(*hs.intersections) ax.plot(x, y, &#39;o&#39;, markersize=8) . [[ 1. 1. 0.] [ 3. 1. 0.] [ 1. 4. 0.] [ 3. -0. 5.]] &lt;class &#39;numpy.ndarray&#39;&gt; . [&lt;matplotlib.lines.Line2D at 0x7f1a35df01d0&gt;] . The above intersection of halfspaces isn&#39;t what i expected. The complete intersection is the sector in the upper right hand corner. . Now if we are truly taking secret points $s_A, s_B$ at random in $ mathbf{R}^2$, then almost all random choices of affine functions will not separate the secrets. For example, given the homogeneity of $ mathbf{R}^2$, we can consider the secrets $s_A, s_B$ as being extremely close such that they are basically coincident, or at least as seen from a far distance. But then a random choice of affine function is extremely unlikely to contain the two points. Thus it appears that truly random choices of affine functions have essentially zero probability of separating the secrets. . This leads to the next step in our study of the Millionaire Problem, namely where the initial distribution on $ mathbf{R}^2$ is not necessarily uniform. E.g., perhaps we know that the secrets are distributed within a given large radius ball. If we have no other information about the secrets except that it lies somewhere on the large ball $D$, then one probabilistic strategy is to bisect the ball with affine functions, i.e. randomly guess an affine $ ell$ such that $ 1_D 1_{ ell&gt;0}$ and $1_D 1_{ ell &gt; 0}$ have equal area. . But what about privacy? In the previous case where the distribution was uniform on its support on $ mathbf{R}^2$, the privacy of the secrets was maintained so long as the affine functions were unbounded (from above and below). And the opponents would always guess such affine functions because there odds of correctly separating the secrets is significantly increased. But now its possible that the distribution will not be uniform on its support, and therefore the secrets can be learned with a reduction in uncertainty, i.e. perhaps we know that the opponents secret lies in a bounded set, or that 90% percent of the time the opponents secret lies in a given domain. . We remark that there is something like a &quot;maximum likelihood&quot; principle being used here. Now regarding privacy: if the domain $D$ is bounded, then depending on the distribution, the secret might have diminished privacy. This leads to Part 2 of our study, where the secrets are distibuted according to nonuniform distributions $ mu_A$, $ mu_B$ on $ mathbf{R}^2$. . a. What is the optimal strategy for nonuniform initial distributions $ mu_A, mu_B$ ? . b. Can we quantify the &quot;loss&quot; of privacy when the distributions are supported on unbounded domains versus bounded domains? . (To be continued...) .",
            "url": "https://jhmartel.github.io/fp/millionaire/secret/convex%20analysis/2022/04/20/MillionaireGame.html",
            "relUrl": "/millionaire/secret/convex%20analysis/2022/04/20/MillionaireGame.html",
            "date": " • Apr 20, 2022"
        }
        
    
  
    
        ,"post29": {
            "title": "Economics of Moving and Delivery.",
            "content": ". It&#39;s commonly reported that &quot;moving&quot; or &quot;la demenagement&quot; is among the most stressful events for consumers, c.f. &quot;Americans find moving more stressful than divorce&quot;. . What are the essential difficulties involved in moving? Here we assume we are moving a residential home. . Moving is labour intensive, involving hands-on moving of numerous boxes, furniture (tables, couches, bedsets, dressers), and fragile items (TVs, mirrors, lamps). . | Moving items is highly constrained, often involving heavy items being securely extracted from dwellings, and all this with zero damage to walls or floors. . | Another difficulty, perhaps unappreciated by the clients, is the necessity of packing the objects into the moving truck. . | This is a type of entropy problem, since the compressed volume of the moving truck restricts the possible range of motions of the objects. Moving requires alot of work and foresight to efficiently pack all the items securely in a truck. . Another view of the entropy difficulty is this: a house has many rooms, with the objects distributed sparsely throughout the space. However in a moving truck, all the objects need to be compressed into a single room (namely the box of the truck). . Can the clients estimate the volume of all their objects? . Is it possible for them to imagine all the objects to be relocated into a single room? . There is considerable stress involved in the action of, say, extracting heavy expensive &quot;precious&quot; furniture through various stairwells, corners, basements, etc.. The business of last-minute kijiji moving is even more stressful, for the clients are typically totally unprepared. For example, they might be selling a freezer located in the basement of a townhouse, with a very tight spiral staircase, and the client has arranged for its delivery to another basement appartment. The client might be moving their entire household, or only moving this single item. Or the client has received a new treadmill, and require its transport into the basement. . Building codes and standard construction methods make the extraction and deliveries somewhat easier. However extreme furniture pieces often push the movers ingenuity to the extreme, and requires alot of experience to immediately know which precise &quot;furniture ballet&quot; is required. As a rule of thumb : if an object makes three points of contact with the wall/floor/ceiling, then the object cannot be pushed any further without causing damage to the surroudings. . So how can the consumer save money when moving? . The answer is basically preparation. . To save money on moving: order a very large truck to make the loading and offloading easier, and pack as much as possible in regular cardboard boxes. This cannot be overremphasized: as much as possible, all the irregular objects should be packed into boxes, and into as many boxes as necessary. . Why? . Because it&#39;s expensive for the movers to waste their time in arranging and sometimes rearranging irregular shapes objects into the truck. The client should dissessemble the furniture as much as possible beforehand. Otherwise the movers need to spend working hours on the assembly/disassembly of furniture, and/or the preparation of more fragile objects. . Another difficulty is the patience and time required to safely move objects in tight spaces. For example, let us consider IKEA items which are very popular. One of the keys to IKEA&#39;s business model is that their furniture is transported and sold totally disassembled, and neatly packing into boxes. I think it&#39;s evident that tables and dressers dissassembled in boxes is more convenient for transport than fully assembled! In IKEA the consumer is required to read the instructions to assemble their items. . Another important constraint in domestic moves is: what is the pathway from the pickup to the truck, then from the truck to the dropoff. These are the environmental factors. Is the client moving in/out of a 20 storey building? Are we moving everything through elevators and long hallways? Is the client moving in/out of a basement? How many stairs? How close can the truck get to the unit? | Now IKEA and movers are not compatible. For the movrs might not be able to move IKEA items in their assembled state. Why? Because the IKEA items are fragile, and not designed to be moved. However IKEA items are also not easily disassembled without causing damage (typically cosmetic) to the items. Therefore movers are typically required to transport IKEA items &quot;as is&quot;, and this is a challenge. For assembled IKEA items have no strength, and are not at all designed for &quot;strongman&quot; transport. . Now for all the complicated parameters that exist in moving, in this article our goal is to reduce everything to the simplest variables. Basically, if a client calls and wants to move their entire household, and wants to have an estimate (or the moving manager wants an estimate for their own schedule), then we ask the following questions: . When is the last time the client has moved their household? . | If appplicable, ask how long it took and how many &quot;human labour hours&quot; were required for their last move? . | How many &quot;bedrooms&quot; are now being moved? . | (Basic logistics: pickup and dropoff addresses). . | What kind of heavy items? (Fridges, couches, exercises equipment, etc.). . | We make some comments: If the client is only moving a select number of items, then ask client &quot;how did the item get here, how many persons were involved, how long did it take&quot;. . Question 1. gives a lower bound (&quot;a floor&quot;) for the moving manager. Our experience is that people only accumulate items, even more items, after they move. When persons are settled in a location, then they collect more and more diverse items. This always adds to the time required and increases the complexity. Question 2. gives some idea, for example was it a team of four movers or two? Was it a big truck, or a smaller cube truck? Were there any incidents during that last move, particular events or damages to the items? . Question 2. is applicable only if the client can remember the last time the item was moved. However it does help manage the clients expectations. . Question 3. is important, especially for single persons who have recently moved themselves. For every single person, there is required approximately 6 to 10 total labour hours required. I.e. two movers require approximately 3 to 5 hours to move a single person (bachelor). This is large interval, which really determines on the particular circumstances. Heavy objects can take 15 -- 30 minutes per item to move per team of two. . Questions 4, 5 are rather standard. Some estimate of the travel time and circumstances is necessary. For example, if the clients are located in an appartment building, then there is frequently a large walking distance required, and if there is an elevator involved, then the time can be much longer. .",
            "url": "https://jhmartel.github.io/fp/2022/04/05/Economics_Moving_Delivery.html",
            "relUrl": "/2022/04/05/Economics_Moving_Delivery.html",
            "date": " • Apr 5, 2022"
        }
        
    
  
    
        ,"post30": {
            "title": "Six Design Principles for BoC's CBDC",
            "content": ". Disclaimer: the author is private citizen with zero affiliation with BoC. These brief articles are based on publically available documents and are completely speculative. . This article is brief. We simply enumerate six design principles which we believe are necessary for the BoC&#39;s CBDC. These principles are obviously necessary for the BoC to fulfill and satisfy it&#39;s own mandate and charter. Thus we propose the following six propositions need be satisfied by any proposal for BoC&#39;s CBDC. . (1) CBDC&#39;s must enable negative interest rates. (savings will be subject to time decay) . (2) CBDC&#39;s will be &quot;programmeable money&quot; (nonfungible) . (3) CBDC&#39;s will require static, hardcoded digital identities for the public. (strictly one wallet per person) . (4) CBDC must allow the BoC to exclusively control and monopolize monetary supply. (monetary supply, e.g. buying and selling treasury bonds is the central banks essential primary tool). . (5) BoC must require that only &quot;authorized participants&quot; are allowed to hold and trade base layer cryptocurrencies. (public prohibited from trading base tokens). . (6) The BoC&#39;s CBDC ledger will not be auditable to the public. (privacy is essential for security). . So we could implement the CBDC as smart constracts over a base layer, like Stellar (XLM) or Ripple (XRP) or Cardano (ADA). The smart contract would have strict conditions under which it&#39;s validator function returns true. These validators would need to depend on many &quot;world&quot; parameters, e.g. specific personal identity. That is, the validator would always need to refer to an outside centralized resource before validating the transaction. . End of article. . -JHM. .",
            "url": "https://jhmartel.github.io/fp/boc/cbdc/money/2022/03/26/6ixDesignPrinciplesOfBoCsCBDC.html",
            "relUrl": "/boc/cbdc/money/2022/03/26/6ixDesignPrinciplesOfBoCsCBDC.html",
            "date": " • Mar 26, 2022"
        }
        
    
  
    
        ,"post31": {
            "title": "Radioactive Gold Remediation",
            "content": ". Disclaimer: The author is interested in commodities and precious metals, and is generally interested in scientific questions, especially about the mysteries of atomic structure and the recent evidence of Saffire-Aureon project of remediating nuclear waste. The subject of this brief article is to simply highlight that directly testing this possibility on gold and silver bullion would significant diminish the threat of radioactive contamination of precious metals vaults. . This is a brief article. Jim Rickards has remarked in some of his videos, and his book &quot;The New Case for Gold&quot; that gold bullion bars (silver included) are at risk of radioactive contamination. For example, if a palette of bullion bars had been previously exposed to radio activity, then they could perhaps unwittingly be introduced into the general bullion vault and contaminate all the specimens. Or perhaps some radioactive material is released intentionally by bad actors and contaminating the bars. This risk is addressed by a recent amendment of the Royal Canadian Mint in its ETR (exchange-traded receipts) program. Risk of Radioactive Gold is Uninsurable . Presumably there would be an issue in the exchange and transit of such contaminated bullion. The radioactive halflife could potentially be millions of years. . However the recent experimental work of the Safire-Aureon Project has indicated that exposing radioactive materials to $H^+$ ions actually increases the decay rate! In otherwords, the decay rate depends on the atoms relative to their environment. Basically, if the contaminated bars were directly exposed to the electrical plasma environment of Aureon&#39;s &quot;star-in-a-jar&quot;, then the radioactivity could be significantly accelerated. The question is: how many orders of magnitude can the decay-rate be accelerated? Can a million year decay rate be reduced to 1 day? What would be the energy required to maintain $H^+$ bombardment? . Many more details are required, and this is only preliminary. -JHM. .",
            "url": "https://jhmartel.github.io/fp/eu/saffire/gold/pollution/2022/03/25/Radioactive_Gold_Remediation.html",
            "relUrl": "/eu/saffire/gold/pollution/2022/03/25/Radioactive_Gold_Remediation.html",
            "date": " • Mar 25, 2022"
        }
        
    
  
    
  
    
        ,"post33": {
            "title": "Brief Remarks on Designing Bank of Canada's CBDC",
            "content": ". Here we collect some hypotheses regarding the design goals of BoC (Bank of Canada)&#39;s CBDC (central bank digital currency). The author has no priviledged &quot;insider&quot; information, but reasons from the point-of-view of a public citizen who is well-informed of open source documentation, e.g. BoC working-papers, keyword = &quot;digital&quot; . So it&#39;s evident that BoC is actively investigating CBDCs, e.g. Jasper. For better or worse, the digitization of all assets marches on. Therefore if BoC wants to further digitize the monetary functions of the Bank of Canada, then alot of questions arise. First I would have to ask Why?, What are the goals of the CBDC? . For it naively appears that electronic money has already been achieved via Interac cards and the debit payment system. So what is deficient in the current form of electronic money? What is the CBDC going to solve for the BoC? What is the valued added? What is the value added for the bank versus for the public and individual?. . The entry of institutional investment into cryptocurrencies has provoked a reaction from central banks, BIS, IMF, around the world, namely because crypto&#39;s are monetary competitors. In otherwords, the public is increasingly aware of digital options for exchanging their fiat dollars into other anti-inflationary (and often speculative) digital tokens. The market cap of cryptos being actively traded and held on exchanges continues to grow. [insert specific figures]. . So competition has entered, and the central banks are reacting. Presently BoC estimates that only 5 per cent of Canadians have BTC (this is surely an underestimate), so there appears to be relatively small demand for cryptocurrencies, and almost zero demand for central bank digital currencies. . A person might be willing to invest in digital yuan, say, in the prospect of that foreign currency being backed by hard money gold reserves. But any foreign holder of digital yuan (especially at this relatively early stage) is subject to a large amount of spam, and inquiries, and phishing emails. In otherwords, acquiring the digital yuan comes with a diminished privacy and/or anonymity. This is something of a liability to the foreign holder of digital yuan. . We observe that the central banks, and specifically BoC, are keen to distinguish their CBDC&#39;s from the general cryptocurrencies. I.e. it is strongly discouraged to conflate or equivocate &quot;CBDCs&quot; with &quot;cryptos&quot;. This has various motivations, e.g. branding and marketing to control public opinion, but it&#39;s also an important technical distinction, for the CBDC will be strongly centralized with the BoC. . How? . Let&#39;s comment on centralized versus decentralized currencies. The decentralization in crypto refers to various properties of the networks. First, it refers to the fact that transactions can be confirmed by any node that either solves the nonce problem (in PoW) or is randomly elected (where the random choice is decentrally chosen). . In BTC&#39;s original design, there was an anti-inflationary mechanism, namely in the &quot;halving&quot; principle and miner rewards. The miners expend the energy and invest in the IT infrastructure to send and receive and confirm transactions in the ledger, and in most cases, the miner&#39;s receive token rewards for these services. Eventually, I think it&#39;s approximately 2150 AD, the miners will not receive any BTC rewards, and there will be a strict fee structure. I don&#39;t think anybody has any idea whether this will sufficiently motivate miners to preserve the integrity of the ledger, i.e. it might become more profitable to destroy the record of the ledger. . There are differences between proof-of-work (like original BTC) or proof-of-stake (like ETH, or ADA). But the point is that tokens are issued as rewards for miners and nodes. The various cryptocurrencies have different governance policies for how tokens are distributed, to prevent monetary inflation. This is indeed one of the standard arguments in favour of BTC as &quot;perfect money&quot; (although we don&#39;t necessarily agree with Saylor, Keiser, Breedlove, et.al. that BTC is &quot;perfect&quot; money.) . The Bank Of Canada has a monopoly on printing currency (i.e. fiat dollar bills, or electronic money), and it&#39;s extremely unlikely that they will surrender this monopoly in their CBDC. (It&#39;s basically certain, since this monopoly is the exclusive right and foundation of BoC&#39;s mandate). This is one reason why BoC cannot allow cryptocurrencies to compete with BoC&#39;s dollar. If people flee the Canadian dollar, then the main tool and mandate of the BoC (control of monetary supply) is annulled. . So we can assume that the goal of the CBDC is to have tokens which can only be issued by the centralized authority of BoC. This suggests that BoC will either develop their own network for CBDC transactions (extremely unlikely, i think), or they will attempt to build their token on top of another pre-existing network. For example, the BoC&#39;s CBDC&#39;s might essentially be smart contracts on top of either Ripple (XRP) ledger or Stellar (XLM) ledger, or something else. For security purposes I would recommend ADA (Cardano). . This raises the question What will the form of the CBDC smart contracts take? Reading the central bank literature, from our open source public civilian position, it appears that the BoC desires a CBDC with the following properties. . (1) CBDC&#39;s must enable negative interest rates. . (c.f. IMF&#39;s &quot;Breaking the zero bound&quot;, and Finance Minister Mme. Freeland&#39;s comments on &quot;savings as preloaded stimulus&quot;). This is effected by adding a time-decay factor on savings. . (2) CBDC&#39;s will be &quot;programmeable money&quot;, i.e. the CBDC will be nonfungible. . If the CBDC seeks maximal control, then purchases would be validated depending on the parameters (PersonalID, VendorID, ItemID, Price. The PersonalID will have an associated &quot;clearance&quot; or &quot;allowance&quot; of goods and services. These clearances/allowances are like digital permissions, saying &quot;This Person X is permitted to purchase Item Y from vendor Z at this time and date and location at this price up to this amount&quot;. . (3) CBDC&#39;s will require static, hardcoded digital identities for the public. . In otherwords, the pseudo-anonymity of general cryptocurrencies will be replaced with total disclosure of the users to the centralized authority. E.g., if dollar bills always had GPS and PersonalID embedded which exclusively is updated and permissioned by a centralized authority. Moreover the total history of the bill would be recorded in a ledger. . Cryptocurrencies (in general) can function with pseudo-anonymous addresses and multiple wallets for users. However the CBDC can only function with hard-coded digital identity for all public users. . (4) CBDC must allow the BoC to directly control and monopolize monetary supply. . So we must examine how the BoC will maintain CBDC supply and the &quot;fiat price&quot; of the CBDC on the chains. In principle, there exists nearly zero cost for the BoC to issue new money, i.e. they &quot;print&quot; money (brrrrr) either with electronic entries or by increasing the physical printed money supply (at some fixed cost). . Moreover, there is another issue. If the CBDC is a contract on top of, say, the Ripple ledger, then what prevents users from bypassing the BoC CBDC and directly acquiring XRP tokens? For the XRP tokens will be &quot;harder money&quot; and more functional than the CBDC, and there will be a flow from CBDC to XRP via Gresham&#39;s Law. This will require some law or federal emergency mandate that . (5) BoC must require that only authorized participants will be allowed to hold and trade base layer cryptocurrencies. . And indeed this is a trend observed around the world. . There&#39;s more to say, but I&#39;ll keep it short for now. . JHM. .",
            "url": "https://jhmartel.github.io/fp/money/boc/cbdc/2022/03/24/CBDC_Opinion.html",
            "relUrl": "/money/boc/cbdc/2022/03/24/CBDC_Opinion.html",
            "date": " • Mar 24, 2022"
        }
        
    
  
    
  
    
  
    
  
    
        ,"post37": {
            "title": "Closing Steinberg, Part 1. Mapping Class Group",
            "content": "We present the formal definition of Closing Steinberg symbols. . Let $ Gamma$ be a group acting on a space $X$ by group action $X times Gamma to X$. If $P subset X$ is a subset of $X$, then a finite subset $I$ of $ Gamma$ is said to formally close $P$ if the iterated symmetric difference of $ { gamma. P ~|~ gamma in I }$ vanishes (equal to empty set $ emptyset$). . N.B. The vanishing of the iterated symmetric difference means the chain sum $ gamma.P$, for $ gamma in I$, vanishes over mod 2 coefficients. I.e. every element in the $I$-translates of $P$ occurs an even number of times. . The above is the abstract formulation of a problem we call Closing the Steinberg symbol. In our applications the subset $P$ is a panel representing the convex hull of a sphere at infinity which is called the &quot;Steinberg symbol&quot;. . Now the key to Closing Steinberg (CS) is to find nontrivial formal solutions. We will illustrate with the specific group $ Gamma = Mod(S)$ which is the mapping class group of a compact hyperbolic surface $S$. We will begin with genus $g(S)=2$. . Now we introduce the basic functions using Mark C. Bell&#39;s curver: . Now we make the basic definitions of the reference pant, and the mapping class elements $ zeta, nu, mu$. . pant={a,b,c} ## pant={a,b,c} is the standard pant. zeta=a*e*c*f*b ## zeta is the order 6 element in MCG arising from chain relation. ## nu=a*e*c*f ## nu is order 10 element in MCG mu=nu**4 ## mu is order 5 element in MCG. . We define $ xi$ as the union of the standard pair of pants with its $ mu$-translate. The $ mu$-translate of $ {a,b,c }$ is a pair of pants dual to $ {a,b,c }$. . The mapping class element $ mu$ is an order 5 element in $Mod(S_2)$. We propose that the powers of $ mu$, namely $I= {Id, mu, mu^2, mu^3, mu^4 }$, formally close the symbol $ xi$. This solution will be nontrivial because we will establish that $ mu^i xi = mu^j xi$ if and only if $i=j$. Thus the $I$-translates of $ xi$ are distinct, while the chain sum $ sum_{i=0}^4 mu^i xi$ vanishes over ${ bf{Z}}/2$ coefficient. . ## xi is obtained by joining the initial pant p with its mu translate. xi=pant|Translate(mu, pant) ## important to verify that pant and the mu-translate are disjoint. ## Ad(mu,pant) is &quot;opposite pair of pants&quot; print(&quot;The mu translate of the standard pant is disjoint from pant. &quot;, pant &amp; Translate(mu, pant) == set()) print() M0=xi M1=Translate(mu,xi) M2=Translate(mu**2,xi) M3=Translate(mu**3,xi) M4=Translate(mu**4,xi) ## The following proves that all the symbol translates are nontrivial, and there is no complete coincidence ## between the translated symbols. print(&quot;The mu translates of xi are all pairwise distinct:&quot;, M0!=M1 and M0!=M2 and M0!=M3 and M0!=M4 and M1!=M2 and M1!=M3 and M1!=M4 and M2!=M3 and M2!=M4 and M3!=M4 ) print() ## The following proves that the total chain sum of the translated symbols vanishes mod 2. ## I.e. the iterated symmetric difference of the translated symbols is equal to empty set. print(&quot;The iterated symmetric difference of the mu translates is empty.&quot;, ((((M0^M1))^M2)^M3)^M4 ==set()) print() print(&quot;The mu-orbit of xi is supported on ten curves.&quot;, 10==len(M0|M1|M2|M3|M4) ) print() print(&quot;Therefore we find I={Id, mu, mu**2, mu**3, mu**4} is a formal solution to Closing the Steinberg symbol xi in genus two.&quot;) print(&quot;&quot;) . The mu translate of the standard pant is disjoint from pant. True The mu translates of xi are all pairwise distinct: True The iterated symmetric difference of the mu translates is empty. True The mu-orbit of xi is supported on ten curves. True Therefore we find I={Id, mu, mu**2, mu**3, mu**4} is a formal solution to Closing the Steinberg symbol xi in genus two. . So we have found a formal solution $I$ to CS. For applications we need further verify that $I$ satisfies further geometric properties. Specifically we need establish: . the $I$-translates of $ xi$ have a well-defined convex hull $F:=conv(I. xi)$ in $Teich(S)$. | the $ Gamma$-translates of $F$ generate a chain sum $ underline{F}:= sum_{ gamma in Gamma} gamma.F$ with well separated gates equal to the $ Gamma$-translates of $ xi$. | . Informally the idea is that $ xi$ represents a &quot;panel&quot; $P$, and $I$ closes the panel in the sense that the $I$-translates of the panel assemble to a closed ball. (Similar to how the (triangular, hexagonal) panels of a soccer ball assemble to form the closed ball). . But we must further study the $ Gamma$-translates of the ball itself, i.e. of the convex hull $F$. Most important for our setting is that the intersections of the various translates $ gamma F cap gamma&#39; F$ have a &quot;standard form&quot;, namely isometric to $ xi$ (the panel $P$). . Remark. It&#39;s not clear whether the above verification of ``well-separated gates&quot; can be performed in curver. While we are capable of considering the $ Gamma$ action on the elements of $I. xi$, we cannot necessarily compute the intersections $F cap gamma F$. The issue is that $F cap gamma F$ can intersect &quot;at-infinity&quot; (i.e. be asymptotic) without the convex hulls having an intersection in the interior of $Teich(S)$. Formally the solutions to CS solve a problem at infinity, but the next step is to study the solutions in the interior, and this becomes more geometric. .",
            "url": "https://jhmartel.github.io/fp/closing%20steinberg/curver/mcg/2022/02/28/ClosingSteinberg_Intro.html",
            "relUrl": "/closing%20steinberg/curver/mcg/2022/02/28/ClosingSteinberg_Intro.html",
            "date": " • Feb 28, 2022"
        }
        
    
  
    
        ,"post38": {
            "title": "Positronium Part I.",
            "content": ". Today we begin the study of Weber&#39;s potential in the isolated two-body system consisting of an electron and positron pair $e^-$ and $e^+$. We assume the particles $e^ pm$ have equal mass $m=m_{e^{ pm}}$. The reduced mass is concentrated at the centre-of-mass $ mu=m/2$. . Weber&#39;s force is attractive between the pair $e^ pm$ at all distances. . The particles $e^ pm$ do not indefinitely spiral inwards. Simulations indicate that the radial distance between $e^ pm$ stays strictly bounded between two upper and lower limits $$0 &lt; r_{lower} leq r leq r_{upper} &lt; + infty .$$ This is rigorously proved in Weber-Clemente, 1990.pdf). . If the electron is indivisible particle, then the above two-body problem models a pair $e^-$ and $e^+$ of isolated electron and positron. . But do the particles $e^ pm$ ever &#39;collide&#39; and annihalate? . In the standard physics textbooks, it seems well known that annihalation between $e^ pm$ occurs and two gamma rays are ejected in opposite directions when $e^ pm$. conserving momentum, etc., and converting all their mass into energy. Thus it&#39;s determined that two gamma rays of energy $0.511 keV$ are released, where Einstein&#39;s formula $E=m_ec^2$ is applied, where $m_e$ is the reduced mass. [ref] The annihalation of $e^ pm$ is apparently an experimental test of the validity of Einstein&#39;s &quot;mass-energy&quot; hypothesis. . But what does Weber&#39;s potential say about the annihalation of $e^+$ and $e^-$ ? . If we know the centre of mass has zero net force, then we can replace the positions $r_1$, $r_2$ of the particles by their relative distance $r_{12}$ from the centre of mass. This yields $$r_1=R + frac{m_2}{m_1+m_2} r_{12}$$ and $$r_2=R - frac{m_1}{m_1+m_2} r_{12}.$$ . Applying Newton&#39;s Second Law that $F_{21}=-F_{12}$ yields the following equation for $r_{12}&#39;&#39;$: $$ mu . r_{12}&#39;&#39; = F_{21},$$ where $ mu$ is the reduced mass of the system, namely $ mu= frac{m_1 m_2}{m_1+m_2}=0.5$. . In the following equations we use numpy.odeint to solve Weber equations of motion of the relative distance $r_{12}$. Therefore we have reduced the two-body problem to a one-body problem. This is a standard reduction. . Given the solution for $r_{12}$, how do we reconstruct the paths/positions of the particles $r_1$, $r_2$ ? Answer: via the relation $r_1=R+ frac{m_2}{m_1+m_2} r_{12}$ and $r_2=R- frac{m_1}{m_1 + m_2}r_{12}$. . Now the relative distance $r_{12}$ is a type of radial distance, and if $r, omega$ is spherical coordinates, then we have $$r&#39;^2=|v|^2= x&#39;^2+y&#39;^2+z&#39;^2=(r&#39;)^2+r^2 ( theta&#39;)^2. $$ The above formula is the usual $|v|^2=v_r^2+v_t^2$, and the tangent velocity $v_t$ satisfies $v_t=r theta&#39;$, where $ theta&#39;$ is the angular velocity. . The conservation of angular momentum says that the angular moment $L= mu r times v$, where $v$ is the linear velocity of $r$, is constant along the motion. Moreover one has $$|L|= mu r^2 theta&#39;.$$ Thus we find the formula $$ theta&#39;= frac{|L|}{ mu r^2}.$$ This implies $$T= frac{ mu}{2}v^2= frac{ mu}{2}[(r&#39;)^2+ frac{|r times v|^2}{r^{2}}]$$ represents the kinetic energy of the system. . The conservation of energy says $T+U$ is constant along trajectories. . # Here we define basic functions. def cross(v1, v2): x1, y1, z1 = v1 x2, y2, z2 = v2 return [y1*z2 - z1*y2, -(x1*z2 - z1*x2), x1*y2 - y1*x2 ] def rho(rel_position): x,y,z = rel_position return (x*x+y*y+z*z)**0.5 def dot(vector1, vector2): x1, y1, z1 = vector1 x2, y2, z2 = vector2 return x1*x2+y1*y2+z1*z2 def rdot(position, vector): return dot(position, vector)/rho(position) def norm(rel_velocity): return rho(rel_velocity) mu=0.5 ## reduced mass of the system. We assume m1 and m2 are equal, hence mu=1/2. c=1.0 ## speed of light constant in Weber&#39;s potential # Define the angular momentum def AngMom(rel_position, rel_velocity): return cross(rel_position, rel_velocity) def L(rel_position, rel_velocity): return norm(cross(rel_position, rel_velocity)) # Linear Kinetic Energy def T(rel_position, rel_velocity): vt = norm(cross(rel_position, rel_velocity)) # next formula decomposes v^2=(vr)^2+(vt)^2, where vt=r*θ&#39;=|L|/(mu*r) return (mu/2)*(rdot(rel_position, rel_velocity)**2) + (mu/2)*(rho(rel_position)**-2)*(vt**2) ## Weber Potential Energy ## Negative sign given -1=q1*q2 def U(rel_position, rel_velocity): x,y,z = rel_position vx,vy,vz = rel_velocity rdot=dot(rel_position, rel_velocity)/rho(rel_position) return -(1/rho(rel_position))*(1-(rdot*rdot)/2) . . ## import the basic packages import numpy as np from scipy.integrate import odeint, solve_ivp import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D ## Integrating two-body isolated system of oppositely charged particles ## i.e. positron+electron pair. ## The product of the charges q1*q2 is factor in Weber&#39;s force law, and appears twice ## in the formula of Newton&#39;s F=ma. def weber(t, rel_state): x, y, z, vx, vy, vz = rel_state r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r A=(-1)*r**-2 ## minus sign from q1*q2 B=1-(rdot*rdot)/2 C=(mu+((c*c*r)**-1))**-1 ## +plus instead of -minus. dxdt = vx dydt = vy dzdt = vz dvxdt = (x/r)*A*B*C dvydt = (y/r)*A*B*C dvzdt = (z/r)*A*B*C return [dxdt, dydt, dzdt, dvxdt, dvydt, dvzdt] t_span = (0.0, 100.0) t = np.arange(0.0, 100.0, 0.1) y1=[2.0,0,0,] # initial relative position v1=[-0.4, 0.4, 0] # initial relative velocity result = odeint(weber, y1+v1, t, tfirst=True) #here odeint solves the weber equations of motion relative y1+v1 for t. Energy=T(y1,v1) + U(y1,v1) print(&#39;The initial total energy T+U is equal to:&#39;, Energy) print(&#39;The initial angular momentum is equal to&#39;, norm(AngMom(y1,v1))) fig = plt.figure() ax = fig.add_subplot(1, 2, 1, projection=&#39;3d&#39;) ax.plot(result[:, 0], result[:, 1], result[:, 2]) ax.set_title(&quot;position&quot;) ax = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax.plot(result[:, 3], result[:, 4], result[:, 5]) ax.set_title(&quot;velocity&quot;) . . The initial total energy T+U is equal to: -0.37999999999999995 The initial angular momentum is equal to 0.8 . Text(0.5, 0.92, &#39;velocity&#39;) . What does the above plot demonstrate? . It reveals a precession motion around the centre of mass. This is not predicted by Coulomb&#39;s force, which bounds the trajectories to elliptical orbits like Newton&#39;s Law of Gravitation. . The force is central, therefore we have conservation of angular momentum, and implies the system is constrained to a plane, namely orthogonal to the angular moment of the system. . Moreover the system satisfies a conservation of linear momentum, namely the sum $T+U$ is constant. . Problem: Verify Assis-Clemente&#39;s 1990 formula for the lower and upper limits of the relative distance along the orbits. | . import matplotlib.pyplot import pylab r_list=[] for j in range(1000): sample_position=[result[j,0], result[j,1], result[j,2]] sample_velocity=[result[j,3], result[j,4], result[j,5]] r_list.append( ( int(j), rho(sample_position) ) ) prelistr = list(zip(*r_list)) pylab.scatter(list(prelistr[0]),list(prelistr[1])) pylab.xlabel(&#39;time&#39;) pylab.ylabel(&#39;rho&#39;) pylab.title(&#39;Solutions have Upper and Lower Limits&#39;) pylab.show() #TU_list=[] #for j in range(1000): # sample_position=[result[j,0], result[j,1], result[j,2]] # sample_velocity=[result[j,3], result[j,4], result[j,5]] # TU_list.append( # (rho(sample_position),T(sample_position,sample_velocity)+U(sample_position, sample_velocity))) #prelist1 = list(zip(*TU_list)) #pylab.scatter(list(prelist1[0]),list(prelist1[1])) #pylab.xlabel(&#39;distance r&#39;) #pylab.ylabel(&#39;T+U&#39;) #pylab.title(&#39;&#39;) #pylab.show() # The plot below demonstrates the conservation of angular momentum. # Note that rdot is directly equal to the sample_velocity. I.e. there is # no need to define rdot=v.hatr/r. This was error. #A_list=[] #for j in range(1000): # sample_position=[result[j,0], result[j,1], result[j,2]] # sample_velocity=[result[j,3], result[j,4], result[j,5]] # A_list.append( # (rho(sample_position), norm(cross(sample_position, sample_velocity)) ) ) #prelist2 = list(zip(*A_list)) #pylab.scatter(list(prelist2[0]),list(prelist2[1])) #pylab.xlabel(&#39;rho&#39;) #pylab.ylabel(&#39;Angular Momentum&#39;) #pylab.title(&#39;Conservation of Angular Momentum&#39;) #pylab.show() #rho_list=[] #for j in range(180): # rho_list.append( # (int(j), rho([result[j,0], result[j,1], result[j,2]]), # ) # ) . . from sympy import * t=symbols(&#39;t&#39;) m=symbols(&#39;m&#39;) c=symbols(&#39;c&#39;) r=Function(&#39;r&#39;)(t) P=Function(&#39;P&#39;)(r,t) F=Function(&#39;F&#39;)(r,t) U=-(r**-1)*(1-((r.diff(t))**2)*(2*c*c)**-1) F=(-1)*(U.diff(t))*((r.diff(t))**-1) pprint(simplify(U)) print() pprint(simplify(F)) ## symbolic computation of the Force law. . .",
            "url": "https://jhmartel.github.io/fp/weber/positronium/two-body/2022/02/22/Positronium_Part1.html",
            "relUrl": "/weber/positronium/two-body/2022/02/22/Positronium_Part1.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post39": {
            "title": "One-Dimensional Ampere Force",
            "content": ". One-Dimensional Ampere . We continue with our discussion of Ampere&#39;s force, and consider the special case where the current elements are all pairwise parallel and colinear. In otherwords we assume the current elements belong the a line $ ell$ in $ bf{R}^3$. For simplicity we consider the current elements belonging to the $z$-axis. . The state of a current element on the $z$-axis is then represented by a real-valued function $v(x)$ representing the velocity of the current element. Likewise we interpret $|v(x)|$ as the intensity of the current element. Ampere&#39;s force law in the one-dimensional case then becomes: . $$F_{y ~ text{on}~x} =|v(x)|~|v(y)|~v(x)~v(y) ~ nabla_1 ( frac{1}{r}) . $$ . If we integrate the force $F$ over the entire line we obtain $$F_{ text{current on}~ x} = |v(x)|~ v(x) ~[ nabla_1 cdot int dy~ frac{|v(y)| v(y)}{x-y} ]. $$ . The hypothesis of the force being balanced on the line is equivalent to the vanishing $$ nabla_1 ~ int dy frac{|v(y)|~ v(y)}{x-y}= frac{d}{dx} ~ int dy frac{|v(y)|~ v(y)}{x-y}=0 .$$ Equivalently, the force is balanced if the integral $$H(v)(x)=h(x):= int_{- infty}^{+ infty} dy~ frac{|v(y)|~ v(y)}{x-y} $$ is a constant function of $x$ wherever $v(x) neq 0$. . N.B. The integral defining $H(v)$ is taken over the entire real line. We observe that there is no absolute value sign on the denominator $x-y$. This is to include the direction of $ hat{r}$ in the original Ampere law. . Our definition of $H$ is Hilbert&#39;s singular integral transform for real-valued functions. Strictly speaking, the singular integral does not always integrate to a finite number. In the literature, the integral is frequently replaced with a Cauchy principal value. This is related to the function $ frac{1}{y}$ being not absolutely integrable on the real line. However there is sufficient cancellation in the integral to obtain a well-defined principal value. . If the current $v$ is everywhere constant, then symmetry shows the integral $H(v)$ is identically zero, and therefore the total force $F$ of the current is everywhere zero. Therefore we find $v=constant$ is a solution to the balance equation. . N.B. Most discussion of the Hilbert transform is restricted to square-integrable functions on the line $L^2( bf{R})$. On $L^2$ one finds the Hilbert transform defines a unitary operator and is an anti-involution, satisfying $H circ H=-Id$. However the only $L^2$ constant function is the zero function. . Question: Are there any time-independant solutions to the balance equation, i.e. does there exist nonconstant $v in L^ infty( bf{R})$ with $H(v)$ constant? . Question: Can we find time-dependant solutions to the balance equation? I.e. find functions $v_t in L^ infty$ such that $H(v_t)$ is constant for all time $t$. . Remark. The Hilbert transform (modulo homothety) is the only singular operator in one-dimensions which commutes with the affine translation and affine dilatation. This corresponds to the Ampere force commuting with the affine change of variables $x mapsto ax+b$. Since Ampere&#39;s force is relational, it seems proper that the corresponding Hilbert transform also possess these same properties. . Remark. It appears that the only Borel-Radon measures $ mu$ on the real line $ bf{R}$ which have a constant Hilbert transform $H( mu)=c$ are the uniform measures $ mu=const.dy$. If this remains true, then we identify the kernel of $H circ H$ with the uniform measures on $ bf{R}$. .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/02/18/One_Dimensional_Ampere.html",
            "relUrl": "/fastpages/jupyter/2022/02/18/One_Dimensional_Ampere.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post40": {
            "title": "Weber's Critical Radius, Work, and E=mc2 Formulas",
            "content": ". In the [previous post] we discussed the equations of motion of a charged particle in free fall w.r.t. Weber&#39;s potential $U$. Now we consider the problem of work, and the problem of moving particles &quot;upstream&quot; of the potential. . Recall the Coulomb-type expression that &quot;opposite charges attract&quot; and &quot;like charges repel&quot;. If we arranged some charged particles on a plate, then in a short time these particles would either be repelled outwards to the boundary of the plate or the opposite charges would cancel on the interior. Again this is with respect to the Coulomb model $V(r)=1/r$. . But Weber&#39;s model has the following amazing prediction: &quot;opposite charges attract except at a small critical distance $r=r_c$ where the charges acquires a negative inertial mass and the Weber force becomes repulsive&quot;. Furthermore, &quot;like charges repel except at a small critical distance where the Weber force becomes attractive&quot;. This latter prediction leads to Weber&#39;s *planetary model of the atom&quot;. [Insert ref]. . In the (cgs) units the critical radius $r_c$ is computed as $r_c= frac{1}{m c^2}$ where $m$ is the relative inertial mass of one of the particles. The proof of this formula is an application of Newton&#39;s second law, that $m a = bf{F}$ where $ bf{F}$ is Weber&#39;s force. [insert ref] . Now the question arises: if we have two identical but opposite electrically charged particles, say $(-1)e$ and $(+1)e$, where $e$ is a small mass, then how much work is required to drive $(-1)e$ and $(+1)e$ to within Weber&#39;s critical radius $r_c$? . Weber comments in his final memoir [ref] that an infinite amount of work would be required, however no technical details are provided. In fact this author thinks its evident that a large finite amount of work is required, and the computation of this work required is the subject of this post. . How to compute Work: We imagine a particle moving through a potential $U$ along a trajectory $ gamma.$ Our goal is to determine how much work is required to breach (&quot;pass through&quot;) the critical radius $r_c= frac{1}{mc^2}$ in units where $4 pi epsilon_0 = 1$. . We begin assuming the particle&#39;s trajectory is rectilinear. This means the tangent $ gamma&#39;$ and $ gamma$ are parallel for all values of the parameter. Moreover we can use the Riemannian idea of parameterizing the curve by arclength. Then we ask how much work is required to move the particle through $U$ at a constant rate of speed, namely $|| gamma&#39;||=1$. . Weber&#39;s force is conservative, so there is some path independance, however the terminal conditions are not fixed a priori. It might happen that breaching the Weber radius is easier if the particles velocity is nearly parallel to the &quot;virtual&quot; surface of the Weber&#39;s critical radius. That is to say, the curvature term $r r&#39;&#39;/c^2$ in Weber&#39;s force law might sometimes reduce the work needed, depending on the sign. . In terms of the relational variables recall the useful formula/definition $$ r&#39; = frac{ bf{r} cdot bf{r&#39;}}{r}.$$ In our setting we find $$r&#39;= frac{ gamma cdot gamma&#39;}{|| gamma||}.$$ Applying the Cauchy-Schwartz inequality, we find $$r&#39;=1.$$ I.e. equality is obtained in Cauchy-Schwartz because $ gamma, gamma&#39;$ are parallel by hypothesis and $|| gamma&#39;||=1$. . Moreover the rectilinear motion implies $r&#39;&#39;=0$, i.e. the trajectory has zero curvature, since the direction of the particle does not change. Therefore Weber&#39;s force leads to work being computed by the integral $$W=(1- frac{1}{2c^2}) int_{+ infty}^{r_c} frac{1}{r^2} dr = (1- frac{1}{2c^2})mc^2=mc^2-m/2.$$ So what is the total energy of the above system? The total energy is the work required $mc^2 - m/2$ plus the initial kinetic energy $T_i=m| gamma&#39;|^2/2=m/2$. Therefore the total energy required to breach the Weber critical radius is $E=W+T_i=mc^2$. . N.B. All the while our particles are &quot;travelling upstream&quot;. So the above computation indicates that if a particle is given sufficient energy (i.e. sufficient kinetic energy, then the particle could breach the Weber critical radius, and arriving at the Weber radius with almost zero kinetic energy). . N.B. The integral representing $W$ is parameterization independant. Therefore the work required by the unit-parameterized path is not overly specialized, but represents the general computation. . The above discussion was restricted to rectilinear trajectories. But the possibility remains that curvilinear (&quot;spiralling&quot;) trajectories require less work to breach the Weber radius. Thus while it appears that breaching the Weber radius via rectilinear paths requires large energy $ approx mc^2$, perhaps the spiralling paths -- where the curvature term maintains a definite sign -- are the more interesting. . Problem: Determine the minimum energy required for an isolated two-body system to breach the Weber critical radius. . Answer: The minimum energy required to breach the critical radius is $E=mc^2$. . This is consequence of the fact that $ bf{F}$ is a conservative force, therefore dependant only on the initial and terminal states, and not the path taken. Moreover the work done by a particle traversing a path $ gamma$ depends only on the difference in potential energies. This implies that the above evaluation in the rectilinear case is essentially the same for all paths from some initial point to to within the critical radius. . Is the energy spectrum within the critical radius continuous? (Yes, i think the orbits can have arbitrary energy within the radius). | How to relate Birkeland currents (FFAC) to Weber&#39;s potential? Don Scott considers the plasma cylinder, with a stead current. Then the min energy state has zero internal pressure, therefore the net internal energy of the plasma cylinder needs be minimized, i.e. vanishing. Difficulty: without the Maxwell equations we do not know relationship between the current flow $I d ell=qdv$. This equality is Weber&#39;s hypothesis, i.e. a current flow is equivalent to a charge in motion, where $dv$ represents the instantaneous velocity of the charged particle. |",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/02/16/WeberEMC2.html",
            "relUrl": "/fastpages/jupyter/2022/02/16/WeberEMC2.html",
            "date": " • Feb 16, 2022"
        }
        
    
  
    
        ,"post41": {
            "title": "Phipp's Potential",
            "content": "Here we examine Phipp&#39;s potential $P= frac{1}{r} sqrt{1- frac{r&#39;}{c}}$. It&#39;s clear that Phipp&#39;s potential imposes an upper bound on the velocities of particles in free fall relative to $P$. But while Phipp&#39;s potential guarantees subluminal velocities, it does not appear to satisfy a conservation of energy, and the quantity $T+P$ is not constant along trajectories. . The Phipp potential can induce negative effective mass at small distances, similar to the case of Weber&#39;s potential. However the critical radius depends on the relative velocity of the particles. By contrast Weber&#39;s critical radius depends only on the mass $m$ and $c^2$. . These two facts are demonstrated in the cells below, and demonstrates some severe disadvantages to Phipp&#39;s potential. We are therefore inclined to investigate Weber&#39;s potential as a model of electrodynamics. . from sympy import * t=symbols(&#39;t&#39;) m=symbols(&#39;m&#39;) c=symbols(&#39;c&#39;) r=Function(&#39;r&#39;)(t) P=Function(&#39;P&#39;)(r,t) F=Function(&#39;F&#39;)(r,t) P=(r**-1)*sqrt(1-r.diff(t)/c) F=(-1)*(P.diff(t))*((r.diff(t))**-1) pprint(simplify(P)) print() pprint(simplify(F)) ## symbolic computation of the Force law. . ______________ ╱ d ╱ c - ──(r(t)) ╱ dt ╱ ──────────── ╲╱ c ──────────────────── r(t) 2 d r(t)⋅───(r(t)) 2 ⎛ d ⎞ d dt ⎜c - ──(r(t))⎟⋅──(r(t)) + ────────────── ⎝ dt ⎠ dt 2 ──────────────────────────────────────── ______________ ╱ d ╱ c - ──(r(t)) ╱ dt 2 d c⋅ ╱ ──────────── ⋅r (t)⋅──(r(t)) ╲╱ c dt . import numpy as np from scipy.integrate import odeint, solve_ivp import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D m=0.5 ## mass of second test particle c=1.0 ## speed of light constant in Weber&#39;s potential ## Kinetic Energy: we use the naive expression for vis viva. Is it correct? def T(vx, vy, vz): return m*(vx*vx+vy*vy+vz*vz)/2 ## Phipp&#39;s Potential Energy def P(x,y,z,vx,vy,vz): r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r return (r**-1)*(1-rdot/c)**0.5 ## Integrating equations of motion relative Phipp&#39;s Force Law. def phipp(t, state): x, y, z, vx, vy, vz = state r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r A=r**-2 B=(1-rdot)**0.5 C=(m- ( 2*r*rdot*((1-rdot)**0.5) )**-1 )**-1 dxdt = vx dydt = vy dzdt = vz dvxdt = (x/r)*A*B*C dvydt = (y/r)*A*B*C dvzdt = (z/r)*A*B*C return [dxdt, dydt, dzdt, dvxdt, dvydt, dvzdt] t_span = (0.0, 1000.0) t = np.arange(0.0, 1000.0, 0.001) y0=[10, 0.3, 0, -0.2, -0.9, .1] ## initial state of the system y0=[x, y, z, vx, vy, vz] result = odeint(phipp, y0, t, tfirst=True) Energy=T(y0[3], y0[4], y0[5] ) + P(y0[0],y0[1],y0[2],y0[3],y0[4],y0[5]) print(&#39;The initial total energy T+P is equal to:&#39;, Energy) print(&#39;The luminal energy is:&#39;, m*c*c/2) print(&#39;The energy is subliminal:&#39;, Energy &lt; m*c**2 /2) r = ( y0[0]*y0[0] + y0[1]*y0[1] + y0[2]*y0[2])**0.5 rdot = ( y0[0]*y0[3] + y0[1]*y0[4] + y0[2]*y0[5] ) /r print(&#39;Phipp`s critical radius is equal to:&#39;,(m- ( 2*r*rdot*((1-rdot)**0.5) )**-1)**-1) ## N.B. Phipp&#39;s critical radius depends on the distance and velocity! print(&#39;The initial radius r and velocity r` is within Phipp`s critical distance:&#39;, r &lt; (m*c*c)**-1 ) fig = plt.figure() ax = fig.add_subplot(1, 2, 1, projection=&#39;3d&#39;) ax.plot(result[:, 0], result[:, 1], result[:, 2]) ax.set_title(&quot;position&quot;) ax = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax.plot(result[:, 3], result[:, 4], result[:, 5]) ax.set_title(&quot;velocity&quot;) . The initial total energy T+P is equal to: 0.3257156133373373 The luminal energy is: 0.25 The energy is subliminal: False Phipp`s critical radius is equal to: 1.4309087883533131 The initial radius r and velocity r` is within Phipp`s critical distance: False . Text(0.5, 0.92, &#39;velocity&#39;) . import matplotlib.pyplot import pylab def rho(x,y,z): return (x*x+y*y+z*z)**0.5 v_list=[] for j in range(4000): v_list.append( (rho(result[j,0], result[j,1], result[j,2]), T(result[j,3], result[j,4], result[j,5])) ) P_list=[] for j in range(4000): P_list.append( (rho(result[j,0], result[j,1], result[j,2]), P(result[j,0],result[j,1],result[j,2],result[j,3],result[j,4],result[j,5])+T(result[j,3], result[j,4], result[j,5])) ) prelist1 = list(zip(*v_list)) pylab.scatter(list(prelist1[0]),list(prelist1[1])) pylab.show() prelist2 = list(zip(*P_list)) pylab.scatter(list(prelist2[0]),list(prelist2[1])) pylab.show() .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/02/11/PhippPotential.html",
            "relUrl": "/fastpages/jupyter/2022/02/11/PhippPotential.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post42": {
            "title": "Letter To Prof. Assis.",
            "content": "2022-02-10-Verifying Conservation of Energy with Python. . Dear Professor Assis, . I am currently studying the Weber potential $U= frac{e_1 e_2}{r} (1- frac{r&#39;^2}{2c^2})$ in python, and specifically looking at the equations of motion of an isolated two-body system. For computation i have chosen one particle to be the origin, and look to solve the equations of motion for the second particle. In computations I take $e_1=e_2=1$ and $c=1$, and the relative mass of the second particle to be $m=0.5$. (Is that mass to small?) . I am fairly confident that I have correctly integrated the equations of motion using python&#39;s odeint. . However I am not able to satisfactorily verify conservation of energy $d(T+U)=0$ along the trajectories, and I am not confident that I have the correct expression for kinetic energy $T$, i use the naive expression $T=mv^2/2$, where $v^2$ is computed in usual way as sum of squares of the velocities of the second particle of mass $m=0.5$. When I use the above expression for $T$ I find the sum $T+U$ is not constant along solutions to the equations of motion $mr&#39;&#39;=ma=F$, where $F=- hat{r} frac{dU}{dr}$ is Weber&#39;s force. . From your book on &quot;Relational Mechanics&quot; i understand that kinetic energy is more properly defined as the interaction energy of the particle with the stars at infinity. But is the naive kinetic energy $T=mv^2/2$ the correct expression for the isolated two-body system? . import numpy as np from scipy.integrate import odeint, solve_ivp import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D m=0.5 ## mass of second test particle c=1.0 ## speed of light constant in Weber&#39;s potential ## Kinetic Energy: we use the naive expression for vis viva. Is it correct? def T(vx, vy, vz): return m*(vx*vx+vy*vy+vz*vz)/2 ## Weber Potential Energy def U(x,y,z,vx,vy,vz): r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r return (r**-1)*(1-(rdot**2)/2) ## Integrating equations of motion relative Weber&#39;s Force Law. ## I think I have implemented everything correctly. The formula for rdot is from Relational Mechanics, pp.168 ## but perhaps I have the wrong formula for r&#39;&#39; and have not applied Newtons second law F=mr&#39;&#39; correctly? def weber(t, state): x, y, z, vx, vy, vz = state r=(x*x + y*y + z*z)**0.5 rdot=(x*vx+y*vy+z*vz)/r A=r**-2 B=1-(rdot*rdot)/2 C=(m-((c*c*r)**-1))**-1 dxdt = vx dydt = vy dzdt = vz dvxdt = (x/r)*A*B*C dvydt = (y/r)*A*B*C dvzdt = (z/r)*A*B*C return [dxdt, dydt, dzdt, dvxdt, dvydt, dvzdt] t_span = (0.0, 40.0) t = np.arange(0.0, 40.0, 0.001) y0=[0, 1.2, 0, 0, -0.4, .1] ## initial state of the system y0=[x, y, z, vx, vy, vz] result = odeint(weber, y0, t, tfirst=True) Energy=T(y0[3], y0[4], y0[5] ) + U(y0[0],y0[1],y0[2],y0[3],y0[4],y0[5]) print(&#39;The initial total energy T+U is equal to:&#39;, Energy) print(&#39;The luminal energy is:&#39;, m*c*c/2) print(&#39;The energy is subliminal:&#39;, Energy &lt; m*c**2 /2) r=(y0[0]*y0[0] + y0[1]*y0[1] + y0[2]*y0[2])**0.5 print(&#39;Webers critical radius is equal to:&#39;,(m*c*c)**-1) print(&#39;The initial radius r is within the Weber critical distance:&#39;, r &lt; (m*c*c)**-1 ) fig = plt.figure() ax = fig.add_subplot(1, 2, 1, projection=&#39;3d&#39;) ax.plot(result[:, 0], result[:, 1], result[:, 2]) ax.set_title(&quot;position&quot;) ax = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;) ax.plot(result[:, 3], result[:, 4], result[:, 5]) ax.set_title(&quot;velocity&quot;) . The initial total energy T+U is equal to: 0.8091666666666666 The luminal energy is: 0.25 The energy is subliminal: False Webers critical radius is equal to: 2.0 The initial radius r is within the Weber critical distance: True . Text(0.5, 0.92, &#39;velocity&#39;) . import matplotlib.pyplot import pylab def rho(x,y,z): return (x*x+y*y+z*z)**0.5 v_list=[] for j in range(4000): v_list.append( (rho(result[j,0], result[j,1], result[j,2]), T(result[j,3], result[j,4], result[j,5])) ) U_list=[] for j in range(4000): U_list.append( (rho(result[j,0], result[j,1], result[j,2]), U(result[j,0],result[j,1],result[j,2],result[j,3],result[j,4],result[j,5])+T(result[j,3], result[j,4], result[j,5])) ) prelist1 = list(zip(*v_list)) pylab.scatter(list(prelist1[0]),list(prelist1[1])) pylab.xlabel(&#39;distance r&#39;) pylab.ylabel(&#39;Kinetic Energy T&#39;) pylab.title(&#39;rT plot&#39;) pylab.show() prelist2 = list(zip(*U_list)) pylab.scatter(list(prelist2[0]),list(prelist2[1])) pylab.xlabel(&#39;distance r&#39;) pylab.ylabel(&#39;T+U&#39;) pylab.title(&#39;Conservation of Energy&#39;) pylab.show() ## Error. Expect to see conservation of energy T+U = constant in the second figure. However the sum T+U appears nonconstant. . Discussion: The horizontal axis is the radial distance $r$ from the origin, and the vertical axis is the energy value. We expect the second figure to be a horizontal straight line. It does appear basically flat except for blow-up behaviour at small distance. Perhaps given the relative size of the interval of integration, namely $0.001$, the total energy $T+U$ is constant within error. . Is this loss/gain of energy a defect from the odeint routine? In otherwords, is energy not conserved because of cumulative errors and approximations in the solution? . for j in range(50): print( U(result[j,0],result[j,1],result[j,2],result[j,3],result[j,4],result[j,5]) + T(result[j,3], result[j,4], result[j,5]) ) . 0.8091666666666666 0.8091694528769837 0.8091722560959542 0.8091750763849768 0.8091779137441633 0.8091807683256578 0.8091836401044897 0.8091865291060831 0.8091894353679061 0.8091923589396186 0.8091952998832174 0.8091982582731821 0.8092012341218966 0.8092042274175154 0.8092072382239642 0.8092102666157523 0.8092133126780814 0.8092163765069865 0.8092194582094769 0.8092225578481547 0.8092256753069924 0.8092288106302348 0.8092319638914872 0.8092351351750412 0.8092383245760144 0.8092415322004902 0.8092447570890509 0.8092480017730359 0.8092512645080137 0.8092545454217692 0.8092578446432682 0.8092611623026272 0.8092644985310876 0.8092678534609862 0.809271227118879 0.8092746194540041 0.8092780305448041 0.8092814604679077 0.8092849092969379 0.8092883771024215 0.8092918639517022 0.8092953699088504 0.8092988950345746 0.80930243938613 0.8093060030172264 0.8093095859779383 0.8093131883146094 0.8093168100697619 0.8093204512819989 0.8093241120128701 . Radius of Weber&#39;s Planetary Atomic Model . Now we consider the diameter of Weber&#39;s critical radius if the test particles $x_1$, $x_2$ are equal to a positron, electron pair. . The mass of the electron is: [ref]. . Lagrangian L=T-U . Given the conservation of energy $d(T+U)=0$ it seems natural to consider the so-called Lagrangian $L=T-U$. But what do we expect from this lagrangian? Is there any reason to believe that a &quot;minimum action&quot; principle holds for two-body systems? Does the above naive expression for $L$ actually correspond to a reasonable action functional on the state space? . L_list=[] for j in range(4000): L_list.append( (rho(result[j,0], result[j,1], result[j,2]), -U(result[j,0],result[j,1],result[j,2],result[j,3],result[j,4],result[j,5])+T(result[j,3], result[j,4], result[j,5])) ) prelist2 = list(zip(*L_list)) pylab.scatter(list(prelist2[0]),list(prelist2[1])) pylab.show() .",
            "url": "https://jhmartel.github.io/fp/fastpages/jupyter/2022/02/10/LetterToAssisWeberPotentials.html",
            "relUrl": "/fastpages/jupyter/2022/02/10/LetterToAssisWeberPotentials.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post43": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jhmartel.github.io/fp/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Welcome to JHM Labs! . The purpose of this website is to broadcast and record our ideas in short articles. This website is powered by fastpages 1. . JHM is mathematician originally from Aylmer, Quebec, Canada. . Our goal is to build a math-physics lab for the 21st century. Our research includes python, jupyter notebooks, optimal transport, topology of singularities, energy and electrodynamics, history of science, computation, mapping class groups, geometric topology, and whatever else comes along. . Any questions or comments are welcome at: jhmlabs [at] gmail [dot] com . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jhmartel.github.io/fp/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jhmartel.github.io/fp/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}